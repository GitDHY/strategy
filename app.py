import streamlit as st
import yfinance as yf
import pandas as pd
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import numpy as np
import json
import os
import requests
import io

import datetime

import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import threading
import time

# Set page config must be the first streamlit command
st.set_page_config(layout="wide", page_title="Stock Strategy Analyzer v1.5")

# --- Helper Functions for Indicators ---

# === åŸæœ‰é˜ˆå€¼ (v1.6 æ”¶ç›Šå¢å¼ºä¼˜åŒ–) ===
VIX_BOOST_LO = 14.0                 # æé«˜: 13â†’14ï¼Œå‡å°‘æ¿€è¿›æ¨¡å¼è¯¯è§¦å‘
VIX_CUT_HI = 23.0                   # æé«˜: 20â†’23ï¼Œå»¶è¿Ÿé˜²å¾¡ï¼ˆVIX 20-23æ˜¯æ­£å¸¸æ³¢åŠ¨ï¼‰
VIX_PANIC = 28.0                    # æé«˜: 25â†’28ï¼Œæé«˜ææ…Œé˜ˆå€¼
YIELD_CURVE_CUTOFF = -0.35          # æ”¾å®½: -0.30â†’-0.35ï¼Œå‡å°‘è¯¯æŠ¥

# === ä¼˜åŒ–å‚æ•° (v1.6 æ”¶ç›Šå¢å¼º) ===
# 1. æ³¢åŠ¨ç‡ç›®æ ‡æœºåˆ¶ - æé«˜ç›®æ ‡æ³¢åŠ¨ç‡ä»¥è·å–æ›´é«˜æ”¶ç›Š
TARGET_VOL = 0.14                   # æé«˜: 0.12â†’0.14ï¼Œæ¥å—æ›´é«˜æ³¢åŠ¨æ¢å–æ”¶ç›Š
VOL_LOOKBACK = 20                   # ä¿æŒ
VOL_SCALAR_MAX = 1.8                # æé«˜: 1.5â†’1.8ï¼Œå…è®¸æ›´å¤šé¡ºåŠ¿åŠ ä»“
VOL_SCALAR_MIN = 0.4                # æé«˜: 0.3â†’0.4ï¼Œå‡å°‘è¿‡åº¦å‡ä»“

# 2. åŠ¨æ€æ­¢æŸæœºåˆ¶ - æ”¾å®½æ­¢æŸçº¿ï¼Œå‡å°‘è¯¯æ€
DRAWDOWN_STOP_LOSS = -0.12          # æ”¾å®½: -0.10â†’-0.12ï¼Œå‡å°‘å‡çªç ´æ­¢æŸ
DRAWDOWN_REDUCE_RATIO = 0.4         # é™ä½: 0.5â†’0.4ï¼Œæ­¢æŸå‡ä»“æ›´æ¸©å’Œ
DRAWDOWN_RECOVERY_THRESHOLD = -0.06 # æ”¾å®½: -0.05â†’-0.06

# 3. VIXå“åº”å¹³æ»‘åŒ–å‚æ•° - æé«˜å“åº”é˜ˆå€¼
VIX_SMOOTH_START = 18.0             # æé«˜: 15â†’18ï¼Œå‡å°‘ä½VIXåŒºé—´çš„æ‹–ç´¯
VIX_SMOOTH_END = 32.0               # æé«˜: 30â†’32
VIX_MAX_REDUCTION = 0.35            # é™ä½: 0.40â†’0.35ï¼Œå‡å°‘æœ€å¤§å‡ä»“

# 4. ä¿¡å·ç¡®è®¤å»¶è¿Ÿ - å¯¹æŠ„åº•ä¿¡å·æ›´æ¿€è¿›
SIGNAL_CONFIRM_DAYS = 2             # ä¿æŒï¼ˆå°†å¯¹EXTREME_ACCUMULATIONç‰¹æ®Šå¤„ç†ï¼‰

# 5. å†å¹³è¡¡å®¹å¿å¸¦ - ç•¥å¾®æ”¾å®½å‡å°‘äº¤æ˜“æˆæœ¬
REBALANCE_THRESHOLD = 0.06          # æé«˜: 0.05â†’0.06

# 6. çŠ¶æ€è½¬æ¢å¹³æ»‘ - åŠ å¿«è¿‡æ¸¡
STATE_TRANSITION_DAYS = 2           # é™ä½: 3â†’2ï¼Œæ›´å¿«å“åº”

# === æ–°å¢ä¼˜åŒ–å‚æ•°ï¼ˆä½è¿‡æ‹Ÿåˆé£é™©ï¼‰v1.6 ===
# 7. åŠ¨é‡å¼ºåº¦åˆ†å±‚é…ç½® - ç¼©çª„ä¸­æ€§åŒºï¼Œå‡å°‘ä¸å¿…è¦å‡ä»“
MOMENTUM_STRONG_THRESHOLD = 1.03    # é™ä½: 1.05â†’1.03
MOMENTUM_WEAK_THRESHOLD = 0.93      # é™ä½: 0.95â†’0.93ï¼ˆæ›´çª„çš„ç†Šå¸‚å®šä¹‰ï¼‰
MOMENTUM_NEUTRAL_REDUCTION = 0.08   # é™ä½: 0.15â†’0.08ï¼Œä¸­æ€§åŒºå‡ä»“æ›´æ¸©å’Œ

# 8. Sahm Rule é¢„è­¦å¢å¼º - æ”¶çª„é¢„è­¦åŒºé—´
SAHM_EARLY_WARNING_LO = 0.35        # æé«˜: 0.30â†’0.35ï¼Œå‡å°‘è¯¯æŠ¥
SAHM_EARLY_WARNING_HI = 0.50        # ä¿æŒ
SAHM_REDUCTION_RATE = 0.40          # é™ä½: 0.50â†’0.40

# 9. æ”¶ç›Šç‡æ›²çº¿è§£å€’æŒ‚å»¶ä¿æŠ¤ - ç¼©çŸ­ä¿æŠ¤æœŸ
YC_UNINVERT_PROTECTION_MONTHS = 9   # é™ä½: 12â†’9
YC_UNINVERT_REDUCTION = 0.15        # é™ä½: 0.20â†’0.15

# 10. VIXå‡å€¼å›å½’åŠ ä»“ - å¢å¼ºåŠ ä»“åŠ›åº¦
VIX_MEAN_REVERSION_PEAK = 23.0      # é™ä½: 25â†’23ï¼Œæ›´æ—©è§¦å‘åŠ ä»“
VIX_MEAN_REVERSION_RATIO = 0.75     # é™ä½: 0.80â†’0.75ï¼Œæ›´æ—©ç¡®è®¤å›è½
VIX_MEAN_REVERSION_BOOST = 0.12     # æé«˜: 0.10â†’0.12

# 11. ç›¸å…³æ€§åŠ¨æ€å†é…ç½® - æ”¾å®½è§¦å‘æ¡ä»¶
CORR_MID_THRESHOLD = 0.18           # æé«˜: 0.15â†’0.18
CORR_HIGH_THRESHOLD = 0.35          # æé«˜: 0.30â†’0.35
CORR_MAX_REALLOC = 0.12             # é™ä½: 0.15â†’0.12

# === æ–°å¢ä¼˜åŒ–å‚æ•° v1.7 æ”¶ç›Šæœ€å¤§åŒ– ===
# 12. ç°é‡‘ç¼“å†²æœºåˆ¶ - å®Œå…¨ç¦ç”¨ï¼ˆç”¨æˆ·è¦æ±‚ï¼‰
CASH_BUFFER_BASE = 0.0              # ç¦ç”¨: 0.02â†’0
CASH_BUFFER_VIX_THRESHOLD = 999.0   # ç¦ç”¨: æ°¸è¿œä¸è§¦å‘
CASH_BUFFER_MAX = 0.0               # ç¦ç”¨: 0.10â†’0
CASH_BUFFER_VIX_SCALE = 0.0         # ç¦ç”¨: 0.012â†’0

# 13. CAUTIOUS_VOL VIXåˆ†å±‚ - v1.7 æ›´æ¿€è¿›çš„æƒç›Šé…ç½®ï¼ŒåŠ¨æ€IWY/WTMFè½®æ¢
CAUTIOUS_VOL_VIX_TIERS = {
    # VIXåŒºé—´: (lo, hi, IWYæƒé‡, WTMFæƒé‡) - æ›´æ¿€è¿›è½®æ¢
    'tier1': (20, 25, 0.40, 0.20),   # 20-25: IWYâ†‘40%, WTMFâ†“20% (ä½æ³¢æ—¶æ›´å¤šæˆé•¿)
    'tier2': (25, 30, 0.30, 0.30),   # 25-30: å‡è¡¡
    'tier3': (30, 40, 0.20, 0.40),   # 30-40: VIXé«˜æ—¶WTMFå¯¹å†²
    'tier4': (40, 999, 0.10, 0.50),  # 40+:   æç«¯æ³¢åŠ¨ï¼ŒWTMFä¸»å¯¼
}

# 14. åŒå‡çº¿è¶‹åŠ¿ç¡®è®¤ - é™ä½å‡ä»“å¹…åº¦
TREND_MA_SHORT = 50                 # ä¿æŒ
TREND_MA_LONG = 200                 # ä¿æŒ
WEAK_BEAR_REDUCTION = 0.20          # é™ä½: 0.30â†’0.20
STRONG_BEAR_REDUCTION = 0.55        # é™ä½: 0.70â†’0.55

# 15. æ­¢æŸåˆ†é˜¶æ®µæ¢å¤ - æ›´å¿«æ¢å¤
STOP_LOSS_RECOVERY_STAGES = [
    # (å›æ’¤é˜ˆå€¼, æ¢å¤ä»“ä½æ¯”ä¾‹) - ä¼˜åŒ–åæ›´å¿«æ¢å¤
    (-0.12, 0.55),   # -12%: 55%ä»“ä½ (åŸ-10%, 50%)
    (-0.08, 0.75),   # -8%:  75%ä»“ä½ (åŸ-7.5%, 70%)
    (-0.05, 0.90),   # -5%:  90%ä»“ä½ (åŸ-5%, 85%)
    (-0.02, 1.00),   # -2%:  å®Œå…¨æ¢å¤ (åŸ-2.5%)
]

# 16. è·¨èµ„äº§åŠ¨é‡ - é™ä½å‡ä»“åŠ›åº¦
MARKET_BREADTH_LOW = 0.25           # é™ä½: 0.30â†’0.25
MARKET_BREADTH_MID = 0.45           # é™ä½: 0.50â†’0.45
BREADTH_LOW_REDUCTION = 0.10        # é™ä½: 0.15â†’0.10
BREADTH_MID_REDUCTION = 0.03        # é™ä½: 0.05â†’0.03

# === æ–°å¢ v1.6 æ”¶ç›Šå¢å¼ºå‚æ•° ===
# 17. è¶‹åŠ¿é¡ºåŠ¿åŠ ä»“ï¼ˆæ–°å¢ï¼‰
TREND_BOOST_THRESHOLD = 1.08        # Price > MA * 1.08 æ—¶å¯åŠ¨é¡ºåŠ¿åŠ ä»“
TREND_BOOST_AMOUNT = 0.08           # ä»WTMF/MBHè½¬ç§»8%åˆ°IWY
TREND_BOOST_VIX_MAX = 18.0          # åªåœ¨VIX<18æ—¶å¯ç”¨

# 18. æŠ„åº•åŠ é€Ÿç¡®è®¤ï¼ˆæ–°å¢ï¼‰
EXTREME_CONFIRM_DAYS = 1            # EXTREME_ACCUMULATIONåªéœ€1å¤©ç¡®è®¤ï¼ˆæ›´å¿«æŠ„åº•ï¼‰

# 19. ç‰›å¸‚WTMFæœ€å°åŒ–ï¼ˆv1.7 å®Œå…¨å–æ¶ˆï¼‰
NEUTRAL_MIN_WTMF = 0.0              # NEUTRALçŠ¶æ€WTMF=0%ï¼ˆåŸ5%ï¼‰ï¼Œå…¨éƒ¨è½¬IWY
NEUTRAL_WTMF_BOOST_TO_IWY = True    # å¯ç”¨WTMFâ†’IWYè½¬æ¢

# === v1.7 æ–°å¢: æˆé•¿/çº¢åˆ©/WTMFåŠ¨æ€è½®æ¢å‚æ•° ===
# 20. VIXé©±åŠ¨çš„æˆé•¿â†”çº¢åˆ©è½®æ¢
VIX_GROWTH_TO_VALUE_START = 22.0    # VIX>22æ—¶å¼€å§‹ä»IWYè½¬å‘LVHI
VIX_GROWTH_TO_VALUE_FULL = 35.0     # VIXâ‰¥35æ—¶è¾¾åˆ°æœ€å¤§è½¬æ¢æ¯”ä¾‹
GROWTH_TO_VALUE_MAX_SHIFT = 0.20    # æœ€å¤§ä»IWYè½¬ç§»20%åˆ°LVHI

# 21. è¶‹åŠ¿å¼ºåº¦é©±åŠ¨çš„æˆé•¿â†”WTMFè½®æ¢
TREND_STRONG_BULL = 1.10            # Price > MA*1.10: å¼ºç‰›å¸‚
TREND_MILD_BULL = 1.03              # Price > MA*1.03: æ¸©å’Œç‰›å¸‚
TREND_MILD_BEAR = 0.97              # Price < MA*0.97: æ¸©å’Œç†Šå¸‚
TREND_STRONG_BEAR_LINE = 0.90       # Price < MA*0.90: å¼ºç†Šå¸‚
# è½¬æ¢å¹…åº¦
BULL_IWY_BOOST = 0.10               # å¼ºç‰›æ—¶ä»WTMFè½¬10%åˆ°IWY
BEAR_WTMF_BOOST = 0.15              # å¼ºç†Šæ—¶ä»IWYè½¬15%åˆ°WTMF

# 22. çº¢åˆ©ç›¸å¯¹å¼ºå¼±
VALUE_OUTPERFORM_THRESHOLD = 0.03   # LVHIç›¸å¯¹IWYè·‘èµ¢3%æ—¶å¢é…çº¢åˆ©
VALUE_UNDERPERFORM_THRESHOLD = -0.05 # LVHIç›¸å¯¹IWYè·‘è¾“5%æ—¶å‡é…çº¢åˆ©
VALUE_ROTATION_AMOUNT = 0.08        # è½®æ¢å¹…åº¦8%

# === èµ„äº§ç±»åˆ«æ˜ å°„ (ç”¨äºé£é™©æš´éœ²åˆ†æå’Œé‚®ä»¶ç”Ÿæˆ) ===
ASSET_CATEGORIES = {
    'IWY': {'category': 'æƒç›Š', 'sub': 'ç¾è‚¡æˆé•¿', 'risk_level': 'high'},
    'LVHI': {'category': 'æƒç›Š', 'sub': 'ç¾è‚¡çº¢åˆ©', 'risk_level': 'medium'},
    'G3B.SI': {'category': 'æƒç›Š', 'sub': 'æ–°åŠ å¡è“ç­¹', 'risk_level': 'medium'},
    'SRT.SI': {'category': 'å¦ç±»', 'sub': 'REITs', 'risk_level': 'medium'},
    'AJBU.SI': {'category': 'å¦ç±»', 'sub': 'REITs', 'risk_level': 'medium'},
    'MBH.SI': {'category': 'å›ºæ”¶', 'sub': 'æ–°å…ƒå€ºåˆ¸', 'risk_level': 'low'},
    'GSD.SI': {'category': 'å•†å“', 'sub': 'é»„é‡‘', 'risk_level': 'medium'},
    'WTMF': {'category': 'å¯¹å†²', 'sub': 'å±æœºAlpha', 'risk_level': 'low'},
    'OTHERS': {'category': 'å…¶ä»–', 'sub': 'å…¶ä»–èµ„äº§', 'risk_level': 'unknown'},
}

# === èµ„äº§åç§°æ˜ å°„ (ç”¨äºé‚®ä»¶å’ŒUIæ˜¾ç¤º) ===
ASSET_NAMES = {
    'IWY': 'ç¾è‚¡æˆé•¿ (Russell Top 200 Growth)',
    'WTMF': 'å±æœºAlpha (Managed Futures)',
    'LVHI': 'ç¾è‚¡çº¢åˆ© (High Div Low Vol)',
    'G3B.SI': 'æ–°åŠ å¡è“ç­¹ (STI ETF)',
    'MBH.SI': 'æ–°å…ƒå€ºåˆ¸ (Govt Bond)',
    'GSD.SI': 'é»„é‡‘ (Gold)',
    'SRT.SI': 'S-REITs (Supermarket)',
    'AJBU.SI': 'Keppel DC REIT',
    'TLT': 'ç¾å€º (20Y Treasury)',
    'SPY': 'æ ‡æ™®500 (S&P 500)',
    'OTHERS': 'å…¶ä»–/å¾…æ¸…ç†èµ„äº§ (Others)'
}

SCHEDULER_LOCK = os.path.join(os.path.dirname(__file__), "data", "scheduler.lock")
STATE_HISTORY_FILE = os.path.join(os.path.dirname(__file__), "data", "state_history.json")
PORTFOLIO_HISTORY_FILE = os.path.join(os.path.dirname(__file__), "data", "portfolio_history.json")
os.makedirs(os.path.dirname(SCHEDULER_LOCK), exist_ok=True)
os.makedirs(os.path.dirname(STATE_HISTORY_FILE), exist_ok=True)

def normalize_yf_prices(df_raw):
    if df_raw is None or len(df_raw) == 0:
        return pd.DataFrame()
    if isinstance(df_raw.columns, pd.MultiIndex):
        if 'Adj Close' in df_raw.columns.get_level_values(0):
            return df_raw['Adj Close']
        if 'Close' in df_raw.columns.get_level_values(0):
            return df_raw['Close']
        return df_raw
    if 'Adj Close' in df_raw.columns:
        return df_raw['Adj Close']
    if 'Close' in df_raw.columns:
        return df_raw['Close']
    return df_raw


def ensure_fred_cached(series_ids=("UNRATE", "T10Y2Y")):
    """Eager-download FRED CSVs into local cache before analysis/backtest/email."""
    for sid in series_ids:
        try:
            _ = fetch_fred_data(sid)
        except Exception as e:
            log_event("WARN", "fred_prefetch_failed", {"series": sid, "err": str(e)})

def evaluate_risk_triggers(s, gold_bear=False, value_regime=False, asset_trends=None, vix=None, yield_curve=None, sahm=None, corr=None, yc_recently_inverted=False, dual_ma_signals=None, breadth_score=None):
    if asset_trends is None:
        asset_trends = {}
    reasons = []

    # 1. Style Regime
    if s in ["NEUTRAL", "CAUTIOUS_TREND"] and value_regime:
        reasons.append("ğŸ§± é£æ ¼è½®åŠ¨: ä»·å€¼å ä¼˜ (Value Regime) -> å¢åŠ çº¢åˆ©ï¼Œå‡å°‘æˆé•¿")

    # 2. Dynamic Risk Control
    if s == "NEUTRAL" and vix is not None:
        if vix < VIX_BOOST_LO:
            reasons.append(f"ğŸš€ æåº¦å¹³ç¨³ (VIX < {VIX_BOOST_LO}): æ¿€è¿›æ¨¡å¼ -> æ¸…ç©ºWTMF/å‡å€ºï¼ŒåŠ ä»“æˆé•¿")
        elif vix > VIX_CUT_HI:
            reasons.append(f"ğŸŒ¬ï¸ æ—©æœŸé¢„è­¦ (VIX > {VIX_CUT_HI}): é¿é™©æ¨¡å¼ -> å‡ä»“æˆé•¿ 20%ï¼Œå¢åŠ  WTMF")
    
    # v1.5: CAUTIOUS_VOL VIXåˆ†å±‚
    if s == "CAUTIOUS_VOL" and vix is not None:
        if vix >= 30:
            reasons.append(f"ğŸ”´ é«˜æ³¢åŠ¨åˆ†å±‚ (VIX={vix:.1f}â‰¥30): IWYé™è‡³10%ï¼ŒWTMFå‡è‡³40%")
        elif vix >= 25:
            reasons.append(f"ğŸŸ  ä¸­æ³¢åŠ¨åˆ†å±‚ (VIX={vix:.1f}â‰¥25): IWYé™è‡³20%ï¼ŒWTMFå‡è‡³35%")

    if s in ["DEFLATION_RECESSION", "CAUTIOUS_TREND"] and yield_curve is not None:
        if yield_curve < YIELD_CURVE_CUTOFF:
            reasons.append(f"âš ï¸ æ·±åº¦å€’æŒ‚ (Yield Curve < {YIELD_CURVE_CUTOFF}%): å€ºåˆ¸é™·é˜± -> å¤§å¹…å‰Šå‡ MBHï¼Œè½¬å…¥ WTMF")

    # 3. Trend Filters (v1.5: æ”¯æŒåŒå‡çº¿)
    if s != "EXTREME_ACCUMULATION":
        if dual_ma_signals:
            strong_bear = [t for t, sig in dual_ma_signals.items() if sig == "STRONG_BEAR"]
            weak_bear = [t for t, sig in dual_ma_signals.items() if sig == "WEAK_BEAR"]
            if strong_bear:
                reasons.append(f"ğŸ“‰ å¼ºç†Šå¸‚ä¿¡å·: {', '.join(strong_bear)} (ä»·æ ¼<MA200ä¸”MA50<MA200) -> å‡ä»“70%")
            if weak_bear:
                reasons.append(f"ğŸ“Š å¼±ç†Šå¸‚ä¿¡å·: {', '.join(weak_bear)} (å¯èƒ½å›è°ƒ) -> å‡ä»“30%")
        else:
            assets_to_check = ['G3B.SI', 'LVHI', 'MBH.SI', 'GSD.SI', 'SRT.SI', 'AJBU.SI']
            bear_assets = [t for t in assets_to_check if asset_trends.get(t, False)]
            if bear_assets:
                reasons.append(f"ğŸ“‰ è¶‹åŠ¿ç†”æ–­: {', '.join(bear_assets)} ç ´ä½ -> æ¸…ä»“")

        if asset_trends.get('IWY', False):
            cut = "80%" if (vix and vix > VIX_PANIC) else "50%"
            reasons.append(f"ğŸ›¡ï¸ æ ¸å¿ƒç†”æ–­: IWY ç ´ä½ -> å‰Šå‡ {cut} ä»“ä½")
    
    # 4. Sahm Rule é¢„è­¦
    if sahm is not None and SAHM_EARLY_WARNING_LO <= sahm < SAHM_EARLY_WARNING_HI:
        reduction_pct = int((sahm - SAHM_EARLY_WARNING_LO) / (SAHM_EARLY_WARNING_HI - SAHM_EARLY_WARNING_LO) * SAHM_REDUCTION_RATE * 100)
        reasons.append(f"ğŸ“‰ Sahmé¢„è­¦ ({sahm:.2f}): è¡°é€€é£é™©ä¸Šå‡ -> IWYé¢„é˜²æ€§å‡ä»“ {reduction_pct}%")
    
    # 5. æ”¶ç›Šç‡æ›²çº¿è§£å€’æŒ‚ä¿æŠ¤
    if yc_recently_inverted and yield_curve is not None and yield_curve > 0:
        reasons.append(f"ğŸ“ˆ è§£å€’æŒ‚ä¿æŠ¤: æ›²çº¿è½¬æ­£ä½†è¿‘æœŸæ›¾å€’æŒ‚ -> ç»´æŒé˜²å¾¡é…ç½® {int(YC_UNINVERT_REDUCTION*100)}%")
    
    # 6. ç›¸å…³æ€§è°ƒæ•´ (v1.5: æ¸è¿›å“åº”)
    if corr is not None and corr > CORR_MID_THRESHOLD:
        if corr > CORR_HIGH_THRESHOLD:
            reasons.append(f"ğŸ”— ç›¸å…³æ€§å¤±æ•ˆ (Corr={corr:.2f}): è‚¡å€ºåŒæ¶¨åŒè·Œ -> MBHæ¸è¿›è½¬ç§»è‡³WTMF/é»„é‡‘ (æœ€å¤§{int(CORR_MAX_REALLOC*100)}%)")
        else:
            reasons.append(f"ğŸ”— ç›¸å…³æ€§å‡é«˜ (Corr={corr:.2f}): å¼€å§‹å‡å°‘MBHé…ç½®")
    
    # v1.5: å¸‚åœºå¹¿åº¦
    if breadth_score is not None and breadth_score < MARKET_BREADTH_MID:
        if breadth_score < MARKET_BREADTH_LOW:
            reasons.append(f"ğŸ“Š å¸‚åœºå¹¿åº¦å·® ({breadth_score*100:.0f}%<{MARKET_BREADTH_LOW*100:.0f}%): å¤šæ•°èµ„äº§ä¸‹è·Œ -> æƒç›Šå‡ä»“{int(BREADTH_LOW_REDUCTION*100)}%")
        else:
            reasons.append(f"ğŸ“Š å¸‚åœºå¹¿åº¦ä¸€èˆ¬ ({breadth_score*100:.0f}%): æƒç›Šå°å¹…å‡ä»“{int(BREADTH_MID_REDUCTION*100)}%")
    
    # v1.5: ç°é‡‘ç¼“å†²
    if vix is not None and vix > CASH_BUFFER_VIX_THRESHOLD and s != "EXTREME_ACCUMULATION":
        extra_cash = min((vix - CASH_BUFFER_VIX_THRESHOLD) / 5 * CASH_BUFFER_VIX_SCALE, CASH_BUFFER_MAX - CASH_BUFFER_BASE)
        total_cash = CASH_BUFFER_BASE + extra_cash
        reasons.append(f"ğŸ’µ ç°é‡‘ç¼“å†² (VIX={vix:.1f}): ä¿ç•™{total_cash*100:.1f}%ç°é‡‘")

    # 7. Gold
    if gold_bear:
        reasons.append("ğŸ» é»„é‡‘ç†Šå¸‚: Gold < MA200 -> æ¸…ä»“ GSD.SI")

    return reasons

def get_adjustment_reasons(s, gold_bear=False, value_regime=False, asset_trends=None, vix=None, yield_curve=None, sahm=None, corr=None, yc_recently_inverted=False, dual_ma_signals=None, breadth_score=None):
    """
    Returns a list of strings explaining why the allocation differs from the base static model.
    """
    return evaluate_risk_triggers(
        s,
        gold_bear=gold_bear,
        value_regime=value_regime,
        asset_trends=asset_trends,
        vix=vix,
        yield_curve=yield_curve,
        sahm=sahm,
        corr=corr,
        yc_recently_inverted=yc_recently_inverted,
        dual_ma_signals=dual_ma_signals,
        breadth_score=breadth_score,
    )

# Removed cache for debugging connection issues
def fetch_fred_data(series_id, max_attempts: int = 2, timeout_sec: int = 10):
    """
    Robust fetch for FRED data with Auto-Update & Caching logic.
    Priority:
    1. Fresh Local File (modified today): Use directly.
    2. Network Fetch: Download and save to local (fred_{series_id}.csv), then use.
    3. Stale Local File: Fallback if network fails.
    
    æ”¹è¿›ï¼š
    - æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯ï¼ˆçŠ¶æ€ç ã€å¼‚å¸¸åŸå›  + è¿”å›ä½“é¢„è§ˆï¼‰ã€‚
    - å¢åŠ  http å¤‡ä»½ URLï¼Œå…¼å®¹éƒ¨åˆ† TLS æ‹¦æˆª/è¯ä¹¦é—®é¢˜çš„ç½‘ç»œã€‚
    - å¢åŠ  Accept å¤´ï¼Œé¿å…è¢«åˆ¤ä¸ºæœºå™¨äººæµé‡ã€‚
    - å½“æ—¥æ–‡ä»¶æ”¯æŒå¤šè·¯å¾„/å¤šå‘½å (fred_{id}.csv æˆ– {id}.csv)ï¼Œé¿å…æ‰‹åŠ¨ä¸‹è½½åæœªè¢«è¯†åˆ«ã€‚
    - ç¼©çŸ­ UI ç­‰å¾…æ—¶é—´ï¼šé»˜è®¤ 2 æ¬¡å°è¯•ï¼Œæ¯æ¬¡è¶…æ—¶ 10 ç§’ï¼Œé¿å…å‰ç«¯å¡é¡¿ã€‚
    """
    base_dir = os.path.dirname(__file__)
    file_name = f"fred_{series_id}.csv"
    alt_name = f"{series_id}.csv"
    cache_dir = os.path.join(base_dir, "data")
    os.makedirs(cache_dir, exist_ok=True)
    candidates = [
        os.path.join(base_dir, file_name),
        os.path.join(os.getcwd(), file_name),
        os.path.join(base_dir, alt_name),
        os.path.join(os.getcwd(), alt_name),
        os.path.join(cache_dir, file_name),
        os.path.join(cache_dir, alt_name),
    ]
    candidates = list(dict.fromkeys(candidates))
    target_path = candidates[0]
    lastgood_path = os.path.join(cache_dir, f"fred_{series_id}_lastgood.csv")
    
    # 1) å½“æ—¥æœ¬åœ°ç¼“å­˜ï¼ˆè¯†åˆ«æ‰‹åŠ¨ä¸‹è½½çš„ä¸¤ç§å‘½åï¼‰
    for path in candidates:
        if os.path.exists(path):
            try:
                mtime = datetime.date.fromtimestamp(os.path.getmtime(path))
                if mtime == datetime.date.today():
                    df = pd.read_csv(path, parse_dates=['observation_date'], index_col='observation_date')
                    df.columns = [series_id]
                    return df
            except Exception as e:
                print(f"Error reading fresh local file {path}: {e}")
    
    # 2) ç½‘ç»œä¸‹è½½ï¼ˆå« https -> http å¤‡ä»½ + é€€é¿é‡è¯•ï¼‰
    urls = [
        f"https://fred.stlouisfed.org/graph/fredgraph.csv?id={series_id}",
        f"http://fred.stlouisfed.org/graph/fredgraph.csv?id={series_id}",
    ]
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/csv,application/octet-stream;q=0.9,*/*;q=0.8",
        "Connection": "close",
    }
    import urllib3
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
    
    last_err = None
    for attempt in range(max_attempts):
        backoff = max(1, (attempt + 1))
        for url in urls:
            try:
                resp = requests.get(url, headers=headers, timeout=timeout_sec, verify=False, allow_redirects=True)
                status = resp.status_code
                preview = resp.text[:200] if resp is not None else ""
                if status != 200:
                    raise RuntimeError(f"HTTP {status}, preview: {preview}")
                content = resp.content.decode('utf-8', errors='ignore')
                lower_head = content[:200].lower()
                if "<html" in lower_head or "<!doctype" in lower_head:
                    raise RuntimeError(f"HTML page returned, preview: {content[:200]}")
                if 'observation_date' not in content:
                    raise RuntimeError(f"Missing observation_date, preview: {content[:200]}")
                if len(content) < 50:
                    raise RuntimeError(f"Empty/short content (len={len(content)}), preview: {content[:200]}")
                try:
                    with open(target_path, "w", encoding="utf-8") as f:
                        f.write(content)
                    with open(lastgood_path, "w", encoding="utf-8") as f:
                        f.write(content)
                except Exception as e:
                    print(f"Failed to write cache file: {e}")
                df = pd.read_csv(io.StringIO(content), parse_dates=['observation_date'], index_col='observation_date')
                df.columns = [series_id]
                return df
            except Exception as e:
                last_err = f"{url} -> {e}"
                continue
        time.sleep(backoff)
    
    if last_err:
        print(f"Error fetching FRED data ({series_id}): {last_err}")
        safe_warn(f"âš ï¸ è‡ªåŠ¨ä¸‹è½½ FRED æ•°æ®å¤±è´¥ ({series_id})ã€‚é”™è¯¯: {last_err}\n\n**è§£å†³æ–¹æ³•**ï¼š1) æ£€æŸ¥ç½‘ç»œ/ä»£ç†ï¼Œ2) å¯æ‰‹åŠ¨ä¸‹è½½å¹¶æ”¾å…¥ç¨‹åºç›®å½• (fred_{series_id}.csv æˆ– {series_id}.csv)ã€‚")

    # 3) å…œåº•ä½¿ç”¨æœ¬åœ°æ—§æ–‡ä»¶ï¼ˆå« lastgoodï¼‰
    fallback_candidates = list(candidates)
    if lastgood_path not in fallback_candidates:
        fallback_candidates.append(lastgood_path)

    for local_file in fallback_candidates:
        if os.path.exists(local_file):
            try:
                df = pd.read_csv(local_file, parse_dates=['observation_date'], index_col='observation_date')
                df.columns = [series_id]
                file_date = datetime.date.fromtimestamp(os.path.getmtime(local_file))
                safe_warn(f"âš ï¸ æ— æ³•è¿æ¥ FRED æ•°æ®æº ({series_id})ã€‚å·²ä½¿ç”¨æœ¬åœ°å†å²æ•°æ® (æ—¥æœŸ: {file_date})ã€‚\n\n**è§£å†³æ–¹æ³•**ï¼šè¯·æ£€æŸ¥ç½‘ç»œï¼Œæˆ–æ‰‹åŠ¨æ›´æ–°æ•°æ®ã€‚")
                return df
            except Exception:
                continue

    safe_warn(f"âš ï¸ æ— æ³•è¿æ¥ FRED æ•°æ®æº ({series_id}) ä¸”æ— æœ¬åœ°å¤‡ä»½ã€‚\n\n**è§£å†³æ–¹æ³•**ï¼šè¯·å±•å¼€é¡µé¢é¡¶éƒ¨çš„ **â€˜ğŸ“‚ æ‰‹åŠ¨å¯¼å…¥å®è§‚æ•°æ®â€™** é¢æ¿ï¼Œä¸Šä¼ è¯¥æ•°æ®æ–‡ä»¶ã€‚")
    return pd.DataFrame()




# --- Portfolio Manager ---
PORTFOLIO_FILE = os.path.join(os.path.dirname(__file__), "portfolios.json")

def load_portfolios():
    if not os.path.exists(PORTFOLIO_FILE):
        return {}
    try:
        with open(PORTFOLIO_FILE, "r") as f:
            return json.load(f)
    except:
        return {}

def save_portfolio(name, tickers, weights):
    data = load_portfolios()
    data[name] = {"tickers": tickers, "weights": weights}
    with open(PORTFOLIO_FILE, "w") as f:
        json.dump(data, f, indent=4)

def delete_portfolio(name):
    data = load_portfolios()
    if name in data:
        del data[name]
        with open(PORTFOLIO_FILE, "w") as f:
            json.dump(data, f, indent=4)

# --- Alert & Automation Config ---
ALERT_CONFIG_FILE = os.path.join(os.path.dirname(__file__), "alert_config.json")
DEFAULT_ALERT_CONFIG = {
    "enabled": False,
    "email_to": "",
    "email_from": "",
    "email_pwd": "",
    "smtp_server": "smtp.gmail.com",
    "smtp_port": 587,
    "frequency": "Manual",  # Manual, Daily, Weekly
    "trigger_time": "09:30",  # Singapore Time (UTC+8)
    "last_run": "",
    # New: real-time risk alerts
    "state_change_alert": False,
    "vix_alert_enabled": False,
    "vix_alert_threshold": 35,
    "channels": {
        "telegram_bot_token": "",
        "telegram_chat_id": "",
        "wechat_webhook": ""
    }
}

def load_alert_config():
    if os.path.exists(ALERT_CONFIG_FILE):
        try:
            with open(ALERT_CONFIG_FILE, "r") as f:
                cfg = json.load(f)
        except Exception as e:
            log_event("ERROR", "[AlertConfig] load failed, using defaults", {"err": str(e)})
            cfg = DEFAULT_ALERT_CONFIG.copy()
    else:
        cfg = DEFAULT_ALERT_CONFIG.copy()

    merged, issues, warns = validate_alert_config(cfg)
    for w in warns:
        safe_warn(f"âš ï¸ é…ç½®æé†’: {w}")
        log_event("WARN", w)
    for i in issues:
        safe_warn(f"âš ï¸ {i}")
        log_event("ERROR", i)
    return merged

def save_alert_config(config):
    with open(ALERT_CONFIG_FILE, "w") as f:
        json.dump(config, f, indent=4)


def safe_warn(msg: str):
    try:
        if threading.current_thread().name == "MainThread":
            st.warning(msg)
        else:
            print(msg)
    except Exception as e:
        print(f"[warn] {msg} (streamlit warn failed: {e})")


def log_event(level: str, message: str, extra=None):
    ts = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    payload = {"ts": ts, "level": level.upper(), "msg": message}
    if extra:
        payload["extra"] = extra
    try:
        print(json.dumps(payload, ensure_ascii=False))
    except Exception:
        print(f"[{level}] {message} | extra={extra}")


def load_state_history():
    try:
        if os.path.exists(STATE_HISTORY_FILE):
            with open(STATE_HISTORY_FILE, "r") as f:
                data = json.load(f)
                if isinstance(data, list):
                    return data
    except Exception as e:
        log_event("ERROR", "state_history_load_failed", {"err": str(e)})
    return []


def save_state_history(history):
    try:
        with open(STATE_HISTORY_FILE, "w") as f:
            json.dump(history, f, indent=2)
    except Exception as e:
        log_event("ERROR", "state_history_save_failed", {"err": str(e)})


def record_state_history(state, metrics):
    history = load_state_history()
    date_str = metrics.get('date') or datetime.date.today().isoformat()
    fetch_ts = metrics.get('fetch_ts') or datetime.datetime.now().isoformat(timespec='seconds')
    entry = {"date": date_str, "state": state, "ts": fetch_ts}

    if history and history[-1].get("date") == date_str:
        history[-1] = entry
    else:
        history.append(entry)
    save_state_history(history)
    return history


def get_state_change_info(history, current_state, current_date):
    if not current_date:
        return None
    streak_start = current_date
    prev_state = None
    prev_date = None
    for item in reversed(history):
        try:
            d = datetime.date.fromisoformat(item.get("date")) if item.get("date") else None
        except Exception:
            continue
        if item.get("state") == current_state:
            streak_start = d
        else:
            prev_state = item.get("state")
            prev_date = d
            break
    days_in_state = (current_date - streak_start).days + 1 if streak_start else None
    changed_on = streak_start
    return {
        "prev_state": prev_state,
        "prev_date": prev_date,
        "changed_on": changed_on,
        "days_in_state": days_in_state,
    }


# === æŒä»“å†å²è¿½è¸ªä¸å›æ’¤è®¡ç®— ===
def load_portfolio_history():
    """åŠ è½½æŒä»“å†å²è®°å½•"""
    try:
        if os.path.exists(PORTFOLIO_HISTORY_FILE):
            with open(PORTFOLIO_HISTORY_FILE, "r") as f:
                data = json.load(f)
                if isinstance(data, dict):
                    return data
    except Exception as e:
        log_event("ERROR", "portfolio_history_load_failed", {"err": str(e)})
    return {"records": [], "peak_value": 0, "cost_basis": 0}


def save_portfolio_history(history):
    """ä¿å­˜æŒä»“å†å²è®°å½•"""
    try:
        with open(PORTFOLIO_HISTORY_FILE, "w") as f:
            json.dump(history, f, indent=2)
    except Exception as e:
        log_event("ERROR", "portfolio_history_save_failed", {"err": str(e)})


def record_portfolio_snapshot(total_value, holdings_dict, state=None):
    """
    è®°å½•å½“å‰æŒä»“å¿«ç…§ï¼Œç”¨äºè®¡ç®—å›æ’¤
    total_value: å½“å‰æ€»å¸‚å€¼
    holdings_dict: {ticker: value} å½“å‰æŒä»“
    """
    history = load_portfolio_history()
    date_str = datetime.date.today().isoformat()
    
    record = {
        "date": date_str,
        "ts": datetime.datetime.now().isoformat(timespec='seconds'),
        "total_value": total_value,
        "holdings": holdings_dict,
        "state": state
    }
    
    records = history.get("records", [])
    # åŒä¸€å¤©åªä¿ç•™æœ€æ–°è®°å½•
    if records and records[-1].get("date") == date_str:
        records[-1] = record
    else:
        records.append(record)
    
    # åªä¿ç•™æœ€è¿‘90å¤©æ•°æ®
    if len(records) > 90:
        records = records[-90:]
    
    # æ›´æ–°å†å²æœ€é«˜å‡€å€¼
    peak_value = history.get("peak_value", 0)
    if total_value > peak_value:
        peak_value = total_value
    
    history["records"] = records
    history["peak_value"] = peak_value
    
    save_portfolio_history(history)
    return history


def calculate_portfolio_drawdown(current_value, history=None):
    """
    è®¡ç®—å½“å‰ç»„åˆå›æ’¤
    è¿”å›: (drawdown_pct, peak_value, days_since_peak, in_stop_loss_zone, recovery_ratio)
    """
    if history is None:
        history = load_portfolio_history()
    
    peak_value = history.get("peak_value", 0)
    if peak_value <= 0:
        return 0, current_value, 0, False, 1.0
    
    drawdown_pct = (current_value - peak_value) / peak_value
    
    # è®¡ç®—è·ç¦»å³°å€¼çš„å¤©æ•°
    records = history.get("records", [])
    days_since_peak = 0
    for rec in reversed(records):
        if rec.get("total_value", 0) >= peak_value * 0.999:  # å…è®¸0.1%è¯¯å·®
            break
        days_since_peak += 1
    
    # åˆ¤æ–­æ˜¯å¦åœ¨æ­¢æŸåŒºé—´
    in_stop_loss_zone = drawdown_pct < DRAWDOWN_STOP_LOSS
    
    # è®¡ç®—æ¢å¤æ¯”ä¾‹ (åˆ†é˜¶æ®µæ¢å¤)
    recovery_ratio = 1.0
    if in_stop_loss_zone:
        for threshold, ratio in STOP_LOSS_RECOVERY_STAGES:
            if drawdown_pct < threshold:
                recovery_ratio = ratio
                break
    
    return drawdown_pct, peak_value, days_since_peak, in_stop_loss_zone, recovery_ratio


def get_stop_loss_status(current_value, history=None):
    """
    è·å–æ­¢æŸçŠ¶æ€çš„è¯¦ç»†ä¿¡æ¯
    è¿”å›: dict with status details
    """
    drawdown_pct, peak_value, days_since_peak, in_stop_loss, recovery_ratio = calculate_portfolio_drawdown(current_value, history)
    
    status = {
        "current_value": current_value,
        "peak_value": peak_value,
        "drawdown_pct": drawdown_pct,
        "days_since_peak": days_since_peak,
        "in_stop_loss": in_stop_loss,
        "recovery_ratio": recovery_ratio,
        "should_reduce": in_stop_loss,
        "reduction_pct": (1 - recovery_ratio) * 100 if in_stop_loss else 0,
    }
    
    # åˆ¤æ–­æ¢å¤é˜¶æ®µ
    if in_stop_loss:
        status["stage"] = "æ­¢æŸä¸­"
        status["stage_color"] = "#f5222d"
        status["advice"] = f"é£é™©èµ„äº§å»ºè®®å‡ä»“è‡³ {recovery_ratio*100:.0f}%"
    elif drawdown_pct < DRAWDOWN_RECOVERY_THRESHOLD:
        status["stage"] = "æ¢å¤ä¸­"
        status["stage_color"] = "#faad14"
        status["advice"] = f"å›æ’¤{drawdown_pct*100:.1f}%ï¼Œæ¥è¿‘æ­¢æŸçº¿ï¼Œä¿æŒè­¦æƒ•"
    else:
        status["stage"] = "æ­£å¸¸"
        status["stage_color"] = "#52c41a"
        status["advice"] = "æŒä»“å¥åº·ï¼Œæ— éœ€æ­¢æŸè°ƒæ•´"
    
    return status


def reset_portfolio_peak(new_peak_value=None):
    """
    é‡ç½®å†å²æœ€é«˜å‡€å€¼ï¼ˆç”¨äºæ³¨å…¥æ–°èµ„é‡‘æˆ–æ‰‹åŠ¨è°ƒæ•´ï¼‰
    """
    history = load_portfolio_history()
    if new_peak_value is not None:
        history["peak_value"] = new_peak_value
    else:
        # ä½¿ç”¨æœ€è¿‘è®°å½•çš„æœ€é«˜å€¼
        records = history.get("records", [])
        if records:
            history["peak_value"] = max(r.get("total_value", 0) for r in records)
    save_portfolio_history(history)
    return history


def validate_alert_config(cfg: dict):
    merged = DEFAULT_ALERT_CONFIG.copy()
    issues = []
    warns = []
    if not isinstance(cfg, dict):
        issues.append("é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œå·²æ¢å¤é»˜è®¤é…ç½®")
        return merged, issues, warns

    # Required keys & types
    merged.update({k: cfg.get(k, v) for k, v in merged.items()})

    # Email fields
    for key in ["email_to", "email_from"]:
        merged[key] = str(merged.get(key, "") or "").strip()
    merged["email_pwd"] = merged.get("email_pwd", "") or ""
    env_pwd = os.environ.get("ALERT_EMAIL_PWD") or os.environ.get("SMTP_PASSWORD")
    if not merged["email_pwd"] and env_pwd:
        merged["email_pwd"] = env_pwd
        warns.append("å·²ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„é‚®ç®±å¯†ç /æˆæƒç ")

    # SMTP port
    try:
        merged["smtp_port"] = int(merged.get("smtp_port", 587))
    except Exception:
        merged["smtp_port"] = 587
        warns.append("SMTP ç«¯å£æ— æ•ˆï¼Œå·²å›é€€ 587")

    # Frequency
    freq = str(merged.get("frequency", "Manual") or "Manual")
    if freq not in ["Manual", "Daily", "Weekly"]:
        warns.append("frequency éæ³•ï¼Œå·²å›é€€ Manual")
        merged["frequency"] = "Manual"
    else:
        merged["frequency"] = freq

    # Trigger time
    trig = str(merged.get("trigger_time", "09:30") or "09:30")
    try:
        datetime.datetime.strptime(trig, "%H:%M")
        merged["trigger_time"] = trig
    except Exception:
        merged["trigger_time"] = "09:30"
        warns.append("è§¦å‘æ—¶é—´æ ¼å¼æ— æ•ˆï¼Œå·²å›é€€ 09:30")

    # Enabled flag
    merged["enabled"] = bool(merged.get("enabled", False))

    # Realtime alert controls
    merged["state_change_alert"] = bool(merged.get("state_change_alert", False))
    merged["vix_alert_enabled"] = bool(merged.get("vix_alert_enabled", False))
    try:
        merged["vix_alert_threshold"] = float(merged.get("vix_alert_threshold", 35))
    except Exception:
        merged["vix_alert_threshold"] = 35
        warns.append("VIX é˜ˆå€¼æ— æ•ˆï¼Œå·²å›é€€ 35")

    # Channels placeholder (Telegram / WeCom)
    channels = merged.get("channels", {}) or {}
    if not isinstance(channels, dict):
        channels = {}
    merged["channels"] = {
        "telegram_bot_token": channels.get("telegram_bot_token", ""),
        "telegram_chat_id": channels.get("telegram_chat_id", ""),
        "wechat_webhook": channels.get("wechat_webhook", ""),
    }

    return merged, issues, warns


def check_data_health(df_hist: pd.DataFrame, freshness_limit_days: int = 5):
    warnings = []
    latest_date = None
    freshness_days = None
    if df_hist is None or df_hist.empty:
        warnings.append("æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è¯„ä¼°æ–°é²œåº¦")
        return warnings, latest_date, freshness_days

    latest_date = df_hist.index[-1].date()
    freshness_days = (datetime.date.today() - latest_date).days
    if freshness_days > freshness_limit_days:
        warnings.append(f"æ•°æ®å·²æ»å {freshness_days} å¤©ï¼Œè¯·æ£€æŸ¥æ•°æ®æºæˆ–æ‰‹åŠ¨ä¸Šä¼ ã€‚")

    required_cols = ["State", "Sahm", "RateShock", "Corr", "VIX", "Trend_Bear", "YieldCurve"]
    missing = [c for c in required_cols if c not in df_hist.columns]
    if missing:
        warnings.append(f"ç¼ºå°‘å¿…è¦å­—æ®µ: {', '.join(missing)}")
    else:
        na_cols = [c for c in required_cols if df_hist[c].isna().any()]
        if na_cols:
            warnings.append(f"å­˜åœ¨ç©ºå€¼å­—æ®µ: {', '.join(na_cols)}ï¼Œå»ºè®®åˆ·æ–°æˆ–è¡¥é½æ•°æ®ã€‚")

    return warnings, latest_date, freshness_days


# --- Idempotent Daily Lock to Prevent Duplicate Sends ---
LOCK_DIR = os.path.join(os.path.dirname(__file__), ".locks")


def _ensure_lock_dir():
    try:
        os.makedirs(LOCK_DIR, exist_ok=True)
    except Exception as e:
        print(f"[Lock] Failed to ensure lock dir: {e}")


def acquire_daily_lock(date_str: str, ttl_minutes: int = 120) -> bool:
    """Create a dated lock file to avoid duplicate daily sends.
    Returns True if lock acquired; False if an unexpired lock already exists."""
    _ensure_lock_dir()
    lock_path = os.path.join(LOCK_DIR, f"alert_{date_str}.lock")
    now_ts = time.time()
    if os.path.exists(lock_path):
        try:
            with open(lock_path, "r") as f:
                ts = float(f.read().strip() or "0")
            if now_ts - ts < ttl_minutes * 60:
                return False
        except Exception:
            # If reading fails, overwrite to be safe
            pass
    try:
        with open(lock_path, "w") as f:
            f.write(str(now_ts))
    except Exception as e:
        print(f"[Lock] Failed to write lock file: {e}")
    return True


def release_daily_lock(date_str: str):
    """Optional: remove the lock file for the given date."""
    lock_path = os.path.join(LOCK_DIR, f"alert_{date_str}.lock")
    try:
        if os.path.exists(lock_path):
            os.remove(lock_path)
    except Exception as e:
        print(f"[Lock] Failed to release lock: {e}")


def is_pid_running(pid: int) -> bool:
    try:
        os.kill(pid, 0)
    except OSError:
        return False
    return True


def acquire_scheduler_lock(ttl_hours: int = 6) -> bool:
    now_ts = time.time()
    if os.path.exists(SCHEDULER_LOCK):
        try:
            with open(SCHEDULER_LOCK, "r") as f:
                content = f.read().strip().split(",")
            pid = int(content[0]) if content and content[0].isdigit() else None
            ts = float(content[1]) if len(content) > 1 else 0
            if pid and is_pid_running(pid):
                # If lock is fresh and pid alive, refuse
                if now_ts - ts < ttl_hours * 3600:
                    return False
        except Exception:
            pass
    try:
        with open(SCHEDULER_LOCK, "w") as f:
            f.write(f"{os.getpid()},{now_ts}")
        return True
    except Exception as e:
        print(f"[Lock] Failed to write scheduler lock: {e}")
        return False

def analyze_market_state_logic():
    """
    Core logic to fetch data and determine current market state.
    Returns: (success, result_dict_or_error_msg)
    """
    ensure_fred_cached()
    end = datetime.date.today()
    start = end - datetime.timedelta(days=365*3)
    
    # Re-use the robust fetcher
    df_hist, err = get_historical_macro_data(start, end)
    
    if df_hist.empty:
        return False, err

    data_warnings, latest_date, freshness_days = check_data_health(df_hist)
    for w in data_warnings:
        safe_warn(f"âš ï¸ æ•°æ®å¥åº·æé†’: {w}")
        log_event("WARN", "data_health", {"msg": w})
    
    # Extract latest state
    last_row = df_hist.iloc[-1]
    state = last_row['State']
    
    # --- Fetch Portfolio Asset Trends (Dual Momentum) ---
    asset_trends = {}
    try:
        check_assets = ['G3B.SI', 'LVHI', 'SRT.SI', 'AJBU.SI', 'IWY', 'MBH.SI', 'GSD.SI']
        trend_start = datetime.date.today() - datetime.timedelta(days=400)
        data_raw = fetch_yf_with_retry(check_assets, start=trend_start, auto_adjust=False)
        
        df_assets = pd.DataFrame()
        if data_raw is not None and not data_raw.empty:
            df_assets = normalize_yf_prices(data_raw)

        if not df_assets.empty:
            df_assets = df_assets.ffill()
            ma200 = df_assets.rolling(200).mean()
            
            latest_prices = df_assets.iloc[-1]
            latest_ma = ma200.iloc[-1]
            
            for t in check_assets:
                if t in df_assets.columns:
                    # Bearish if Price < MA200
                    try:
                        p = latest_prices[t]
                        m = latest_ma[t]
                        if pd.notna(p) and pd.notna(m):
                            asset_trends[t] = bool(p < m)
                        else:
                            asset_trends[t] = False
                    except:
                        asset_trends[t] = False
    except Exception as e:
        print(f"Error fetching asset trends: {e}")
        log_event("ERROR", "asset_trend_fetch_failed", {"err": str(e)})

    # Logic helpers
    yc_series = df_hist['YieldCurve']
    yc_un_invert = False
    if len(yc_series) > 126:
        recent_min = yc_series.iloc[-126:].min()
        current_yc = yc_series.iloc[-1]
        yc_un_invert = (current_yc < 0.2) and (recent_min < -0.2)

    factor_cols = [c for c in ["VIX", "YieldCurve", "Corr", "Sahm", "RateShock"] if c in df_hist.columns]
    factor_trends = df_hist[factor_cols].tail(90) if factor_cols else pd.DataFrame()

    metrics = {
        'date': last_row.name.strftime('%Y-%m-%d'),
        'state': state,
        'tnx_roc': last_row['RateShock'],
        'rate_shock': last_row['RateShock'] > 0.20,
        'sahm': last_row['Sahm'],
        'recession': last_row['Sahm'] >= 0.50,
        'corr': last_row['Corr'],
        'corr_broken': last_row['Corr'] > 0.30,
        'vix': last_row['VIX'],
        'fear': last_row['VIX'] > 32,
        'yield_curve': last_row['YieldCurve'],
        'yc_un_invert': yc_un_invert,
        'gold_bear': last_row['Gold_Bear'],
        'value_regime': last_row['Value_Regime'],
        'asset_trends': asset_trends,
        'freshness_days': freshness_days,
        'latest_date': last_row.name.date() if hasattr(last_row, 'name') else None,
        'data_warnings': data_warnings,
        'factor_trends': factor_trends,
        'fetch_ts': datetime.datetime.now().isoformat(timespec='seconds')
    }
    
    return True, metrics


@st.cache_data(ttl=300, show_spinner=False)
def analyze_market_state_logic_cached():
    return analyze_market_state_logic()


def generate_email_risk_exposure(targets):
    """
    ç”Ÿæˆé‚®ä»¶ç”¨çš„é£é™©æš´éœ²åˆ†æHTML
    """
    # è®¡ç®—ç›®æ ‡ç±»åˆ«æƒé‡
    target_categories = {}
    for tkr, w in targets.items():
        cat = ASSET_CATEGORIES.get(tkr, {}).get('category', 'å…¶ä»–')
        target_categories[cat] = target_categories.get(cat, 0) + w
    
    cat_colors = {
        'æƒç›Š': '#f5222d', 'å›ºæ”¶': '#1890ff', 'å•†å“': '#faad14', 
        'å¯¹å†²': '#52c41a', 'å¦ç±»': '#722ed1', 'å…¶ä»–': '#999'
    }
    
    bars_html = ""
    for cat in ['æƒç›Š', 'å›ºæ”¶', 'å•†å“', 'å¯¹å†²', 'å¦ç±»']:
        w = target_categories.get(cat, 0)
        if w > 0:
            bar_color = cat_colors.get(cat, '#999')
            bars_html += f"""
            <div style="margin-bottom:8px;">
                <span style="display:inline-block;width:70px;font-size:13px;color:#666;">{cat}</span>
                <span style="display:inline-block;width:150px;background:#e8e8e8;height:18px;border-radius:4px;vertical-align:middle;">
                    <span style="display:block;width:{w*100}%;height:100%;background:{bar_color};border-radius:4px;"></span>
                </span>
                <span style="font-size:13px;margin-left:10px;font-weight:600;">{w*100:.1f}%</span>
            </div>
            """
    
    return bars_html


def generate_email_v15_status(metrics, state, change_info=None):
    """
    ç”Ÿæˆv1.5ä¼˜åŒ–æœºåˆ¶çŠ¶æ€çš„é‚®ä»¶HTML
    change_info: åŒ…å« days_in_state ç­‰ä¿¡æ¯çš„å­—å…¸
    """
    vix = metrics.get('vix', 15)
    sahm = metrics.get('sahm', 0)
    corr = metrics.get('corr', 0)
    yc = metrics.get('yield_curve', 0)
    yc_un_invert = metrics.get('yc_un_invert', False)
    
    status_items = []
    
    # 0. ä¿¡å·æŒç»­å¤©æ•°ä¸ç¡®è®¤çŠ¶æ€
    days_in_state = change_info.get('days_in_state') if change_info else None
    if days_in_state is not None:
        if days_in_state <= SIGNAL_CONFIRM_DAYS:
            confirm_text = f"ç¡®è®¤ä¸­ ({days_in_state}/{SIGNAL_CONFIRM_DAYS}å¤©)"
            confirm_color = "#fa8c16"  # æ©™è‰²è­¦ç¤º
        else:
            confirm_text = f"å·²ç¡®è®¤ ({days_in_state}å¤©)"
            confirm_color = "#52c41a"  # ç»¿è‰²
        
        status_items.append({
            'name': 'ğŸ”„ ä¿¡å·çŠ¶æ€',
            'value': confirm_text,
            'color': confirm_color
        })
    
    # 1. ç°é‡‘ç¼“å†²çŠ¶æ€
    if state == "EXTREME_ACCUMULATION":
        cash_buffer = 0
        cash_color = "#52c41a"
    else:
        cash_buffer = CASH_BUFFER_BASE
        if vix > CASH_BUFFER_VIX_THRESHOLD:
            extra_cash = min((vix - CASH_BUFFER_VIX_THRESHOLD) / 5 * CASH_BUFFER_VIX_SCALE, 
                             CASH_BUFFER_MAX - CASH_BUFFER_BASE)
            cash_buffer = CASH_BUFFER_BASE + extra_cash
        cash_color = "#faad14" if cash_buffer > CASH_BUFFER_BASE else "#52c41a"
    
    status_items.append({
        'name': 'ğŸ’µ ç°é‡‘ç¼“å†²',
        'value': f"{cash_buffer*100:.1f}%",
        'color': cash_color
    })
    
    # 2. CAUTIOUS_VOL VIXåˆ†å±‚
    if state == "CAUTIOUS_VOL":
        if vix >= 30:
            tier_text = "Tier3 (IWYâ†“10%)"
            tier_color = "#f5222d"
        elif vix >= 25:
            tier_text = "Tier2 (IWYâ†“20%)"
            tier_color = "#fa8c16"
        else:
            tier_text = "Tier1 (IWY 30%)"
            tier_color = "#faad14"
    else:
        tier_text = "N/A"
        tier_color = "#999"
    
    status_items.append({
        'name': 'ğŸ“Š VIXåˆ†å±‚',
        'value': tier_text,
        'color': tier_color
    })
    
    # 3. ç›¸å…³æ€§æ¸è¿›å“åº”
    if corr > CORR_HIGH_THRESHOLD:
        corr_text = f"æœ€å¤§è°ƒæ•´ {CORR_MAX_REALLOC*100:.0f}%"
        corr_color = "#f5222d"
    elif corr > CORR_MID_THRESHOLD:
        adjustment_pct = (corr - CORR_MID_THRESHOLD) / (CORR_HIGH_THRESHOLD - CORR_MID_THRESHOLD)
        realloc = adjustment_pct * CORR_MAX_REALLOC
        corr_text = f"æ¸è¿› {realloc*100:.1f}%"
        corr_color = "#fa8c16"
    else:
        corr_text = "æ­£å¸¸"
        corr_color = "#52c41a"
    
    status_items.append({
        'name': 'ğŸ”— ç›¸å…³æ€§å“åº”',
        'value': corr_text,
        'color': corr_color
    })
    
    # 4. Sahmé¢„è­¦
    if sahm >= SAHM_EARLY_WARNING_HI:
        sahm_text = "è¡°é€€ç¡®è®¤"
        sahm_color = "#f5222d"
    elif sahm >= SAHM_EARLY_WARNING_LO:
        reduction_pct = int((sahm - SAHM_EARLY_WARNING_LO) / (SAHM_EARLY_WARNING_HI - SAHM_EARLY_WARNING_LO) * SAHM_REDUCTION_RATE * 100)
        sahm_text = f"é¢„è­¦ -{reduction_pct}%"
        sahm_color = "#fa8c16"
    else:
        sahm_text = "æ­£å¸¸"
        sahm_color = "#52c41a"
    
    status_items.append({
        'name': 'ğŸ“‰ Sahmé¢„è­¦',
        'value': sahm_text,
        'color': sahm_color
    })
    
    # 5. æ›²çº¿ä¿æŠ¤
    if yc < 0:
        yc_text = "å€’æŒ‚ä¸­"
        yc_color = "#f5222d"
    elif yc_un_invert:
        yc_text = f"è§£å€’æŒ‚ä¿æŠ¤"
        yc_color = "#fa8c16"
    else:
        yc_text = "æ­£å¸¸"
        yc_color = "#52c41a"
    
    status_items.append({
        'name': 'ğŸ“ˆ æ›²çº¿ä¿æŠ¤',
        'value': yc_text,
        'color': yc_color
    })
    
    # 6. å¸‚åœºå¹¿åº¦ä¼°ç®—
    asset_trends = metrics.get('asset_trends', {})
    if asset_trends:
        bullish_count = sum(1 for bear in asset_trends.values() if not bear)
        total_count = len(asset_trends)
        breadth = bullish_count / total_count if total_count > 0 else 0.5
    else:
        breadth = 0.5
    
    if breadth < MARKET_BREADTH_LOW:
        breadth_text = f"ä½ ({breadth*100:.0f}%)"
        breadth_color = "#f5222d"
    elif breadth < MARKET_BREADTH_MID:
        breadth_text = f"ä¸€èˆ¬ ({breadth*100:.0f}%)"
        breadth_color = "#fa8c16"
    else:
        breadth_text = f"æ­£å¸¸ ({breadth*100:.0f}%)"
        breadth_color = "#52c41a"
    
    status_items.append({
        'name': 'ğŸ“Š å¸‚åœºå¹¿åº¦',
        'value': breadth_text,
        'color': breadth_color
    })
    
    # æ„å»ºHTML
    items_html = ""
    for i, item in enumerate(status_items):
        items_html += f"""
        <td style="padding:8px 12px;text-align:center;border-right:{'1px solid #e5e7eb' if i < len(status_items)-1 else 'none'};">
            <div style="font-size:11px;color:#666;">{item['name']}</div>
            <div style="font-size:13px;font-weight:600;color:{item['color']};margin-top:2px;">{item['value']}</div>
        </td>
        """
    
    return f"""
    <div style="background:#f8fafc;border:1px solid #e5e7eb;border-radius:10px;padding:12px;margin:12px 0;">
        <div style="font-weight:600;color:#374151;margin-bottom:8px;font-size:14px;">âš™ï¸ v1.5 ä¼˜åŒ–æœºåˆ¶çŠ¶æ€</div>
        <table style="width:100%;border-collapse:collapse;">
            <tr>{items_html}</tr>
        </table>
    </div>
    """


def generate_email_execution_tips(metrics, state):
    """
    ç”Ÿæˆé‚®ä»¶ç”¨çš„æ‰§è¡Œå»ºè®®HTML
    """
    tips = []
    vix = metrics.get('vix')
    sahm = metrics.get('sahm')
    corr = metrics.get('corr')
    yc_val = metrics.get('yield_curve', 0)
    
    # 1. VIXç›¸å…³æç¤º
    if vix is not None:
        if vix > VIX_SMOOTH_END:
            reduction_pct = int(VIX_MAX_REDUCTION * 100)
            tips.append({
                'icon': 'ğŸ“Š',
                'title': 'é«˜æ³¢åŠ¨ç‡è­¦å‘Š',
                'content': f'VIX={vix:.1f} å¤„äºé«˜ä½ï¼Œå»ºè®®æŒ‰ç›®æ ‡é…ç½®çš„ {100-reduction_pct}% æ‰§è¡Œï¼Œå‰©ä½™èµ„é‡‘æŒæœ‰ç°é‡‘æˆ– WTMFã€‚',
                'color': '#cf1322',
                'bg': '#fff2f0'
            })
        elif vix > VIX_SMOOTH_START:
            reduction = (vix - VIX_SMOOTH_START) / (VIX_SMOOTH_END - VIX_SMOOTH_START) * VIX_MAX_REDUCTION
            exec_pct = int((1 - reduction) * 100)
            tips.append({
                'icon': 'ğŸ“Š',
                'title': 'æ³¢åŠ¨ç‡åé«˜',
                'content': f'VIX={vix:.1f}ï¼Œå¯è€ƒè™‘æŒ‰ç›®æ ‡é…ç½®çš„ {exec_pct}% æ‰§è¡Œï¼Œç•™ {100-exec_pct}% ç°é‡‘ç¼“å†²ã€‚',
                'color': '#ad6800',
                'bg': '#fffbe6'
            })
    
    # 2. Sahm Ruleé¢„è­¦
    if sahm is not None and SAHM_EARLY_WARNING_LO <= sahm < SAHM_EARLY_WARNING_HI:
        reduction_pct = int((sahm - SAHM_EARLY_WARNING_LO) / (SAHM_EARLY_WARNING_HI - SAHM_EARLY_WARNING_LO) * SAHM_REDUCTION_RATE * 100)
        tips.append({
            'icon': 'ğŸ“‰',
            'title': 'Sahmé¢„è­¦åŒºé—´',
            'content': f'Sahm Rule={sahm:.2f} å¤„äºé¢„è­¦åŒºé—´ ({SAHM_EARLY_WARNING_LO}-{SAHM_EARLY_WARNING_HI})ï¼ŒIWYå·²é¢„é˜²æ€§å‡ä»“{reduction_pct}%ã€‚',
            'color': '#ad6800',
            'bg': '#fffbe6'
        })
    
    # 3. ç›¸å…³æ€§æ¸è¿›å“åº”
    if corr is not None and corr > CORR_MID_THRESHOLD:
        if corr > CORR_HIGH_THRESHOLD:
            tips.append({
                'icon': 'ğŸ”—',
                'title': 'è‚¡å€ºç›¸å…³æ€§å¤±æ•ˆ',
                'content': f'Corr={corr:.2f} è¶…è¿‡é˜ˆå€¼ï¼Œå€ºåˆ¸å¯¹å†²æ•ˆæœå‡å¼±ï¼ŒMBHå·²è½¬ç§»{CORR_MAX_REALLOC*100:.0f}%è‡³WTMF/é»„é‡‘ã€‚',
                'color': '#cf1322',
                'bg': '#fff2f0'
            })
        else:
            adjustment_pct = (corr - CORR_MID_THRESHOLD) / (CORR_HIGH_THRESHOLD - CORR_MID_THRESHOLD)
            realloc = adjustment_pct * CORR_MAX_REALLOC
            tips.append({
                'icon': 'ğŸ”—',
                'title': 'ç›¸å…³æ€§æ¸è¿›å“åº”',
                'content': f'Corr={corr:.2f} å¤„äºå…³æ³¨åŒºé—´ï¼ŒMBHæ¸è¿›è½¬ç§»{realloc*100:.1f}%è‡³éç›¸å…³èµ„äº§ã€‚',
                'color': '#ad6800',
                'bg': '#fffbe6'
            })
    
    # 4. æ”¶ç›Šç‡æ›²çº¿æç¤º
    if yc_val < 0:
        tips.append({
            'icon': 'ğŸ“ˆ',
            'title': 'æ”¶ç›Šç‡æ›²çº¿å€’æŒ‚',
            'content': f'10Y-2Y={yc_val:.2f}%ï¼Œæ›²çº¿å€’æŒ‚ä¸­ï¼Œå€ºåˆ¸é…ç½®éœ€è°¨æ…ï¼Œä¼˜å…ˆé€‰æ‹©çŸ­ä¹…æœŸæˆ–WTMFã€‚',
            'color': '#ad6800',
            'bg': '#fffbe6'
        })
    elif metrics.get('yc_un_invert', False):
        tips.append({
            'icon': 'âš ï¸',
            'title': 'è§£å€’æŒ‚ä¿æŠ¤æœŸ',
            'content': f'æ”¶ç›Šç‡æ›²çº¿åˆšè½¬æ­£ï¼Œå†å²ä¸Šæ­¤é˜¶æ®µè¡°é€€é£é™©ä»é«˜ï¼ŒIWYå·²é˜²å¾¡æ€§å‡ä»“{YC_UNINVERT_REDUCTION*100:.0f}%ã€‚',
            'color': '#ad6800',
            'bg': '#fffbe6'
        })
    
    # 5. æç«¯çŠ¶æ€æç¤º
    if state == "EXTREME_ACCUMULATION":
        tips.append({
            'icon': 'âš¡',
            'title': 'æŠ„åº•çŠ¶æ€æ³¨æ„',
            'content': 'å½“å‰ä¸ºæç«¯æŠ„åº•çŠ¶æ€ï¼Œç°é‡‘ç¼“å†²å·²å…³é—­ã€‚å»ºè®®åˆ†æ‰¹å»ºä»“ï¼šé¦–æ¬¡40% â†’ åå¼¹ç¡®è®¤å60% â†’ è¶‹åŠ¿ç¡®ç«‹å75%ã€‚',
            'color': '#ad6800',
            'bg': '#fffbe6'
        })
    elif state in ["DEFLATION_RECESSION", "INFLATION_SHOCK"]:
        tips.append({
            'icon': 'ğŸ›¡ï¸',
            'title': 'é˜²å¾¡æ¨¡å¼æé†’',
            'content': 'å½“å‰å¤„äºå±æœºçŠ¶æ€ï¼Œå»ºè®®ä¸¥æ ¼æ‰§è¡Œç›®æ ‡é…ç½®ï¼Œä¼˜å…ˆä¿æŠ¤æœ¬é‡‘ï¼Œé¿å…æŠ„åº•å†²åŠ¨ã€‚',
            'color': '#cf1322',
            'bg': '#fff2f0'
        })
    elif state == "CAUTIOUS_VOL":
        vix_tier = "Tier1" if vix < 25 else ("Tier2" if vix < 30 else "Tier3")
        tips.append({
            'icon': 'âš¡',
            'title': f'é«˜æ³¢éœ‡è¡ ({vix_tier})',
            'content': f'VIX={vix:.1f}ï¼Œå·²å¯ç”¨åˆ†å±‚é…ç½®ã€‚ä¿æŒæ ¸å¿ƒæˆé•¿ï¼ŒWTMFå¯¹å†²æ³¢åŠ¨ã€‚',
            'color': '#ad6800',
            'bg': '#fffbe6'
        })
    
    # 6. é€šç”¨æ‰§è¡Œå»ºè®®
    tips.append({
        'icon': 'ğŸ“',
        'title': 'å†å¹³è¡¡å»ºè®®',
        'content': f'å•ä¸€èµ„äº§åç¦»>{REBALANCE_THRESHOLD*100:.0f}%æ—¶å†è°ƒä»“ï¼›å¤§å¹…è°ƒä»“å»ºè®®åˆ†{STATE_TRANSITION_DAYS}å¤©æ‰§è¡Œï¼Œä¿¡å·éœ€è¿ç»­{SIGNAL_CONFIRM_DAYS}å¤©ç¡®è®¤ã€‚',
        'color': '#0050b3',
        'bg': '#e6f7ff'
    })
    
    tips_html = ""
    for tip in tips:
        tips_html += f"""
        <div style="background:{tip['bg']};border-radius:8px;padding:10px 14px;margin-bottom:8px;">
            <div style="font-weight:600;color:{tip['color']};margin-bottom:2px;">{tip['icon']} {tip['title']}</div>
            <div style="color:#333;font-size:13px;line-height:1.4;">{tip['content']}</div>
        </div>
        """
    
    return tips_html


def render_email_html(metrics, targets, adjustments, s_conf, sent_at, report_date, change_info=None):
    target_rows = ""
    for t, w in targets.items():
        if w > 0:
            target_rows += f"<tr><td>{ASSET_NAMES.get(t, t)}</td><td style='color:#555'>{t}</td><td><b>{w*100:.1f}%</b></td></tr>"

    if adjustments:
        adj_list = "".join([f"<li>{r}</li>" for r in adjustments])
        adj_html = f"""
        <div style="background:#fff6f2;border:1px solid #ffd7c2;border-radius:10px;padding:14px 16px;margin:12px 0;">
            <div style="font-weight:600;color:#d93025;margin-bottom:6px;">ğŸ”§ åŠ¨æ€é£æ§è§¦å‘</div>
            <ul style="line-height:1.6;margin:0;color:#b23c17;">{adj_list}</ul>
        </div>
        """
    else:
        adj_html = """
        <div style="background:#f6ffed;border:1px solid #b7eb8f;border-radius:10px;padding:14px 16px;margin:12px 0;">
            <div style="font-weight:600;color:#237804;">âœ… å½“å‰æœªè§¦å‘é¢å¤–é£æ§</div>
        </div>
        """

    yc_val = metrics.get('yield_curve', 0)
    state = metrics.get('state', 'NEUTRAL')
    
    # ä¿¡å·æŒç»­å¤©æ•°ä¿¡æ¯
    days_in_state = change_info.get('days_in_state') if change_info else None
    days_info = ""
    if days_in_state is not None:
        if days_in_state <= SIGNAL_CONFIRM_DAYS:
            days_info = f" (ç¡®è®¤ä¸­ {days_in_state}/{SIGNAL_CONFIRM_DAYS}å¤©)"
        else:
            days_info = f" (æŒç»­{days_in_state}å¤©)"
    
    summary_points = [
        f"æ•°æ®æˆªè‡³ {report_date}",
        f"çŠ¶æ€: {s_conf['display']}{days_info}",
        f"VIX {metrics['vix']:.1f} ({'âš ï¸ é«˜æ³¢åŠ¨' if metrics['fear'] else 'âœ… æ­£å¸¸'})",
        f"10Y-2Y {yc_val:.2f}% ({'âš ï¸ å€’æŒ‚/è§£å€’æŒ‚' if (yc_val < 0 or metrics.get('yc_un_invert', False)) else 'âœ… æ­£å¸¸'})",
        f"Sahm {metrics['sahm']:.2f} ({'âš ï¸ è¡°é€€ä¿¡å·' if metrics['recession'] else 'âœ… æœªè§¦å‘'})"
    ]
    summary_html = "".join([f"<span style='display:inline-block;background:#f0f4ff;color:#1a73e8;padding:6px 10px;border-radius:20px;margin:4px 4px 0 0;font-size:13px;'>{p}</span>" for p in summary_points])
    
    # ç”Ÿæˆv1.5ä¼˜åŒ–æœºåˆ¶çŠ¶æ€
    v15_status_html = generate_email_v15_status(metrics, state, change_info)
    
    # ç”Ÿæˆé£é™©æš´éœ²åˆ†æ
    risk_exposure_html = generate_email_risk_exposure(targets)
    
    # ç”Ÿæˆæ‰§è¡Œå»ºè®®
    execution_tips_html = generate_email_execution_tips(metrics, state)

    return f"""
    <html>
    <body style=\"font-family: 'Helvetica Neue', Arial, sans-serif; color: #1f2937; background:#f7f8fa;\">
        <div style=\"max-width: 680px; margin: 24px auto; background:#fff; border:1px solid #e5e7eb; border-radius:14px; overflow:hidden; box-shadow:0 10px 30px rgba(0,0,0,0.05);\">
            <div style=\"padding:22px 24px; background: linear-gradient(135deg, {s_conf['border_color']} 0%, #1f1f1f 100%); color:#fff;\">
                <div style=\"font-size:13px; opacity:0.85;\">æ•°æ®æˆªè‡³ {report_date}</div>
                <div style=\"font-size:12px; opacity:0.75;\">å‘é€æ—¶é—´ {sent_at}</div>
                <h2 style=\"margin:6px 0 4px 0; font-weight:700; letter-spacing:0.3px;\">{s_conf['icon']} å®è§‚ç­–ç•¥å¿«æŠ¥ v1.5</h2>
                <div style=\"opacity:0.9; line-height:1.5; font-size:14px;\">{s_conf['desc']}</div>
            </div>

            <div style=\"padding:22px 24px;\">
                <div style=\"margin-bottom:12px;\">{summary_html}</div>
                
                {v15_status_html}

                <h3 style=\"margin:18px 0 10px 0; font-size:16px;\">ğŸ“ˆ æ ¸å¿ƒæŒ‡æ ‡ (Key Metrics)</h3>
                <table style=\"width:100%; border-collapse:separate; border-spacing:0 8px; font-size:14px;\">
                    <tr style=\"background:#f9fafb;\"><td style=\"padding:10px 12px; border-radius:10px 0 0 10px;\">åˆ©ç‡å†²å‡»</td><td style=\"padding:10px 12px; border-radius:0 10px 10px 0; font-weight:600; color:{'#d93025' if metrics['rate_shock'] else '#15803d'};\">{metrics['tnx_roc']:.1%} ({'âš ï¸ è§¦å‘' if metrics['rate_shock'] else 'âœ… å®‰å…¨'})</td></tr>
                    <tr style=\"background:#f9fafb;\"><td style=\"padding:10px 12px; border-radius:10px 0 0 10px;\">Sahm Rule</td><td style=\"padding:10px 12px; border-radius:0 10px 10px 0; font-weight:600; color:{'#d93025' if metrics['recession'] else '#15803d'};\">{metrics['sahm']:.2f} ({'âš ï¸ è§¦å‘' if metrics['recession'] else 'âœ… å®‰å…¨'})</td></tr>
                    <tr style=\"background:#f9fafb;\"><td style=\"padding:10px 12px; border-radius:10px 0 0 10px;\">VIX</td><td style=\"padding:10px 12px; border-radius:0 10px 10px 0; font-weight:600; color:{'#ea580c' if metrics['fear'] else '#15803d'};\">{metrics['vix']:.1f} ({'âš ï¸ ææ…Œ' if metrics['fear'] else 'âœ… æ­£å¸¸'})</td></tr>
                    <tr style=\"background:#f9fafb;\"><td style=\"padding:10px 12px; border-radius:10px 0 0 10px;\">è‚¡å€ºç›¸å…³æ€§</td><td style=\"padding:10px 12px; border-radius:0 10px 10px 0; font-weight:600; color:{'#d93025' if metrics['corr_broken'] else '#15803d'};\">{metrics['corr']:.2f} ({'âš ï¸ å¤±æ•ˆ' if metrics['corr_broken'] else 'âœ… æ­£å¸¸'})</td></tr>
                    <tr style=\"background:#f9fafb;\"><td style=\"padding:10px 12px; border-radius:10px 0 0 10px;\">æ”¶ç›Šç‡æ›²çº¿ (10Y-2Y)</td><td style=\"padding:10px 12px; border-radius:0 10px 10px 0; font-weight:600; color:{'#d93025' if (yc_val < 0 or metrics.get('yc_un_invert', False)) else '#15803d'};\">{yc_val:.2f}%</td></tr>
                </table>

                <h3 style=\"margin:20px 0 10px 0; font-size:16px;\">ğŸ¯ æˆ˜æœ¯æ¦‚è§ˆ (Tactical)</h3>
                <ul style=\"line-height:1.6; margin-top:6px; padding-left:18px; color:#374151;\">
                    <li><b>é»„é‡‘è¶‹åŠ¿:</b> {'ğŸ» å›é¿' if metrics['gold_bear'] else 'ğŸ‚ æŒæœ‰/å¢é…'}</li>
                    <li><b>é£æ ¼è½®åŠ¨:</b> {'ğŸ§± Value ä»·å€¼å ä¼˜' if metrics['value_regime'] else 'ğŸš€ Growth æˆé•¿å ä¼˜'}</li>
                </ul>

                {adj_html}

                <h3 style=\"margin:20px 0 10px 0; font-size:16px;\">ğŸ“Š å»ºè®®é…ç½® (Target Allocation)</h3>
                <table border=\"0\" cellpadding=\"10\" cellspacing=\"0\" style=\"width: 100%; border-collapse: collapse; margin-top: 8px; font-size:14px;\">
                    <tr style=\"background-color: #f3f4f6; text-align: left;\">
                        <th style=\"border-bottom: 2px solid #e5e7eb;\">èµ„äº§åç§°</th>
                        <th style=\"border-bottom: 2px solid #e5e7eb;\">ä»£ç </th>
                        <th style=\"border-bottom: 2px solid #e5e7eb;\">ç›®æ ‡ä»“ä½</th>
                    </tr>
                    {target_rows}
                </table>
                
                <h3 style=\"margin:20px 0 10px 0; font-size:16px;\">ğŸ¯ é£é™©æš´éœ²åˆ†æ (Risk Exposure)</h3>
                <div style=\"background:#f9fafb;border-radius:10px;padding:14px 16px;margin:8px 0;\">
                    {risk_exposure_html}
                </div>
                
                <h3 style=\"margin:20px 0 10px 0; font-size:16px;\">ğŸ’¡ æ‰§è¡Œå»ºè®® (Execution Tips)</h3>
                {execution_tips_html}

                <p style=\"font-size: 12px; color: #6b7280; margin-top: 26px; text-align: center; border-top: 1px solid #e5e7eb; padding-top: 10px;\">
                    æ­¤é‚®ä»¶ç”± Stock Strategy Analyzer v1.5 è‡ªåŠ¨ç”Ÿæˆï¼Œä¾›å‚è€ƒï¼Œä¸æ„æˆæŠ•èµ„å»ºè®®ã€‚
                </p>
            </div>
        </div>
    </body>
    </html>
    """

def send_strategy_email(metrics, config):
    """å‘é€ç­–ç•¥åˆ†æé‚®ä»¶ï¼Œè¿”å› (success, message)ã€‚"""
    ensure_fred_cached()
    email_to = str(config.get("email_to", "")).strip()
    email_from = str(config.get("email_from", "")).strip()
    email_pwd = config.get("email_pwd", "")
    smtp_server = str(config.get("smtp_server", "smtp.gmail.com")).strip() or "smtp.gmail.com"
    try:
        smtp_port = int(config.get("smtp_port", 587))
    except Exception:
        smtp_port = 587

    if not email_to or not email_from or not email_pwd:
        log_event("ERROR", "email config incomplete", {"to": email_to, "from": email_from})
        return False, "é‚®ç®±é…ç½®ä¸å®Œæ•´"

    state = metrics['state']
    s_conf = MACRO_STATES.get(state, MACRO_STATES["NEUTRAL"])
    sent_at = datetime.datetime.now().strftime('%Y-%m-%d %H:%M')
    report_date = metrics.get('date', sent_at.split(' ')[0])
    
    # è®¡ç®—ä¿¡å·æŒç»­å¤©æ•°
    history = load_state_history()
    current_date = metrics.get('latest_date')
    if current_date is None:
        try:
            current_date = datetime.date.fromisoformat(report_date)
        except:
            current_date = datetime.date.today()
    change_info = get_state_change_info(history, state, current_date)
    
    targets = get_target_percentages(
        state,
        gold_bear=metrics['gold_bear'],
        value_regime=metrics['value_regime'],
        asset_trends=metrics.get('asset_trends', {}),
        vix=metrics.get('vix'),
        yield_curve=metrics.get('yield_curve'),
        sahm=metrics.get('sahm'),
        corr=metrics.get('corr'),
        yc_recently_inverted=metrics.get('yc_un_invert', False)
    )

    adjustments = get_adjustment_reasons(
        state,
        gold_bear=metrics['gold_bear'],
        value_regime=metrics['value_regime'],
        asset_trends=metrics.get('asset_trends', {}),
        vix=metrics.get('vix'),
        yield_curve=metrics.get('yield_curve'),
        sahm=metrics.get('sahm'),
        corr=metrics.get('corr'),
        yc_recently_inverted=metrics.get('yc_un_invert', False)
    )

    html_content = render_email_html(metrics, targets, adjustments, s_conf, sent_at, report_date, change_info)

    msg = MIMEMultipart()
    msg['From'] = email_from
    msg['To'] = email_to
    msg['Subject'] = f"[{state}] å®è§‚ç­–ç•¥çŠ¶æ€æ›´æ–° - {sent_at} (æ•°æ®æˆªè‡³ {report_date})"
    msg.attach(MIMEText(html_content, 'html'))
    
    try:
        timeout = 20
        use_ssl = int(smtp_port) == 465
        if use_ssl:
            server = smtplib.SMTP_SSL(smtp_server, smtp_port, timeout=timeout)
        else:
            server = smtplib.SMTP(smtp_server, smtp_port, timeout=timeout)
            try:
                server.starttls()
            except Exception as e:
                server.quit()
                return False, f"TLS æ¡æ‰‹å¤±è´¥: {e}"
        server.login(email_from, email_pwd)
        server.send_message(msg)
        server.quit()
        log_event("INFO", "email sent", {"to": email_to, "state": state, "report_date": report_date})
        return True, "é‚®ä»¶å‘é€æˆåŠŸ"
    except Exception as e:
        log_event("ERROR", "email send failed", {"err": str(e)})
        return False, f"é‚®ä»¶å‘é€å¤±è´¥: {str(e)}"

# --- Background Scheduler (Lightweight) ---

scheduler_thread = None

@st.cache_resource
def start_scheduler_service():
    """
    Starts the background scheduler in a singleton thread.
    Uses @st.cache_resource to ensure only one thread runs per server process,
    preventing duplicate emails when multiple tabs are open.
    """
    global scheduler_thread
    if scheduler_thread:
        return scheduler_thread
    if st.session_state.get("_scheduler_started"):
        return scheduler_thread

    if not acquire_scheduler_lock(ttl_hours=6):
        print("[Scheduler] Lock held by another process/session; skip start.")
        return scheduler_thread

    def run_scheduler_check():
        """Checks if alert needs to be sent. Runs in background thread."""
        while True:
            try:
                cfg = load_alert_config() or {}
                enabled = bool(cfg.get("enabled", False))
                freq = str(cfg.get("frequency", "Manual") or "Manual")
                if enabled and freq != "Manual":
                    sg_tz = datetime.timezone(datetime.timedelta(hours=8))
                    now = datetime.datetime.now(sg_tz)
                    trigger_hm = str(cfg.get("trigger_time", "09:30") or "09:30")
                    last_run_str = str(cfg.get("last_run", "") or "")
                    
                    should_run = False
                    today_str = now.strftime('%Y-%m-%d')
                    
                    # Simple check: Is it past trigger time AND haven't run today?
                    try:
                        trigger_dt = datetime.datetime.strptime(f"{today_str} {trigger_hm}", "%Y-%m-%d %H:%M").replace(tzinfo=sg_tz)
                    except Exception:
                        # Fallback if time parse fails
                        trigger_dt = datetime.datetime.strptime(f"{today_str} 09:30", "%Y-%m-%d %H:%M").replace(tzinfo=sg_tz)
                    
                    if now >= trigger_dt:
                        # Check frequency
                        if freq == "Daily":
                            if last_run_str != today_str:
                                should_run = True
                        elif freq == "Weekly":
                            # Assume Monday is trigger day (weekday=0)
                            if now.weekday() == 0 and last_run_str != today_str:
                                should_run = True
                    
                        if should_run:
                            # Idempotent guard: prevent duplicate sends across threads/processes (24h)
                            if not acquire_daily_lock(today_str, ttl_minutes=1440):
                                log_event("WARN", f"Skip duplicate send for {today_str}")
                            else:
                                log_event("INFO", "Triggering auto-analysis", {"now": str(now)})
                                success, res = analyze_market_state_logic()
                                if success:
                                    email_ok, msg = send_strategy_email(res, cfg)
                                    if email_ok:
                                        log_event("INFO", "Email sent", {"to": cfg.get("email_to")})
                                        cfg = load_alert_config()
                                        cfg["last_run"] = today_str
                                        save_alert_config(cfg)
                                    else:
                                        log_event("ERROR", "Email failed", {"err": msg})
                                else:
                                    log_event("ERROR", "Analysis failed", {"err": res})
            except Exception as e:
                log_event("ERROR", "Scheduler loop error", {"err": str(e)})
            
            time.sleep(60) # Check every minute

    # Create and start the thread
    t = threading.Thread(target=run_scheduler_check, daemon=True)
    t.start()
    scheduler_thread = t
    st.session_state["_scheduler_started"] = True
    print("[System] Global background scheduler service started.")
    return scheduler_thread

# Start scheduler (Singleton)
if __name__ == "__main__":
    start_scheduler_service()

# --- Shared Logic for Backtest & State Machine ---

def base_allocation(s, value_regime=False, vix=None):
    """
    åŸºç¡€èµ„äº§é…ç½®çŸ©é˜µ
    v1.5: CAUTIOUS_VOL çŠ¶æ€æ”¯æŒVIXåˆ†å±‚é…ç½®
    """
    if s == "INFLATION_SHOCK":
        return {
            'IWY': 0.00, 'WTMF': 0.50, 'LVHI': 0.15,
            'G3B.SI': 0.00, 'MBH.SI': 0.00, 'GSD.SI': 0.25,
            'SRT.SI': 0.00, 'AJBU.SI': 0.10
        }
    if s == "DEFLATION_RECESSION":
        return {
            'IWY': 0.05, 'WTMF': 0.20, 'LVHI': 0.05,
            'G3B.SI': 0.00, 'MBH.SI': 0.40, 'GSD.SI': 0.25,
            'SRT.SI': 0.00, 'AJBU.SI': 0.05
        }
    if s == "EXTREME_ACCUMULATION":
        # v1.7: æç«¯æŠ„åº•ï¼Œçº¯æˆé•¿é…ç½®
        return {
            'IWY': 0.85, 'WTMF': 0.00, 'LVHI': 0.00,  # IWY: 0.80â†’0.85
            'G3B.SI': 0.05, 'MBH.SI': 0.00, 'GSD.SI': 0.00,  # å®Œå…¨å–æ¶ˆé¿é™©èµ„äº§
            'SRT.SI': 0.05, 'AJBU.SI': 0.05  # REITsä½œä¸ºæ”¶ç›Šè¡¥å……
        }
    if s == "CAUTIOUS_TREND":
        # v1.7: æ›´æ¿€è¿›çš„çº¢åˆ©é…ç½®ï¼ˆè¶‹åŠ¿è°¨æ…ä½†ä¸æ”¾å¼ƒæ”¶ç›Šï¼‰
        growth_w = 0.15                # ä¿ç•™å°‘é‡æˆé•¿
        value_w = 0.25                 # çº¢åˆ©ä¸ºä¸»
        wtmf_w = 0.20                  # WTMFå¯¹å†²
        if value_regime:
            growth_w = 0.08
            value_w = 0.32             # ä»·å€¼å ä¼˜æ—¶æ›´å¤šçº¢åˆ©
            wtmf_w = 0.18
        return {
            'IWY': growth_w, 'WTMF': wtmf_w, 'LVHI': value_w,
            'G3B.SI': 0.08, 'MBH.SI': 0.10, 'GSD.SI': 0.08,
            'SRT.SI': 0.05, 'AJBU.SI': 0.04
        }
    if s == "CAUTIOUS_VOL":
        # v1.7: VIXåˆ†å±‚é…ç½® - åŠ¨æ€IWY/WTMFè½®æ¢
        iwy_w = 0.40   # åŸºç¡€å€¼æé«˜
        wtmf_w = 0.20  # åŸºç¡€å€¼é™ä½
        lvhi_w = 0.15  # å¢åŠ çº¢åˆ©ä½œä¸ºæ³¢åŠ¨ç¼“å†²
        mbh_w = 0.05
        
        if vix is not None:
            tier1 = CAUTIOUS_VOL_VIX_TIERS.get('tier1', (20, 25, 0.40, 0.20))
            tier2 = CAUTIOUS_VOL_VIX_TIERS.get('tier2', (25, 30, 0.30, 0.30))
            tier3 = CAUTIOUS_VOL_VIX_TIERS.get('tier3', (30, 40, 0.20, 0.40))
            tier4 = CAUTIOUS_VOL_VIX_TIERS.get('tier4', (40, 999, 0.10, 0.50))
            
            if tier1[0] <= vix < tier1[1]:
                iwy_w = tier1[2]
                wtmf_w = tier1[3]
                lvhi_w = 0.15
            elif tier2[0] <= vix < tier2[1]:
                iwy_w = tier2[2]
                wtmf_w = tier2[3]
                lvhi_w = 0.15
            elif tier3[0] <= vix < tier3[1]:
                iwy_w = tier3[2]
                wtmf_w = tier3[3]
                lvhi_w = 0.12  # é«˜æ³¢åŠ¨æ—¶å‡å°‘çº¢åˆ©
            elif vix >= tier4[0]:
                iwy_w = tier4[2]
                wtmf_w = tier4[3]
                lvhi_w = 0.10
        
        return {
            'IWY': iwy_w, 'WTMF': wtmf_w, 'LVHI': lvhi_w,
            'G3B.SI': 0.03, 'MBH.SI': mbh_w, 'GSD.SI': 0.05,
            'SRT.SI': 0.05, 'AJBU.SI': 0.05
        }
    # NEUTRAL - v1.7 æœ€å¤§åŒ–æƒç›Šé…ç½®ï¼ˆæ— ç°é‡‘/æ— WTMFæ‹–ç´¯ï¼‰
    growth_w = 0.68                   # æé«˜: 0.60â†’0.68 (ç‰›å¸‚æ ¸å¿ƒ)
    value_w = 0.05                    # é™ä½: 0.08â†’0.05 (ä½œä¸ºå›è°ƒç¼“å†²)
    wtmf_w = 0.0                      # å–æ¶ˆ: WTMFåœ¨ç‰›å¸‚æ˜¯çº¯æ‹–ç´¯
    if value_regime:
        growth_w = 0.55               # ä»·å€¼å ä¼˜æ—¶å‡å°‘æˆé•¿
        value_w = 0.20                # å¢åŠ çº¢åˆ©
        wtmf_w = 0.0
    return {
        'IWY': growth_w, 'WTMF': wtmf_w, 'LVHI': value_w,
        'G3B.SI': 0.05, 'MBH.SI': 0.05, 'GSD.SI': 0.03,  # å€ºåˆ¸/é»„é‡‘é™åˆ°æœ€ä½
        'SRT.SI': 0.07, 'AJBU.SI': 0.07  # REITsä½œä¸ºæ”¶ç›Šè¡¥å……
    }


def apply_vix_adjustments(targets, state, vix):
    """v1.7: VIXé©±åŠ¨çš„æˆé•¿â†”çº¢åˆ©â†”WTMFè½®æ¢"""
    if vix is None:
        return
    
    if state == "NEUTRAL":
        if vix < VIX_BOOST_LO:
            # æä½VIX: å…¨ä»“æˆé•¿ï¼Œå–æ¶ˆæ‰€æœ‰é¿é™©
            wtmf_amt = targets.get('WTMF', 0)
            mbh_amt = targets.get('MBH.SI', 0) * 0.8  # ä¿ç•™20%å€ºåˆ¸
            gsd_amt = targets.get('GSD.SI', 0) * 0.5  # å‡åŠé»„é‡‘
            
            total_boost = wtmf_amt + mbh_amt + gsd_amt
            targets['WTMF'] = 0.0
            targets['MBH.SI'] = targets.get('MBH.SI', 0) - mbh_amt
            targets['GSD.SI'] = targets.get('GSD.SI', 0) - gsd_amt
            targets['IWY'] = targets.get('IWY', 0) + total_boost
            
        elif vix > VIX_GROWTH_TO_VALUE_START:
            # VIX>22: å¼€å§‹ä»æˆé•¿è½¬å‘çº¢åˆ©ï¼ˆçº¢åˆ©æ›´æŠ—è·Œï¼‰
            shift_ratio = min((vix - VIX_GROWTH_TO_VALUE_START) / (VIX_GROWTH_TO_VALUE_FULL - VIX_GROWTH_TO_VALUE_START), 1.0)
            shift_amt = min(targets.get('IWY', 0), GROWTH_TO_VALUE_MAX_SHIFT * shift_ratio)
            
            if shift_amt > 0:
                targets['IWY'] -= shift_amt
                # 70%è½¬çº¢åˆ©ï¼Œ30%è½¬WTMF
                targets['LVHI'] = targets.get('LVHI', 0) + shift_amt * 0.7
                targets['WTMF'] = targets.get('WTMF', 0) + shift_amt * 0.3
    
    elif state == "CAUTIOUS_VOL":
        # é«˜æ³¢åŠ¨çŠ¶æ€ï¼šåŠ¨æ€è°ƒæ•´å·²åœ¨base_allocationä¸­å¤„ç†
        pass


def apply_yield_curve_guard(targets, state, yield_curve):
    if state not in ["DEFLATION_RECESSION", "CAUTIOUS_TREND"]:
        return
    if yield_curve is None or yield_curve >= YIELD_CURVE_CUTOFF:
        return
    if targets.get('MBH.SI', 0) > 0:
        move_amt = targets['MBH.SI'] * 0.7
        targets['MBH.SI'] -= move_amt
        targets['WTMF'] = targets.get('WTMF', 0) + move_amt


def apply_trend_filters(targets, state, asset_trends):
    if state == "EXTREME_ACCUMULATION":
        return
    assets_to_check = ['G3B.SI', 'LVHI', 'MBH.SI', 'GSD.SI', 'SRT.SI', 'AJBU.SI']
    for asset in assets_to_check:
        if targets.get(asset, 0) > 0 and asset_trends.get(asset, False):
            weight_to_move = targets[asset]
            targets[asset] = 0.0
            if state == "NEUTRAL":
                if not asset_trends.get('IWY', False):
                    targets['IWY'] = targets.get('IWY', 0) + weight_to_move
                else:
                    targets['WTMF'] = targets.get('WTMF', 0) + weight_to_move
            else:
                targets['WTMF'] = targets.get('WTMF', 0) + weight_to_move


def apply_iwy_safety_valve(targets, state, asset_trends, vix):
    if state == "EXTREME_ACCUMULATION" or targets.get('IWY', 0) <= 0:
        return
    if asset_trends.get('IWY', False):
        severity = 0.5
        if vix is not None and vix > VIX_PANIC:
            severity = 0.8
        cut_amount = targets['IWY'] * severity
        targets['IWY'] -= cut_amount
        targets['WTMF'] = targets.get('WTMF', 0) + cut_amount


def apply_gold_filter(targets, gold_bear):
    if gold_bear and targets.get('GSD.SI', 0) > 0:
        cut_amount = targets['GSD.SI']
        targets['GSD.SI'] -= cut_amount
        targets['WTMF'] = targets.get('WTMF', 0) + cut_amount


def apply_momentum_intensity(targets, state, momentum_scores):
    """
    ä¼˜åŒ–1: åŠ¨é‡å¼ºåº¦åˆ†å±‚é…ç½®
    æ ¹æ®ä»·æ ¼è·ç¦»MAçš„å¹…åº¦åˆ†å±‚è°ƒæ•´æƒé‡ï¼Œè€Œéç®€å•çš„äºŒå…ƒåˆ¤æ–­
    momentum_scores: dict {ticker: score} where score = (price - ma) / ma
    """
    if state == "EXTREME_ACCUMULATION" or not momentum_scores:
        return
    
    # IWYåŠ¨é‡å¼ºåº¦è°ƒæ•´
    iwy_score = momentum_scores.get('IWY')
    if iwy_score is not None and targets.get('IWY', 0) > 0:
        if iwy_score < (MOMENTUM_WEAK_THRESHOLD - 1):
            # å¼±åŠ¿åŒºï¼šå·²ç”±è¶‹åŠ¿ç†”æ–­å¤„ç†ï¼Œè¿™é‡Œä¸é‡å¤
            pass
        elif iwy_score < (MOMENTUM_STRONG_THRESHOLD - 1):
            # ä¸­æ€§åŒº (-5% ~ +5%)ï¼šå‡ä»“ä¸€éƒ¨åˆ†
            reduction = targets['IWY'] * MOMENTUM_NEUTRAL_REDUCTION
            targets['IWY'] -= reduction
            targets['WTMF'] = targets.get('WTMF', 0) + reduction


def apply_sahm_early_warning(targets, state, sahm):
    """
    ä¼˜åŒ–2: Sahm Rule é¢„è­¦å¢å¼º
    åœ¨Sahm 0.30-0.50åŒºé—´æå‰å‡ä»“ï¼Œè€Œéç­‰åˆ°0.50æ‰è§¦å‘
    """
    if state not in ["NEUTRAL", "CAUTIOUS_VOL"] or sahm is None:
        return
    
    if SAHM_EARLY_WARNING_LO <= sahm < SAHM_EARLY_WARNING_HI:
        # çº¿æ€§å‡ä»“: 0.30æ—¶å‡0%, 0.50æ—¶å‡50%
        reduction_pct = (sahm - SAHM_EARLY_WARNING_LO) / (SAHM_EARLY_WARNING_HI - SAHM_EARLY_WARNING_LO) * SAHM_REDUCTION_RATE
        iwy_current = targets.get('IWY', 0)
        if iwy_current > 0:
            move_amt = iwy_current * reduction_pct
            targets['IWY'] = iwy_current - move_amt
            targets['WTMF'] = targets.get('WTMF', 0) + move_amt


def apply_yield_curve_uninvert_protection(targets, state, yield_curve, yc_recently_inverted):
    """
    ä¼˜åŒ–3: æ”¶ç›Šç‡æ›²çº¿è§£å€’æŒ‚åå»¶ä¿æŠ¤
    æ”¶ç›Šç‡æ›²çº¿ä»è´Ÿè½¬æ­£å12ä¸ªæœˆå†…ä¿æŒé˜²å¾¡é…ç½®
    yc_recently_inverted: bool, è¿‡å»12ä¸ªæœˆå†…æ˜¯å¦æ›¾æ·±åº¦å€’æŒ‚
    """
    if state not in ["NEUTRAL", "CAUTIOUS_VOL"]:
        return
    
    # å½“å‰æ›²çº¿å·²è½¬æ­£ä½†è¿‘æœŸæ›¾å€’æŒ‚ -> ä¿æŠ¤æœŸ
    if yield_curve is not None and yield_curve > 0 and yc_recently_inverted:
        iwy_current = targets.get('IWY', 0)
        if iwy_current > 0:
            move_amt = iwy_current * YC_UNINVERT_REDUCTION
            targets['IWY'] = iwy_current - move_amt
            targets['MBH.SI'] = targets.get('MBH.SI', 0) + move_amt * 0.5
            targets['WTMF'] = targets.get('WTMF', 0) + move_amt * 0.5


def apply_vix_mean_reversion(targets, state, vix, vix_recent_peak):
    """
    ä¼˜åŒ–4: VIXå‡å€¼å›å½’åŠ ä»“
    VIXä»é«˜ä½å›è½æ—¶è§¦å‘æ¸©å’ŒåŠ ä»“
    vix_recent_peak: è¿‘æœŸVIXæœ€é«˜å€¼
    """
    if state not in ["NEUTRAL", "CAUTIOUS_VOL"] or vix is None or vix_recent_peak is None:
        return
    
    # æ¡ä»¶: è¿‘æœŸå³°å€¼>25ï¼Œå½“å‰VIXå·²å›è½è¶…è¿‡20%
    if vix_recent_peak >= VIX_MEAN_REVERSION_PEAK and vix < vix_recent_peak * VIX_MEAN_REVERSION_RATIO:
        # ä»WTMFè½¬ç§»åˆ°IWY
        wtmf_current = targets.get('WTMF', 0)
        if wtmf_current > VIX_MEAN_REVERSION_BOOST:
            targets['WTMF'] = wtmf_current - VIX_MEAN_REVERSION_BOOST
            targets['IWY'] = targets.get('IWY', 0) + VIX_MEAN_REVERSION_BOOST


def apply_correlation_adjustment(targets, state, corr):
    """
    ä¼˜åŒ–5: ç›¸å…³æ€§åŠ¨æ€å†é…ç½®ï¼ˆv1.5 æ¸è¿›å“åº”ï¼‰
    è‚¡å€ºç›¸å…³æ€§ä¸Šå‡æ—¶æ¸è¿›å¢é…éç›¸å…³èµ„äº§
    """
    if state not in ["NEUTRAL", "CAUTIOUS_VOL", "CAUTIOUS_TREND"] or corr is None:
        return
    
    if corr > CORR_MID_THRESHOLD:
        # æ¸è¿›å¼è°ƒæ•´ï¼š0.15-0.30åŒºé—´çº¿æ€§å¢åŠ è°ƒæ•´å¹…åº¦
        adjustment_pct = min((corr - CORR_MID_THRESHOLD) / (CORR_HIGH_THRESHOLD - CORR_MID_THRESHOLD), 1.0)
        realloc = adjustment_pct * CORR_MAX_REALLOC
        
        mbh_current = targets.get('MBH.SI', 0)
        if mbh_current > realloc:
            targets['MBH.SI'] = mbh_current - realloc
            targets['WTMF'] = targets.get('WTMF', 0) + realloc * 0.7
            targets['GSD.SI'] = targets.get('GSD.SI', 0) + realloc * 0.3  # éƒ¨åˆ†è½¬é»„é‡‘


def apply_cash_buffer(targets, state, vix):
    """
    ä¼˜åŒ–6: ç°é‡‘ç¼“å†²æœºåˆ¶
    åœ¨ä¸ç¡®å®šæ€§é«˜çš„æ—¶æœŸä¿ç•™æˆ˜æœ¯ç°é‡‘
    """
    if state == "EXTREME_ACCUMULATION":
        return  # æŠ„åº•æ¨¡å¼ä¸ç•™ç°é‡‘
    
    cash_buffer = CASH_BUFFER_BASE
    if vix is not None and vix > CASH_BUFFER_VIX_THRESHOLD:
        # VIXæ¯å‡é«˜5ç‚¹ï¼Œç°é‡‘å¢åŠ ä¸€å®šæ¯”ä¾‹
        extra_cash = min((vix - CASH_BUFFER_VIX_THRESHOLD) / 5 * CASH_BUFFER_VIX_SCALE, 
                         CASH_BUFFER_MAX - CASH_BUFFER_BASE)
        cash_buffer = CASH_BUFFER_BASE + extra_cash
    
    # æŒ‰æ¯”ä¾‹ç¼©å‡æ‰€æœ‰èµ„äº§
    if cash_buffer > 0:
        scale = 1 - cash_buffer
        for asset in targets:
            targets[asset] *= scale


def apply_dual_ma_trend_filter(targets, state, dual_ma_signals):
    """
    ä¼˜åŒ–7: åŒå‡çº¿è¶‹åŠ¿ç¡®è®¤
    ä½¿ç”¨50æ—¥å’Œ200æ—¥å‡çº¿åˆ¤æ–­è¶‹åŠ¿å¼ºåº¦
    dual_ma_signals: dict {ticker: 'STRONG_BEAR'|'WEAK_BEAR'|'BULLISH'}
    """
    if state == "EXTREME_ACCUMULATION" or not dual_ma_signals:
        return
    
    for asset, signal in dual_ma_signals.items():
        if asset not in targets or targets.get(asset, 0) <= 0:
            continue
        
        weight = targets[asset]
        if signal == "STRONG_BEAR":
            # å¼ºç†Šå¸‚ï¼šå¤§å¹…å‡ä»“
            cut_amount = weight * STRONG_BEAR_REDUCTION
            targets[asset] = weight - cut_amount
            targets['WTMF'] = targets.get('WTMF', 0) + cut_amount
        elif signal == "WEAK_BEAR":
            # å¼±ç†Šå¸‚ï¼ˆå¯èƒ½æ˜¯å›è°ƒï¼‰ï¼šå°å¹…å‡ä»“
            cut_amount = weight * WEAK_BEAR_REDUCTION
            targets[asset] = weight - cut_amount
            targets['WTMF'] = targets.get('WTMF', 0) + cut_amount


def apply_market_breadth_adjustment(targets, state, breadth_score):
    """
    ä¼˜åŒ–8: è·¨èµ„äº§åŠ¨é‡ï¼ˆå¸‚åœºå¹¿åº¦ï¼‰
    æ ¹æ®æ•´ä½“å¸‚åœºåŠ¨é‡è°ƒæ•´æƒç›Šé…ç½®
    breadth_score: 0-1ï¼Œè¡¨ç¤ºæœ‰å¤šå°‘æ¯”ä¾‹çš„èµ„äº§å¤„äºä¸Šå‡è¶‹åŠ¿
    """
    if state == "EXTREME_ACCUMULATION" or breadth_score is None:
        return
    
    reduction = 0
    if breadth_score < MARKET_BREADTH_LOW:
        # å¸‚åœºå¹¿åº¦å¾ˆå·®ï¼Œæ•´ä½“å‡ä»“æƒç›Š
        reduction = BREADTH_LOW_REDUCTION
    elif breadth_score < MARKET_BREADTH_MID:
        # å¸‚åœºå¹¿åº¦ä¸€èˆ¬ï¼Œå°å¹…å‡ä»“
        reduction = BREADTH_MID_REDUCTION
    
    if reduction > 0:
        # åªå‡ä»“é«˜é£é™©æƒç›Šèµ„äº§
        risk_assets = ['IWY', 'G3B.SI']
        total_cut = 0
        for asset in risk_assets:
            if targets.get(asset, 0) > 0:
                cut_amount = targets[asset] * reduction
                targets[asset] -= cut_amount
                total_cut += cut_amount
        
        # å·®é¢è¡¥åˆ°WTMF
        if total_cut > 0:
            targets['WTMF'] = targets.get('WTMF', 0) + total_cut


def apply_trend_boost(targets, state, momentum_scores, vix):
    """
    v1.7: è¶‹åŠ¿é©±åŠ¨çš„æˆé•¿â†”WTMFåŠ¨æ€è½®æ¢
    å¼ºç‰›å¸‚: æœ€å¤§åŒ–æˆé•¿
    å¼ºç†Šå¸‚: WTMFå¯¹å†²
    """
    if state not in ["NEUTRAL", "CAUTIOUS_VOL"] or not momentum_scores or vix is None:
        return
    
    iwy_score = momentum_scores.get('IWY')
    if iwy_score is None:
        return
    
    # è®¡ç®—ä»·æ ¼ç›¸å¯¹MAçš„åç¦»åº¦ (score = (price - ma) / ma)
    price_vs_ma = iwy_score + 1  # è½¬æ¢ä¸º price/ma æ¯”å€¼
    
    if price_vs_ma >= TREND_STRONG_BULL:
        # å¼ºç‰›å¸‚ (>10%): æœ€å¤§åŒ–æˆé•¿æ•å£
        if vix < 18:  # åªåœ¨ä½æ³¢åŠ¨æ—¶æ¿€è¿›åŠ ä»“
            boost_from_wtmf = targets.get('WTMF', 0)
            boost_from_lvhi = targets.get('LVHI', 0) * 0.3  # ä»çº¢åˆ©è½¬30%
            boost_from_mbh = targets.get('MBH.SI', 0) * 0.5
            
            total_boost = boost_from_wtmf + boost_from_lvhi + boost_from_mbh
            targets['WTMF'] = 0.0
            targets['LVHI'] = targets.get('LVHI', 0) - boost_from_lvhi
            targets['MBH.SI'] = targets.get('MBH.SI', 0) - boost_from_mbh
            targets['IWY'] = targets.get('IWY', 0) + total_boost
            
    elif price_vs_ma >= TREND_MILD_BULL:
        # æ¸©å’Œç‰›å¸‚ (3-10%): é€‚åº¦å€¾æ–œæˆé•¿
        boost_amt = min(targets.get('WTMF', 0), BULL_IWY_BOOST)
        if boost_amt > 0:
            targets['WTMF'] = targets.get('WTMF', 0) - boost_amt
            targets['IWY'] = targets.get('IWY', 0) + boost_amt
            
    elif price_vs_ma < TREND_MILD_BEAR:
        # æ¸©å’Œç†Šå¸‚ (<-3%): å¢åŠ WTMFå¯¹å†²
        # ä»æˆé•¿è½¬ç§»åˆ°WTMFå’Œçº¢åˆ©
        shift_amt = min(targets.get('IWY', 0), BEAR_WTMF_BOOST * 0.6)
        if shift_amt > 0:
            targets['IWY'] -= shift_amt
            targets['WTMF'] = targets.get('WTMF', 0) + shift_amt * 0.7
            targets['LVHI'] = targets.get('LVHI', 0) + shift_amt * 0.3  # çº¢åˆ©æ›´æŠ—è·Œ


def apply_value_rotation(targets, state, momentum_scores):
    """
    v1.7 æ–°å¢: çº¢åˆ©ç›¸å¯¹å¼ºå¼±è½®æ¢
    å½“çº¢åˆ©ç›¸å¯¹æˆé•¿è·‘èµ¢æ—¶ï¼Œå¢é…çº¢åˆ©ï¼›åä¹‹å¢é…æˆé•¿
    """
    if state not in ["NEUTRAL", "CAUTIOUS_VOL"] or not momentum_scores:
        return
    
    iwy_score = momentum_scores.get('IWY')
    lvhi_score = momentum_scores.get('LVHI')
    
    if iwy_score is None or lvhi_score is None:
        return
    
    # è®¡ç®—ç›¸å¯¹å¼ºå¼± (LVHIç›¸å¯¹IWYçš„è¶…é¢æ”¶ç›Š)
    relative_strength = lvhi_score - iwy_score
    
    if relative_strength > VALUE_OUTPERFORM_THRESHOLD:
        # çº¢åˆ©è·‘èµ¢: ä»æˆé•¿è½¬å‘çº¢åˆ©
        shift_amt = min(targets.get('IWY', 0) * 0.3, VALUE_ROTATION_AMOUNT)
        if shift_amt > 0:
            targets['IWY'] -= shift_amt
            targets['LVHI'] = targets.get('LVHI', 0) + shift_amt
            
    elif relative_strength < VALUE_UNDERPERFORM_THRESHOLD:
        # æˆé•¿è·‘èµ¢: ä»çº¢åˆ©è½¬å‘æˆé•¿
        shift_amt = min(targets.get('LVHI', 0) * 0.5, VALUE_ROTATION_AMOUNT)
        if shift_amt > 0:
            targets['LVHI'] -= shift_amt
            targets['IWY'] = targets.get('IWY', 0) + shift_amt


def get_target_percentages(s, gold_bear=False, value_regime=False, asset_trends=None, vix=None, yield_curve=None,
                           sahm=None, corr=None, momentum_scores=None, yc_recently_inverted=False, vix_recent_peak=None,
                           dual_ma_signals=None, breadth_score=None):
    """
    Returns target asset allocation based on macro state.
    Shared by State Machine Diagnosis and Backtest.
    
    v1.5 æ–°å¢å‚æ•°:
    - dual_ma_signals: dict {ticker: 'STRONG_BEAR'|'WEAK_BEAR'|'BULLISH'}
    - breadth_score: 0-1ï¼Œè·¨èµ„äº§åŠ¨é‡åˆ†æ•°
    """
    asset_trends = asset_trends or {}

    # v1.5: base_allocation æ”¯æŒ VIX åˆ†å±‚
    targets = base_allocation(s, value_regime, vix)

    # åŸæœ‰è°ƒæ•´
    apply_vix_adjustments(targets, s, vix)
    apply_yield_curve_guard(targets, s, yield_curve)
    
    # v1.5: åŒå‡çº¿è¶‹åŠ¿è¿‡æ»¤ï¼ˆæ›¿ä»£åŸæœ‰ç®€å•è¶‹åŠ¿è¿‡æ»¤ï¼‰
    if dual_ma_signals:
        apply_dual_ma_trend_filter(targets, s, dual_ma_signals)
    else:
        apply_trend_filters(targets, s, asset_trends)
    
    apply_iwy_safety_valve(targets, s, asset_trends, vix)
    apply_gold_filter(targets, gold_bear)
    
    # æ–°å¢ä¼˜åŒ–è°ƒæ•´ï¼ˆæŒ‰å½±å“ç¨‹åº¦æ’åºï¼Œåæ‰§è¡Œçš„ä¼˜å…ˆçº§æ›´é«˜ï¼‰
    apply_momentum_intensity(targets, s, momentum_scores)
    apply_sahm_early_warning(targets, s, sahm)
    apply_yield_curve_uninvert_protection(targets, s, yield_curve, yc_recently_inverted)
    apply_correlation_adjustment(targets, s, corr)
    apply_vix_mean_reversion(targets, s, vix, vix_recent_peak)
    
    # v1.5: æ–°å¢ä¼˜åŒ–
    apply_market_breadth_adjustment(targets, s, breadth_score)
    
    # v1.7: åŠ¨æ€è½®æ¢ï¼ˆæ ¸å¿ƒæ”¶ç›Šå¢å¼ºï¼‰
    apply_trend_boost(targets, s, momentum_scores, vix)
    apply_value_rotation(targets, s, momentum_scores)
    
    # ç°é‡‘ç¼“å†²å·²ç¦ç”¨ï¼ˆv1.7ï¼‰

    return targets


def generate_execution_tips(metrics, change_info, current_holdings=None, targets=None, total_value=None):
    """
    ç”Ÿæˆæ‰§è¡Œå»ºè®®æç¤ºï¼Œå¸®åŠ©ç”¨æˆ·åœ¨å®é™…æ“ä½œæ—¶å‚è€ƒå›æµ‹ä¸­çš„ä¼˜åŒ–æœºåˆ¶ã€‚
    
    æ”¹è¿›ç‚¹:
    1. ä¿¡å·ç¡®è®¤: æ˜ç¡®æ˜¾ç¤º"å¾…ç¡®è®¤"çŠ¶æ€ï¼Œå»ºè®®è§‚æœ›
    2. æ³¢åŠ¨ç‡: åŸºäºæ”¶ç›˜æ•°æ®ï¼ˆå·²ç¡®å®šï¼‰ï¼Œç»™å‡ºæ˜ç¡®æ‰§è¡Œæ¯”ä¾‹
    3. æ­¢æŸ: åŸºäºç”¨æˆ·å®é™…æŒä»“è®¡ç®—å›æ’¤ï¼Œç»™å‡ºå…·ä½“æ“ä½œ
    4. å¯æ‰§è¡Œæ€§: ç»™å‡ºå…·ä½“çš„èµ„äº§å’Œé‡‘é¢å»ºè®®
    """
    tips = []
    
    vix = metrics.get('vix')
    state = metrics.get('state')
    days_in_state = change_info.get('days_in_state') if change_info else None
    prev_state = change_info.get('prev_state') if change_info else None
    
    # === 0. æ­¢æŸçŠ¶æ€æ£€æŸ¥ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰===
    if total_value and total_value > 0:
        stop_loss_status = get_stop_loss_status(total_value)
        if stop_loss_status.get('in_stop_loss'):
            drawdown = stop_loss_status['drawdown_pct']
            recovery_ratio = stop_loss_status['recovery_ratio']
            reduce_pct = (1 - recovery_ratio) * 100
            tips.append({
                'type': 'error',
                'icon': 'ğŸš¨',
                'title': f'æ­¢æŸè§¦å‘ (å›æ’¤ {drawdown*100:.1f}%)',
                'content': f'å½“å‰ç»„åˆå›æ’¤å·²è¶…è¿‡{abs(DRAWDOWN_STOP_LOSS)*100:.0f}%æ­¢æŸçº¿ï¼'
                           f'å»ºè®®ç«‹å³å°†é£é™©èµ„äº§ï¼ˆIWY, G3B.SIç­‰ï¼‰å‡ä»“è‡³ç›®æ ‡çš„{recovery_ratio*100:.0f}%ï¼Œ'
                           f'é‡Šæ”¾çš„èµ„é‡‘è½¬å…¥WTMFæˆ–ç°é‡‘ã€‚å›æ’¤æ¢å¤è‡³{abs(DRAWDOWN_RECOVERY_THRESHOLD)*100:.0f}%å†…åå†é€æ­¥æ¢å¤ã€‚'
            })
        elif stop_loss_status['drawdown_pct'] < -0.05:  # æ¥è¿‘æ­¢æŸçº¿
            drawdown = stop_loss_status['drawdown_pct']
            tips.append({
                'type': 'warning',
                'icon': 'âš ï¸',
                'title': f'æ¥è¿‘æ­¢æŸçº¿ (å›æ’¤ {drawdown*100:.1f}%)',
                'content': f'å½“å‰ç»„åˆå›æ’¤{drawdown*100:.1f}%ï¼Œè·ç¦»æ­¢æŸçº¿({abs(DRAWDOWN_STOP_LOSS)*100:.0f}%)è¾ƒè¿‘ã€‚'
                           f'å»ºè®®é™ä½é£é™©æ•å£ï¼Œæˆ–è®¾ç½®ç›˜ä¸­ä»·æ ¼æé†’ï¼Œè‹¥ç»§ç»­ä¸‹è·Œ{(DRAWDOWN_STOP_LOSS - drawdown)*100:.1f}%å³è§¦å‘æ­¢æŸã€‚'
            })
    
    # === 1. ä¿¡å·ç¡®è®¤æç¤ºï¼ˆé‡è¦æé†’ï¼‰===
    if days_in_state is not None and days_in_state <= SIGNAL_CONFIRM_DAYS:
        remaining_days = SIGNAL_CONFIRM_DAYS - days_in_state + 1
        if prev_state and prev_state != state:
            tips.append({
                'type': 'warning',
                'icon': 'ğŸ”„',
                'title': f'çŠ¶æ€å¾…ç¡®è®¤ ({days_in_state}/{SIGNAL_CONFIRM_DAYS}å¤©)',
                'content': f'çŠ¶æ€åˆšä» {prev_state} åˆ‡æ¢åˆ° {state}ï¼Œéœ€è¿ç»­{SIGNAL_CONFIRM_DAYS}å¤©ç¡®è®¤ã€‚'
                           f'ã€å»ºè®®ã€‘æš‚ä¸æ‰§è¡Œå¤§å¹…è°ƒä»“ï¼Œç­‰å¾…{remaining_days}å¤©ç¡®è®¤åå†è¡ŒåŠ¨ã€‚'
                           f'è‹¥æ€¥éœ€æ“ä½œï¼Œå¯å…ˆæ‰§è¡Œç›®æ ‡é…ç½®çš„50%ã€‚'
            })
        else:
            tips.append({
                'type': 'info',
                'icon': 'ğŸ”„',
                'title': f'ä¿¡å·ç¡®è®¤ä¸­ ({days_in_state}/{SIGNAL_CONFIRM_DAYS}å¤©)',
                'content': f'å½“å‰çŠ¶æ€ {state} æŒç»­{days_in_state}å¤©ï¼Œè¿˜éœ€{remaining_days}å¤©ç¡®è®¤ã€‚å¯æŒ‰ç›®æ ‡çš„50-70%å…ˆè¡Œé…ç½®ã€‚'
            })
    elif days_in_state is not None and days_in_state > SIGNAL_CONFIRM_DAYS:
        tips.append({
            'type': 'success',
            'icon': 'âœ…',
            'title': f'ä¿¡å·å·²ç¡®è®¤ (æŒç»­{days_in_state}å¤©)',
            'content': f'çŠ¶æ€ {state} å·²ç¡®è®¤ï¼Œå¯æŒ‰ç›®æ ‡é…ç½®å…¨é¢æ‰§è¡Œã€‚'
        })
    
    # === 2. æ³¢åŠ¨ç‡æ‰§è¡Œå»ºè®®ï¼ˆåŸºäºæ”¶ç›˜VIXï¼Œå·²ç¡®å®šï¼‰===
    if vix is not None:
        if vix > VIX_SMOOTH_END:
            reduction_pct = int(VIX_MAX_REDUCTION * 100)
            exec_pct = 100 - reduction_pct
            tips.append({
                'type': 'error',
                'icon': 'ğŸ“Š',
                'title': f'é«˜æ³¢åŠ¨è­¦å‘Š (VIX={vix:.1f})',
                'content': f'VIXè¶…è¿‡{VIX_SMOOTH_END:.0f}ï¼Œå¸‚åœºæ³¢åŠ¨å‰§çƒˆã€‚'
                           f'ã€æ‰§è¡Œã€‘æŒ‰ç›®æ ‡é…ç½®çš„{exec_pct}%å»ºä»“ï¼Œ{reduction_pct}%ç•™ä½œç°é‡‘/WTMFã€‚'
                           f'ä¾‹å¦‚ç›®æ ‡IWY 55%ï¼Œå®é™…æ‰§è¡ŒIWY {55*exec_pct/100:.0f}%ã€‚'
            })
        elif vix > VIX_SMOOTH_START:
            reduction = (vix - VIX_SMOOTH_START) / (VIX_SMOOTH_END - VIX_SMOOTH_START) * VIX_MAX_REDUCTION
            exec_pct = int((1 - reduction) * 100)
            tips.append({
                'type': 'warning',
                'icon': 'ğŸ“Š',
                'title': f'æ³¢åŠ¨åé«˜ (VIX={vix:.1f})',
                'content': f'VIXå¤„äº{VIX_SMOOTH_START:.0f}-{VIX_SMOOTH_END:.0f}åŒºé—´ï¼Œå»ºè®®ä¿å®ˆæ‰§è¡Œã€‚'
                           f'ã€æ‰§è¡Œã€‘æŒ‰ç›®æ ‡é…ç½®çš„{exec_pct}%å»ºä»“ï¼Œç•™{100-exec_pct}%ç°é‡‘ç¼“å†²ã€‚'
            })
        elif vix < VIX_BOOST_LO:
            tips.append({
                'type': 'success',
                'icon': 'ğŸš€',
                'title': f'ä½æ³¢åŠ¨æœºä¼š (VIX={vix:.1f})',
                'content': f'VIX<{VIX_BOOST_LO:.0f}ï¼Œå¸‚åœºæåº¦å¹³ç¨³ã€‚å¯å…¨é¢æ‰§è¡Œç›®æ ‡é…ç½®ï¼Œç”šè‡³è€ƒè™‘å‡å°‘WTMF/å€ºåˆ¸ï¼Œå¢åŠ æƒç›Šã€‚'
            })
    
    # === 3. å…·ä½“è°ƒä»“å»ºè®® ===
    if targets and current_holdings and total_value and total_value > 0:
        # è®¡ç®—å„èµ„äº§åç¦»
        deviations = []
        for ticker, target_w in targets.items():
            current_val = current_holdings.get(ticker, 0)
            current_w = current_val / total_value if isinstance(current_val, (int, float)) else 0
            deviation = target_w - current_w
            diff_val = deviation * total_value
            if abs(deviation) > 0.02:  # è¶…è¿‡2%æ‰æ˜¾ç¤º
                deviations.append({
                    'ticker': ticker,
                    'name': ASSET_NAMES.get(ticker, ticker),
                    'deviation': deviation,
                    'diff_val': diff_val,
                    'action': 'ä¹°å…¥' if deviation > 0 else 'å–å‡º'
                })
        
        # æ£€æŸ¥éœ€è¦æ¸…ä»“çš„èµ„äº§
        for ticker, current_val in current_holdings.items():
            if ticker not in targets and isinstance(current_val, (int, float)) and current_val > 100:
                deviations.append({
                    'ticker': ticker,
                    'name': ASSET_NAMES.get(ticker, ticker),
                    'deviation': -current_val / total_value,
                    'diff_val': -current_val,
                    'action': 'æ¸…ä»“'
                })
        
        # æŒ‰åç¦»å¤§å°æ’åº
        deviations.sort(key=lambda x: abs(x['deviation']), reverse=True)
        
        total_change = sum(abs(d['deviation']) for d in deviations) / 2  # å•è¾¹æ¢æ‰‹
        max_deviation = max(abs(d['deviation']) for d in deviations) if deviations else 0
        
        if max_deviation < REBALANCE_THRESHOLD:
            tips.append({
                'type': 'success',
                'icon': 'ğŸ“',
                'title': 'æ— éœ€è°ƒä»“',
                'content': f'æ‰€æœ‰èµ„äº§åç¦»å‡<{REBALANCE_THRESHOLD*100:.0f}%ï¼Œå¯æš‚ä¸è°ƒä»“ä»¥èŠ‚çœäº¤æ˜“æˆæœ¬ï¼ˆé¢„ä¼°0.1-0.3%ï¼‰ã€‚'
            })
        elif total_change > 0.20:
            # å¤§å¹…è°ƒä»“ï¼Œå»ºè®®åˆ†æ­¥
            top_actions = deviations[:3]
            action_text = "; ".join([
                f"{d['action']}{d['name'][:6]}çº¦${abs(d['diff_val']):,.0f}" for d in top_actions
            ])
            tips.append({
                'type': 'info',
                'icon': 'ğŸ”€',
                'title': f'åˆ†æ­¥è°ƒä»“ (æ¢æ‰‹{total_change*100:.0f}%)',
                'content': f'è°ƒä»“å¹…åº¦è¾ƒå¤§ï¼Œå»ºè®®åˆ†{STATE_TRANSITION_DAYS}å¤©æ‰§è¡Œã€‚'
                           f'ã€ä»Šæ—¥æ“ä½œã€‘{action_text}ã€‚æ¯å¤©è°ƒæ•´çº¦{total_change/STATE_TRANSITION_DAYS*100:.0f}%ã€‚'
            })
        elif deviations:
            top_actions = deviations[:2]
            action_text = "; ".join([
                f"{d['action']}{d['name'][:6]}çº¦${abs(d['diff_val']):,.0f}" for d in top_actions
            ])
            tips.append({
                'type': 'info',
                'icon': 'ğŸ“‹',
                'title': 'è°ƒä»“å»ºè®®',
                'content': f'ã€æ“ä½œã€‘{action_text}ã€‚'
            })
    
    # === 4. æç«¯çŠ¶æ€æç¤º ===
    if state == "EXTREME_ACCUMULATION":
        tips.append({
            'type': 'warning',
            'icon': 'âš¡',
            'title': 'æŠ„åº•çŠ¶æ€',
            'content': 'æç«¯æŠ„åº•æ¨¡å¼ï¼Œé£é™©ä¸æœºä¼šå¹¶å­˜ã€‚ã€æ‰§è¡Œã€‘åˆ†æ‰¹å»ºä»“ï¼šé¦–æ¬¡40% â†’ åå¼¹5%ååŠ è‡³60% â†’ çªç ´MA50ååŠ è‡³75%ã€‚'
        })
    elif state in ["DEFLATION_RECESSION", "INFLATION_SHOCK"]:
        tips.append({
            'type': 'error',
            'icon': 'ğŸ›¡ï¸',
            'title': 'å±æœºé˜²å¾¡æ¨¡å¼',
            'content': 'å½“å‰ä¸ºå±æœºçŠ¶æ€ï¼Œä¼˜å…ˆä¿æœ¬ã€‚ä¸¥æ ¼æ‰§è¡Œç›®æ ‡é…ç½®ï¼Œé¿å…æŠ„åº•å†²åŠ¨ã€‚WTMFå’Œé»„é‡‘æ˜¯ä¸»è¦é¿é™©å·¥å…·ã€‚'
        })
    
    # === 5. Sahm Rule é¢„è­¦æç¤º ===
    sahm = metrics.get('sahm')
    if sahm is not None and SAHM_EARLY_WARNING_LO <= sahm < SAHM_EARLY_WARNING_HI:
        reduction_pct = int((sahm - SAHM_EARLY_WARNING_LO) / (SAHM_EARLY_WARNING_HI - SAHM_EARLY_WARNING_LO) * SAHM_REDUCTION_RATE * 100)
        tips.append({
            'type': 'warning',
            'icon': 'ğŸ“‰',
            'title': f'Sahmé¢„è­¦ ({sahm:.2f})',
            'content': f'Sahm Ruleå¤„äºé¢„è­¦åŒºé—´({SAHM_EARLY_WARNING_LO}-{SAHM_EARLY_WARNING_HI})ã€‚'
                       f'ã€å½±å“ã€‘IWYç›®æ ‡å·²è‡ªåŠ¨å‡å°‘{reduction_pct}%ï¼Œè½¬å…¥WTMFã€‚'
        })
    
    # === 6. æ”¶ç›Šç‡æ›²çº¿è§£å€’æŒ‚æç¤º ===
    yc_un_invert = metrics.get('yc_un_invert', False)
    yield_curve = metrics.get('yield_curve')
    if yc_un_invert and yield_curve is not None and yield_curve > 0:
        tips.append({
            'type': 'warning',
            'icon': 'ğŸ“ˆ',
            'title': 'è§£å€’æŒ‚ä¿æŠ¤æœŸ',
            'content': f'æ”¶ç›Šç‡æ›²çº¿å·²è½¬æ­£({yield_curve:.2f}%)ï¼Œä½†è¿‘æœŸæ›¾æ·±åº¦å€’æŒ‚ã€‚'
                       f'ã€å†å²è§„å¾‹ã€‘è§£å€’æŒ‚å6-18ä¸ªæœˆæ˜“å‘ç”Ÿè¡°é€€ã€‚'
                       f'ã€å½±å“ã€‘IWYç›®æ ‡å·²å‡å°‘{int(YC_UNINVERT_REDUCTION*100)}%ï¼Œç»´æŒé˜²å¾¡é…ç½®ã€‚'
        })
    
    # === 7. ç›¸å…³æ€§è­¦å‘Š ===
    corr = metrics.get('corr')
    if corr is not None and corr > CORR_HIGH_THRESHOLD:
        tips.append({
            'type': 'info',
            'icon': 'ğŸ”—',
            'title': f'ç›¸å…³æ€§åé«˜ ({corr:.2f})',
            'content': f'è‚¡å€ºç›¸å…³æ€§>{CORR_HIGH_THRESHOLD}ï¼Œå€ºåˆ¸å¯¹å†²æ•ˆæœå‡å¼±ã€‚'
                       f'ã€å½±å“ã€‘MBH.SIç›®æ ‡å·²è½¬ç§»{int(CORR_MAX_REALLOC*100)}%è‡³WTMF/é»„é‡‘ã€‚'
        })
    
    # === 8. è·¨å¸‚åœºæ‰§è¡Œæé†’ ===
    if targets:
        sg_assets = [t for t in targets.keys() if '.SI' in t and targets[t] > 0.02]
        us_assets = [t for t in targets.keys() if '.SI' not in t and t != 'OTHERS' and targets[t] > 0.02]
        if sg_assets and us_assets:
            tips.append({
                'type': 'info',
                'icon': 'ğŸŒ',
                'title': 'è·¨å¸‚åœºæ‰§è¡Œ',
                'content': f'æ¶‰åŠæ–°åŠ å¡({", ".join(sg_assets[:3])})å’Œç¾è‚¡({", ".join(us_assets[:3])})ã€‚'
                           f'ã€æ—¶åŒºã€‘SGX 9:00-17:00(+8), NYSE 21:30-04:00(+8)ã€‚å»ºè®®å…ˆæ‰§è¡ŒSGXï¼Œæ¬¡æ—¥å†æ‰§è¡ŒUSã€‚'
            })
    
    return tips


def calculate_dual_ma_signals(price_data, ma_short=TREND_MA_SHORT, ma_long=TREND_MA_LONG):
    """
    è®¡ç®—åŒå‡çº¿è¶‹åŠ¿ä¿¡å·
    è¿”å›: dict {ticker: 'STRONG_BEAR'|'WEAK_BEAR'|'BULLISH'}
    - STRONG_BEAR: ä»·æ ¼ < MA200 ä¸” MA50 < MA200 (å¼ºç†Šå¸‚)
    - WEAK_BEAR: ä»·æ ¼ < MA200 ä½† MA50 > MA200 (å¯èƒ½æ˜¯å›è°ƒ)
    - BULLISH: ä»·æ ¼ > MA200
    """
    signals = {}
    
    if price_data is None or price_data.empty:
        return signals
    
    for ticker in price_data.columns:
        try:
            prices = price_data[ticker].dropna()
            if len(prices) < ma_long:
                continue
            
            ma50 = prices.rolling(ma_short).mean().iloc[-1]
            ma200 = prices.rolling(ma_long).mean().iloc[-1]
            price = prices.iloc[-1]
            
            if pd.isna(ma50) or pd.isna(ma200) or pd.isna(price):
                continue
            
            if price < ma200 and ma50 < ma200:
                signals[ticker] = "STRONG_BEAR"
            elif price < ma200:
                signals[ticker] = "WEAK_BEAR"
            else:
                signals[ticker] = "BULLISH"
        except Exception:
            continue
    
    return signals


def calculate_market_breadth(price_data, ma_window=200):
    """
    è®¡ç®—è·¨èµ„äº§åŠ¨é‡ï¼ˆå¸‚åœºå¹¿åº¦ï¼‰
    è¿”å›: 0-1 ä¹‹é—´çš„åˆ†æ•°ï¼Œè¡¨ç¤ºæœ‰å¤šå°‘æ¯”ä¾‹çš„èµ„äº§å¤„äºä¸Šå‡è¶‹åŠ¿
    """
    if price_data is None or price_data.empty:
        return None
    
    above_ma_count = 0
    total_count = 0
    
    for ticker in price_data.columns:
        try:
            prices = price_data[ticker].dropna()
            if len(prices) < ma_window:
                continue
            
            ma = prices.rolling(ma_window).mean().iloc[-1]
            price = prices.iloc[-1]
            
            if pd.notna(ma) and pd.notna(price):
                total_count += 1
                if price > ma:
                    above_ma_count += 1
        except Exception:
            continue
    
    if total_count == 0:
        return None
    
    return above_ma_count / total_count


def calculate_portfolio_health(current_holdings, targets, total_value):
    """
    è®¡ç®—æŒä»“å¥åº·åº¦è¯„åˆ† (0-100åˆ†)
    è¿”å›: (score, details_dict)
    """
    if total_value <= 0:
        return 0, {'reason': 'æ€»å¸‚å€¼ä¸ºé›¶'}
    
    # 1. æƒé‡åç¦»åº¦ (40åˆ†)
    total_deviation = 0
    max_single_deviation = 0
    deviations = {}
    
    all_tickers = set(targets.keys()).union(current_holdings.keys())
    for tkr in all_tickers:
        target_w = targets.get(tkr, 0)
        current_val = current_holdings.get(tkr, 0)
        current_w = current_val / total_value if total_value > 0 else 0
        dev = abs(target_w - current_w)
        deviations[tkr] = {'target': target_w, 'current': current_w, 'deviation': dev}
        total_deviation += dev
        max_single_deviation = max(max_single_deviation, dev)
    
    # åç¦»åº¦è¯„åˆ†: æ€»åç¦»<10%å¾—æ»¡åˆ†ï¼Œ>50%å¾—0åˆ†
    deviation_score = max(0, 40 * (1 - total_deviation / 0.5))
    
    # 2. å•ä¸€èµ„äº§é›†ä¸­åº¦ (20åˆ†)
    max_weight = max([v / total_value for v in current_holdings.values()]) if current_holdings else 0
    # å•ä¸€èµ„äº§<40%å¾—æ»¡åˆ†ï¼Œ>70%å¾—0åˆ†
    concentration_score = max(0, 20 * (1 - (max_weight - 0.4) / 0.3)) if max_weight > 0.4 else 20
    
    # 3. èµ„äº§ç±»åˆ«å¤šæ ·æ€§ (20åˆ†)
    category_weights = {}
    for tkr, val in current_holdings.items():
        if val <= 0:
            continue
        cat = ASSET_CATEGORIES.get(tkr, {}).get('category', 'å…¶ä»–')
        category_weights[cat] = category_weights.get(cat, 0) + val / total_value
    
    # è‡³å°‘è¦†ç›–3ä¸ªç±»åˆ«å¾—æ»¡åˆ†
    diversity_score = min(20, len([c for c, w in category_weights.items() if w > 0.05]) * 5)
    
    # 4. ç°é‡‘/å¯¹å†²è¦†ç›– (20åˆ†) - æ£€æŸ¥é˜²å¾¡æ€§é…ç½®
    defensive_weight = category_weights.get('å›ºæ”¶', 0) + category_weights.get('å¯¹å†²', 0) + category_weights.get('å•†å“', 0)
    # é˜²å¾¡é…ç½®åœ¨15-40%ä¹‹é—´å¾—æ»¡åˆ†
    if 0.15 <= defensive_weight <= 0.40:
        defensive_score = 20
    elif defensive_weight < 0.15:
        defensive_score = max(0, 20 * defensive_weight / 0.15)
    else:
        defensive_score = max(0, 20 * (1 - (defensive_weight - 0.40) / 0.30))
    
    total_score = deviation_score + concentration_score + diversity_score + defensive_score
    
    return total_score, {
        'deviation_score': deviation_score,
        'concentration_score': concentration_score,
        'diversity_score': diversity_score,
        'defensive_score': defensive_score,
        'total_deviation': total_deviation,
        'max_single_deviation': max_single_deviation,
        'max_weight': max_weight,
        'category_weights': category_weights,
        'deviations': deviations
    }


def generate_rebalance_priority(current_holdings, targets, total_value, metrics):
    """
    ç”Ÿæˆè°ƒä»“ä¼˜å…ˆçº§åˆ—è¡¨ï¼ŒæŒ‰ç´§è¿«ç¨‹åº¦æ’åº
    è¿”å›: [(ticker, priority_score, reason, action_detail), ...]
    """
    priorities = []
    
    if total_value <= 0:
        return priorities
    
    vix = metrics.get('vix', 15)
    state = metrics.get('state', 'NEUTRAL')
    
    all_tickers = set(targets.keys()).union(current_holdings.keys())
    
    for tkr in all_tickers:
        target_w = targets.get(tkr, 0)
        current_val = current_holdings.get(tkr, 0)
        current_w = current_val / total_value
        diff_w = target_w - current_w
        diff_val = diff_w * total_value
        
        if abs(diff_w) < 0.02:  # åç¦»<2%å¿½ç•¥
            continue
        
        # åŸºç¡€ä¼˜å…ˆçº§åˆ†æ•° (0-100)
        priority = abs(diff_w) * 100  # åç¦»è¶Šå¤§è¶Šç´§æ€¥
        reason = []
        
        # åŠ æƒå› å­
        cat_info = ASSET_CATEGORIES.get(tkr, {})
        risk_level = cat_info.get('risk_level', 'medium')
        
        # 1. é£é™©èµ„äº§åœ¨é«˜æ³¢åŠ¨æœŸä¼˜å…ˆå‡ä»“
        if diff_w < 0 and risk_level == 'high' and vix > 20:
            priority *= 1.5
            reason.append(f"é«˜é£é™©èµ„äº§+VIX={vix:.0f}")
        
        # 2. ç›®æ ‡ä¸º0çš„èµ„äº§ä¼˜å…ˆæ¸…ä»“
        if target_w == 0 and current_val > 0:
            priority *= 1.3
            reason.append("ç›®æ ‡æ¸…ä»“")
        
        # 3. é˜²å¾¡çŠ¶æ€ä¸‹ä¼˜å…ˆå¢é…é˜²å¾¡èµ„äº§
        if state in ['DEFLATION_RECESSION', 'CAUTIOUS_VOL', 'CAUTIOUS_TREND']:
            if diff_w > 0 and cat_info.get('category') in ['å›ºæ”¶', 'å¯¹å†²', 'å•†å“']:
                priority *= 1.2
                reason.append("é˜²å¾¡æ€åŠ¿å¢é…")
        
        # 4. æç«¯æŠ„åº•çŠ¶æ€ä¼˜å…ˆå¢é…æƒç›Š
        if state == 'EXTREME_ACCUMULATION':
            if diff_w > 0 and cat_info.get('category') == 'æƒç›Š':
                priority *= 1.2
                reason.append("æŠ„åº•å¢é…")
        
        action = "ä¹°å…¥" if diff_w > 0 else "å–å‡º"
        action_detail = f"{action} ${abs(diff_val):,.0f} ({abs(diff_w)*100:.1f}%)"
        
        priorities.append({
            'ticker': tkr,
            'name': ASSET_NAMES.get(tkr, tkr),
            'priority': priority,
            'reasons': reason,
            'action': action,
            'action_detail': action_detail,
            'diff_val': diff_val,
            'diff_w': diff_w,
            'current_w': current_w,
            'target_w': target_w
        })
    
    # æŒ‰ä¼˜å…ˆçº§é™åºæ’åº
    priorities.sort(key=lambda x: x['priority'], reverse=True)
    return priorities


def estimate_rebalance_cost(priorities, cost_bps=10):
    """
    ä¼°ç®—è°ƒä»“æˆæœ¬
    cost_bps: äº¤æ˜“æˆæœ¬ (åŸºç‚¹, é»˜è®¤10bps = 0.1%)
    """
    total_turnover = sum(abs(p['diff_val']) for p in priorities)
    estimated_cost = total_turnover * cost_bps / 10000
    return total_turnover, estimated_cost


def generate_stepwise_plan(priorities, total_value, days=3):
    """
    ç”Ÿæˆåˆ†æ­¥è°ƒä»“è®¡åˆ’
    """
    if not priorities:
        return []
    
    # æŒ‰å¤©åˆ†é…æ“ä½œ
    plan = []
    total_change = sum(abs(p['diff_val']) for p in priorities)
    
    if total_change / total_value < 0.10:
        # å˜åŒ–<10%ï¼Œä¸€æ¬¡æ€§è°ƒæ•´
        plan.append({
            'day': 1,
            'description': 'ä¸€æ¬¡æ€§å®Œæˆè°ƒä»“',
            'actions': [(p['ticker'], p['action_detail']) for p in priorities]
        })
    else:
        # åˆ†æ­¥æ‰§è¡Œ
        # ç¬¬1å¤©: å–å‡ºæ“ä½œ + ç´§æ€¥ä¹°å…¥
        day1_actions = []
        day2_actions = []
        day3_actions = []
        
        for p in priorities:
            if p['diff_val'] < 0:  # å–å‡ºä¼˜å…ˆ
                day1_actions.append((p['ticker'], p['action_detail']))
            elif p['priority'] > 30:  # é«˜ä¼˜å…ˆçº§ä¹°å…¥
                day2_actions.append((p['ticker'], p['action_detail']))
            else:
                day3_actions.append((p['ticker'], p['action_detail']))
        
        if day1_actions:
            plan.append({'day': 1, 'description': 'æ‰§è¡Œå–å‡ºæ“ä½œï¼Œå›æ”¶èµ„é‡‘', 'actions': day1_actions})
        if day2_actions:
            plan.append({'day': 2, 'description': 'é«˜ä¼˜å…ˆçº§ä¹°å…¥', 'actions': day2_actions})
        if day3_actions:
            plan.append({'day': 3, 'description': 'å®Œæˆå‰©ä½™è°ƒæ•´', 'actions': day3_actions})
    
    return plan


def render_portfolio_health_card(score, details, state):
    """æ¸²æŸ“æŒä»“å¥åº·åº¦å¡ç‰‡"""
    st.markdown("### ğŸ“Š æŒä»“å¥åº·åº¦è¯„ä¼°")
    
    # å¥åº·åº¦é¢œè‰²
    if score >= 80:
        color, status = '#52c41a', 'ä¼˜ç§€'
    elif score >= 60:
        color, status = '#1890ff', 'è‰¯å¥½'
    elif score >= 40:
        color, status = '#faad14', 'éœ€è°ƒæ•´'
    else:
        color, status = '#f5222d', 'éœ€é‡é…'
    
    col1, col2 = st.columns([1, 2])
    
    with col1:
        st.markdown(f"""
        <div style="text-align:center;padding:20px;background:#f9fafb;border-radius:12px;">
            <div style="font-size:48px;font-weight:700;color:{color};">{score:.0f}</div>
            <div style="font-size:16px;color:#666;margin-top:4px;">{status}</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        # åˆ†é¡¹è¯„åˆ†
        items = [
            ('æƒé‡åç¦»', details['deviation_score'], 40),
            ('é›†ä¸­åº¦', details['concentration_score'], 20),
            ('å¤šæ ·æ€§', details['diversity_score'], 20),
            ('é˜²å¾¡é…ç½®', details['defensive_score'], 20),
        ]
        for name, score_item, max_score in items:
            pct = score_item / max_score * 100
            bar_color = '#52c41a' if pct >= 70 else ('#faad14' if pct >= 40 else '#f5222d')
            st.markdown(f"""
            <div style="margin-bottom:8px;">
                <div style="display:flex;justify-content:space-between;font-size:13px;">
                    <span>{name}</span><span>{score_item:.0f}/{max_score}</span>
                </div>
                <div style="background:#e8e8e8;height:6px;border-radius:3px;overflow:hidden;">
                    <div style="width:{pct}%;height:100%;background:{bar_color};"></div>
                </div>
            </div>
            """, unsafe_allow_html=True)


def render_risk_exposure_chart(details, targets):
    """æ¸²æŸ“é£é™©æš´éœ²åˆ†æ"""
    st.markdown("### ğŸ¯ é£é™©æš´éœ²åˆ†æ")
    
    category_weights = details.get('category_weights', {})
    
    # è®¡ç®—ç›®æ ‡ç±»åˆ«æƒé‡
    target_categories = {}
    for tkr, w in targets.items():
        cat = ASSET_CATEGORIES.get(tkr, {}).get('category', 'å…¶ä»–')
        target_categories[cat] = target_categories.get(cat, 0) + w
    
    # æ‰€æœ‰ç±»åˆ«
    all_cats = ['æƒç›Š', 'å›ºæ”¶', 'å•†å“', 'å¯¹å†²', 'å¦ç±»', 'å…¶ä»–']
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("**å½“å‰é…ç½®**")
        for cat in all_cats:
            w = category_weights.get(cat, 0)
            if w > 0 or target_categories.get(cat, 0) > 0:
                bar_color = {'æƒç›Š': '#f5222d', 'å›ºæ”¶': '#1890ff', 'å•†å“': '#faad14', 'å¯¹å†²': '#52c41a', 'å¦ç±»': '#722ed1'}.get(cat, '#999')
                st.markdown(f"""
                <div style="margin-bottom:6px;">
                    <span style="display:inline-block;width:60px;font-size:13px;">{cat}</span>
                    <span style="display:inline-block;width:120px;background:#e8e8e8;height:16px;border-radius:4px;vertical-align:middle;">
                        <span style="display:block;width:{w*100}%;height:100%;background:{bar_color};border-radius:4px;"></span>
                    </span>
                    <span style="font-size:13px;margin-left:8px;">{w*100:.1f}%</span>
                </div>
                """, unsafe_allow_html=True)
    
    with col2:
        st.markdown("**ç›®æ ‡é…ç½®**")
        for cat in all_cats:
            w = target_categories.get(cat, 0)
            if w > 0 or category_weights.get(cat, 0) > 0:
                bar_color = {'æƒç›Š': '#f5222d', 'å›ºæ”¶': '#1890ff', 'å•†å“': '#faad14', 'å¯¹å†²': '#52c41a', 'å¦ç±»': '#722ed1'}.get(cat, '#999')
                st.markdown(f"""
                <div style="margin-bottom:6px;">
                    <span style="display:inline-block;width:60px;font-size:13px;">{cat}</span>
                    <span style="display:inline-block;width:120px;background:#e8e8e8;height:16px;border-radius:4px;vertical-align:middle;">
                        <span style="display:block;width:{w*100}%;height:100%;background:{bar_color};border-radius:4px;"></span>
                    </span>
                    <span style="font-size:13px;margin-left:8px;">{w*100:.1f}%</span>
                </div>
                """, unsafe_allow_html=True)


def render_rebalance_priority_table(priorities, turnover, cost):
    """æ¸²æŸ“è°ƒä»“ä¼˜å…ˆçº§è¡¨æ ¼"""
    st.markdown("### ğŸ”¥ è°ƒä»“ä¼˜å…ˆçº§")
    st.caption(f"é¢„ä¼°æ¢æ‰‹: ${turnover:,.0f} | äº¤æ˜“æˆæœ¬: ~${cost:,.0f}")
    
    if not priorities:
        st.info("å½“å‰æŒä»“ä¸ç›®æ ‡é…ç½®åç¦»è¾ƒå°ï¼Œæ— éœ€è°ƒæ•´")
        return
    
    # æ„å»ºè¡¨æ ¼æ•°æ®
    data = []
    for i, p in enumerate(priorities, 1):
        urgency = 'ğŸ”´ ç´§æ€¥' if p['priority'] > 30 else ('ğŸŸ¡ å»ºè®®' if p['priority'] > 15 else 'ğŸŸ¢ å¯é€‰')
        reasons = ', '.join(p['reasons']) if p['reasons'] else '-'
        data.append({
            'ä¼˜å…ˆçº§': i,
            'ç´§è¿«åº¦': urgency,
            'èµ„äº§': f"{p['name']} ({p['ticker']})",
            'æ“ä½œ': p['action_detail'],
            'å½“å‰â†’ç›®æ ‡': f"{p['current_w']*100:.1f}% â†’ {p['target_w']*100:.1f}%",
            'è§¦å‘å› ç´ ': reasons
        })
    
    df = pd.DataFrame(data)
    st.dataframe(df, hide_index=True, use_container_width=True)


def render_stepwise_plan(plan):
    """æ¸²æŸ“åˆ†æ­¥è°ƒä»“è®¡åˆ’"""
    if not plan:
        return
    
    st.markdown("### ğŸ“… åˆ†æ­¥æ‰§è¡Œè®¡åˆ’")
    
    for step in plan:
        day = step['day']
        desc = step['description']
        actions = step['actions']
        
        with st.expander(f"**ç¬¬{day}å¤©**: {desc}", expanded=(day == 1)):
            for tkr, action in actions:
                st.markdown(f"- **{ASSET_NAMES.get(tkr, tkr)}** ({tkr}): {action}")


def render_enhanced_diagnosis(metrics, current_holdings, total_value, targets, change_info):
    """æ¸²æŸ“å¢å¼ºç‰ˆæŒä»“è¯Šæ–­"""
    st.markdown("---")
    st.markdown("## ğŸ”¬ æ·±åº¦æŒä»“è¯Šæ–­")
    
    # 0. v1.5 ä¼˜åŒ–æœºåˆ¶å®æ—¶çŠ¶æ€
    st.markdown("### âš™ï¸ v1.5 ä¼˜åŒ–æœºåˆ¶çŠ¶æ€")
    
    vix = metrics.get('vix', 15)
    sahm = metrics.get('sahm', 0)
    corr = metrics.get('corr', 0)
    yc = metrics.get('yield_curve', 0)
    state = metrics.get('state', 'NEUTRAL')
    
    # è®¡ç®—å„æœºåˆ¶å½“å‰çŠ¶æ€
    col_v1, col_v2, col_v3, col_v4 = st.columns(4)
    
    with col_v1:
        # ç°é‡‘ç¼“å†²çŠ¶æ€
        if state == "EXTREME_ACCUMULATION":
            cash_buffer = 0
            cash_status = "æŠ„åº•æ¨¡å¼-ä¸ç•™ç°é‡‘"
        else:
            cash_buffer = CASH_BUFFER_BASE
            if vix > CASH_BUFFER_VIX_THRESHOLD:
                extra_cash = min((vix - CASH_BUFFER_VIX_THRESHOLD) / 5 * CASH_BUFFER_VIX_SCALE, 
                                 CASH_BUFFER_MAX - CASH_BUFFER_BASE)
                cash_buffer = CASH_BUFFER_BASE + extra_cash
            cash_status = "æ­£å¸¸" if cash_buffer <= CASH_BUFFER_BASE else "å¢å¼º"
        
        st.metric(
            "ğŸ’µ ç°é‡‘ç¼“å†²",
            f"{cash_buffer*100:.1f}%",
            cash_status,
            delta_color="normal" if cash_status == "æ­£å¸¸" else "off"
        )
    
    with col_v2:
        # VIXåˆ†å±‚çŠ¶æ€
        if state == "CAUTIOUS_VOL":
            if vix >= 30:
                vix_tier = "Tier3 (IWY 10%)"
            elif vix >= 25:
                vix_tier = "Tier2 (IWY 20%)"
            else:
                vix_tier = "Tier1 (IWY 30%)"
        else:
            vix_tier = "ä¸é€‚ç”¨"
        
        st.metric(
            "ğŸ“Š VIXåˆ†å±‚",
            f"VIX={vix:.1f}",
            vix_tier,
            delta_color="off"
        )
    
    with col_v3:
        # ç›¸å…³æ€§æ¸è¿›å“åº”
        if corr > CORR_HIGH_THRESHOLD:
            corr_status = f"æœ€å¤§è°ƒæ•´ {CORR_MAX_REALLOC*100:.0f}%"
            corr_delta = "inverse"
        elif corr > CORR_MID_THRESHOLD:
            adjustment_pct = (corr - CORR_MID_THRESHOLD) / (CORR_HIGH_THRESHOLD - CORR_MID_THRESHOLD)
            realloc = adjustment_pct * CORR_MAX_REALLOC
            corr_status = f"æ¸è¿›è°ƒæ•´ {realloc*100:.1f}%"
            corr_delta = "off"
        else:
            corr_status = "æ­£å¸¸"
            corr_delta = "normal"
        
        st.metric(
            "ğŸ”— ç›¸å…³æ€§å“åº”",
            f"Corr={corr:.2f}",
            corr_status,
            delta_color=corr_delta
        )
    
    with col_v4:
        # Sahmé¢„è­¦çŠ¶æ€
        if sahm >= SAHM_EARLY_WARNING_HI:
            sahm_status = "è¡°é€€ç¡®è®¤"
            sahm_delta = "inverse"
        elif sahm >= SAHM_EARLY_WARNING_LO:
            reduction_pct = int((sahm - SAHM_EARLY_WARNING_LO) / (SAHM_EARLY_WARNING_HI - SAHM_EARLY_WARNING_LO) * SAHM_REDUCTION_RATE * 100)
            sahm_status = f"é¢„è­¦ -{reduction_pct}%"
            sahm_delta = "off"
        else:
            sahm_status = "æ­£å¸¸"
            sahm_delta = "normal"
        
        st.metric(
            "ğŸ“‰ Sahmé¢„è­¦",
            f"Sahm={sahm:.2f}",
            sahm_status,
            delta_color=sahm_delta
        )
    
    # ç¬¬äºŒè¡Œä¼˜åŒ–çŠ¶æ€
    col_v5, col_v6, col_v7, col_v8 = st.columns(4)
    
    with col_v5:
        # æ”¶ç›Šç‡æ›²çº¿ä¿æŠ¤
        yc_un_invert = metrics.get('yc_un_invert', False)
        if yc < 0:
            yc_status = "å€’æŒ‚ä¸­"
            yc_delta = "inverse"
        elif yc_un_invert:
            yc_status = f"è§£å€’æŒ‚ä¿æŠ¤ -{YC_UNINVERT_REDUCTION*100:.0f}%"
            yc_delta = "off"
        else:
            yc_status = "æ­£å¸¸"
            yc_delta = "normal"
        
        st.metric(
            "ğŸ“ˆ æ›²çº¿ä¿æŠ¤",
            f"10Y-2Y={yc:.2f}%",
            yc_status,
            delta_color=yc_delta
        )
    
    with col_v6:
        # å¸‚åœºå¹¿åº¦ï¼ˆä¼°ç®—ï¼‰
        asset_trends = metrics.get('asset_trends', {})
        if asset_trends:
            bullish_count = sum(1 for bear in asset_trends.values() if not bear)
            total_count = len(asset_trends)
            breadth = bullish_count / total_count if total_count > 0 else 0.5
        else:
            breadth = 0.5  # é»˜è®¤ä¸­æ€§
        
        if breadth < MARKET_BREADTH_LOW:
            breadth_status = f"ä½å¹¿åº¦ -{BREADTH_LOW_REDUCTION*100:.0f}%"
        elif breadth < MARKET_BREADTH_MID:
            breadth_status = f"ä¸€èˆ¬ -{BREADTH_MID_REDUCTION*100:.0f}%"
        else:
            breadth_status = "æ­£å¸¸"
        
        st.metric(
            "ğŸ“Š å¸‚åœºå¹¿åº¦",
            f"{breadth*100:.0f}%",
            breadth_status
        )
    
    with col_v7:
        # ä¿¡å·ç¡®è®¤
        days_in_state = change_info.get('days_in_state') if change_info else None
        if days_in_state is not None and days_in_state <= SIGNAL_CONFIRM_DAYS:
            confirm_status = f"ç¡®è®¤ä¸­ ({days_in_state}/{SIGNAL_CONFIRM_DAYS})"
        else:
            confirm_status = "å·²ç¡®è®¤"
        
        st.metric(
            "ğŸ”„ ä¿¡å·ç¡®è®¤",
            f"{days_in_state or 0}å¤©",
            confirm_status
        )
    
    with col_v8:
        # å†å¹³è¡¡çŠ¶æ€
        max_dev = 0
        for tkr in set(targets.keys()).union(current_holdings.keys()):
            target_w = targets.get(tkr, 0)
            current_val = current_holdings.get(tkr, 0)
            current_w = current_val / total_value if total_value > 0 else 0
            dev = abs(target_w - current_w)
            max_dev = max(max_dev, dev)
        
        if max_dev > REBALANCE_THRESHOLD:
            rebal_status = "éœ€è¦è°ƒä»“"
        else:
            rebal_status = "æ— éœ€è°ƒä»“"
        
        st.metric(
            "ğŸ“ å†å¹³è¡¡å¸¦",
            f"æœ€å¤§åç¦» {max_dev*100:.1f}%",
            rebal_status,
            delta_color="inverse" if max_dev > REBALANCE_THRESHOLD else "normal"
        )
    
    # === æ–°å¢ï¼šæ­¢æŸçŠ¶æ€é¢æ¿ ===
    if total_value > 0:
        st.markdown("---")
        st.markdown("### ğŸ›¡ï¸ æ­¢æŸçŠ¶æ€ç›‘æ§")
        
        stop_loss_status = get_stop_loss_status(total_value)
        drawdown_pct = stop_loss_status['drawdown_pct']
        peak_value = stop_loss_status['peak_value']
        days_since_peak = stop_loss_status['days_since_peak']
        in_stop_loss = stop_loss_status['in_stop_loss']
        recovery_ratio = stop_loss_status['recovery_ratio']
        
        col_sl1, col_sl2, col_sl3, col_sl4 = st.columns(4)
        
        with col_sl1:
            if drawdown_pct < DRAWDOWN_STOP_LOSS:
                dd_color = "inverse"
            elif drawdown_pct < DRAWDOWN_RECOVERY_THRESHOLD:
                dd_color = "off"
            else:
                dd_color = "normal"
            st.metric(
                "ğŸ“‰ å½“å‰å›æ’¤",
                f"{drawdown_pct*100:.1f}%",
                f"æ­¢æŸçº¿: {DRAWDOWN_STOP_LOSS*100:.0f}%",
                delta_color=dd_color
            )
        
        with col_sl2:
            st.metric(
                "ğŸ“ˆ å†å²æœ€é«˜",
                f"${peak_value:,.0f}",
                f"å½“å‰: ${total_value:,.0f}"
            )
        
        with col_sl3:
            st.metric(
                "ğŸ“… è·å³°å€¼",
                f"{days_since_peak} å¤©",
                "æŒç»­æ—¶é—´"
            )
        
        with col_sl4:
            stage = stop_loss_status.get('stage', 'æ­£å¸¸')
            stage_color = stop_loss_status.get('stage_color', '#52c41a')
            if in_stop_loss:
                st.metric(
                    "ğŸš¨ æ­¢æŸé˜¶æ®µ",
                    f"å‡ä»“è‡³{recovery_ratio*100:.0f}%",
                    stage,
                    delta_color="inverse"
                )
            else:
                st.metric(
                    "âœ… æ­¢æŸçŠ¶æ€",
                    stage,
                    stop_loss_status.get('advice', ''),
                    delta_color="normal" if stage == "æ­£å¸¸" else "off"
                )
        
        # æ­¢æŸæ“ä½œå»ºè®®
        if in_stop_loss:
            st.error(f"""
            **âš ï¸ æ­¢æŸè§¦å‘ï¼** å½“å‰å›æ’¤ {drawdown_pct*100:.1f}% å·²è¶…è¿‡æ­¢æŸçº¿ {DRAWDOWN_STOP_LOSS*100:.0f}%ã€‚
            
            **å»ºè®®æ“ä½œï¼š**
            - å°†é£é™©èµ„äº§ï¼ˆIWY, G3B.SI, LVHIç­‰ï¼‰å‡ä»“è‡³ç›®æ ‡çš„ **{recovery_ratio*100:.0f}%**
            - é‡Šæ”¾èµ„é‡‘è½¬å…¥ WTMF æˆ– ç°é‡‘
            - å›æ’¤æ¢å¤è‡³ **{DRAWDOWN_RECOVERY_THRESHOLD*100:.0f}%** å†…åå†é€æ­¥æ¢å¤ä»“ä½
            """)
        elif drawdown_pct < -0.05:
            st.warning(f"""
            **âš ï¸ æ¥è¿‘æ­¢æŸçº¿ï¼** å½“å‰å›æ’¤ {drawdown_pct*100:.1f}%ï¼Œè·æ­¢æŸçº¿ {DRAWDOWN_STOP_LOSS*100:.0f}% ä»…å·® {(DRAWDOWN_STOP_LOSS - drawdown_pct)*100:.1f}%ã€‚
            
            å»ºè®®ï¼šé™ä½é£é™©æ•å£æˆ–è®¾ç½®ç›˜ä¸­ä»·æ ¼æé†’ã€‚
            """)
        
        # é‡ç½®å³°å€¼æŒ‰é’®ï¼ˆç”¨äºæ³¨å…¥æ–°èµ„é‡‘åï¼‰
        with st.expander("âš™ï¸ æ­¢æŸè®¾ç½®", expanded=False):
            st.caption("å¦‚æœæ‚¨æ–°æ³¨å…¥èµ„é‡‘ï¼Œå¯é‡ç½®å†å²æœ€é«˜å‡€å€¼ä»¥é¿å…è¯¯è§¦æ­¢æŸã€‚")
            col_reset1, col_reset2 = st.columns(2)
            with col_reset1:
                if st.button("ğŸ”„ é‡ç½®ä¸ºå½“å‰å‡€å€¼", help="å°†å†å²æœ€é«˜å‡€å€¼é‡ç½®ä¸ºå½“å‰æ€»å¸‚å€¼"):
                    reset_portfolio_peak(total_value)
                    st.success(f"å·²é‡ç½®å†å²æœ€é«˜å‡€å€¼ä¸º ${total_value:,.0f}")
                    st.rerun()
            with col_reset2:
                new_peak = st.number_input("æ‰‹åŠ¨è®¾ç½®å³°å€¼", value=float(peak_value), step=1000.0, key="manual_peak")
                if st.button("ç¡®è®¤è®¾ç½®"):
                    reset_portfolio_peak(new_peak)
                    st.success(f"å·²è®¾ç½®å†å²æœ€é«˜å‡€å€¼ä¸º ${new_peak:,.0f}")
                    st.rerun()
    
    st.markdown("---")
    
    # 1. å¥åº·åº¦è¯„ä¼°
    score, details = calculate_portfolio_health(current_holdings, targets, total_value)
    render_portfolio_health_card(score, details, metrics.get('state'))
    
    st.markdown("---")
    
    # 2. é£é™©æš´éœ²åˆ†æ
    render_risk_exposure_chart(details, targets)
    
    st.markdown("---")
    
    # 3. è°ƒä»“ä¼˜å…ˆçº§
    priorities = generate_rebalance_priority(current_holdings, targets, total_value, metrics)
    turnover, cost = estimate_rebalance_cost(priorities)
    render_rebalance_priority_table(priorities, turnover, cost)
    
    # 4. åˆ†æ­¥æ‰§è¡Œè®¡åˆ’
    plan = generate_stepwise_plan(priorities, total_value)
    render_stepwise_plan(plan)


def render_execution_tips(tips):
    """æ¸²æŸ“æ‰§è¡Œå»ºè®®æç¤ºå¡ç‰‡"""
    if not tips:
        return
    
    st.markdown("### ğŸ’¡ æ‰§è¡Œå»ºè®® (Execution Tips)")
    st.caption("åŸºäºå›æµ‹ä¼˜åŒ–æœºåˆ¶çš„å®æ—¶æ“ä½œå‚è€ƒ")
    
    for tip in tips:
        tip_type = tip.get('type', 'info')
        icon = tip.get('icon', 'ğŸ’¡')
        title = tip.get('title', '')
        content = tip.get('content', '')
        
        if tip_type == 'error':
            bg_color = '#fff2f0'
            border_color = '#ffccc7'
            text_color = '#cf1322'
        elif tip_type == 'warning':
            bg_color = '#fffbe6'
            border_color = '#ffe58f'
            text_color = '#ad6800'
        elif tip_type == 'success':
            bg_color = '#f6ffed'
            border_color = '#b7eb8f'
            text_color = '#389e0d'
        else:  # info
            bg_color = '#e6f7ff'
            border_color = '#91d5ff'
            text_color = '#0050b3'
        
        st.markdown(f"""
        <div style="background:{bg_color};border:1px solid {border_color};border-radius:8px;padding:12px 16px;margin-bottom:10px;">
            <div style="font-weight:600;color:{text_color};margin-bottom:4px;">{icon} {title}</div>
            <div style="color:#333;font-size:14px;line-height:1.5;">{content}</div>
        </div>
        """, unsafe_allow_html=True)


def calculate_equity_curve_metrics(series, risk_free_rate=0.03):
    """
    Calculates comprehensive performance metrics for an equity curve.
    series: pd.Series of portfolio values or prices, indexed by datetime.
    risk_free_rate: Annualized risk-free rate (decimal).
    """
    if series.empty or len(series) < 2:
        return {}
    
    # 1. Basic Returns
    total_return = (series.iloc[-1] / series.iloc[0] - 1) * 100
    days = (series.index[-1] - series.index[0]).days
    if days > 0:
        cagr = ((series.iloc[-1] / series.iloc[0]) ** (365 / days) - 1) * 100
    else:
        cagr = 0.0

    # 2. Drawdown & Duration (Updated)
    rolling_max = series.cummax()
    drawdown = (series / rolling_max - 1) * 100
    max_dd = drawdown.min()

    # Calculate Max Drawdown Duration (Days Underwater)
    # Logic: Find peaks, fill dates forward, subtract current date from last peak date
    is_peak = series == rolling_max
    peak_dates = pd.Series(series.index, index=series.index).where(is_peak).ffill()
    dd_days = series.index - peak_dates
    max_dd_days = dd_days.max().days if not dd_days.empty else 0
    
    # 3. Daily Returns Analysis
    daily_ret = series.pct_change().fillna(0)
    
    # 4. Volatility (Annualized)
    vol = daily_ret.std() * np.sqrt(252) * 100
    
    # 5. Risk-Adjusted Returns
    rf_daily = risk_free_rate / 252
    excess_ret = daily_ret - rf_daily
    
    if daily_ret.std() > 0:
        sharpe = (excess_ret.mean() / daily_ret.std()) * np.sqrt(252)
    else:
        sharpe = 0.0
        
    # Sortino (Downside Deviation)
    downside_ret = daily_ret[daily_ret < 0]
    if len(downside_ret) > 0:
        downside_std = downside_ret.std() * np.sqrt(252)
        if downside_std > 0:
            sortino = (excess_ret.mean() * 252) / downside_std 
        else:
            sortino = 0.0
    else:
        sortino = 0.0 
        
    # Calmar
    if abs(max_dd) > 0:
        calmar = cagr / abs(max_dd)
    else:
        calmar = 0.0
        
    # 6. Trade/Win Analysis
    winning_days = daily_ret[daily_ret > 0].count()
    losing_days = daily_ret[daily_ret < 0].count()
    total_trading_days = winning_days + losing_days
    
    win_rate = (winning_days / total_trading_days * 100) if total_trading_days > 0 else 0.0
    
    avg_win = daily_ret[daily_ret > 0].mean() if winning_days > 0 else 0
    avg_loss = abs(daily_ret[daily_ret < 0].mean()) if losing_days > 0 else 0
    
    pl_ratio = (avg_win / avg_loss) if avg_loss > 0 else 0.0

    # 7. Annual Returns (New)
    annual_rets = {}
    yearly_vals = series.groupby(series.index.year).last()
    previous_val = series.iloc[0]
    
    for year in yearly_vals.index:
        current_val = yearly_vals.loc[year]
        # Return for the year = (End Value / Start Value) - 1
        ret = (current_val / previous_val) - 1
        annual_rets[f"{year} (%)"] = ret * 100
        previous_val = current_val

    # Construct Final Result
    results = {
        "Total Return (%)": total_return,
        "CAGR (%)": cagr,
        "Max Drawdown (%)": max_dd,
        "Max DD Days": max_dd_days, # æ–°å¢å­—æ®µ
        "Volatility (%)": vol,
        "Sharpe Ratio": sharpe,
        "Sortino Ratio": sortino,
        "Calmar Ratio": calmar,
        "Win Rate (Daily %)": win_rate,
        "Profit/Loss Ratio": pl_ratio
    }
    
    # Merge Annual Returns into results
    results.update(annual_rets)

    return results

def run_dynamic_backtest(df_states, start_date, end_date, initial_capital=10000.0, ma_window=200, use_proxies=False, rebal_freq='Daily', transaction_cost_bps=10):
    """
    Simulates the strategy over historical states.
    df_states: DataFrame with 'State', 'Gold_Bear', 'Value_Regime' columns, indexed by Date.
    rebal_freq: 'Daily', 'Weekly', 'Monthly', 'Quarterly'
    transaction_cost_bps: äº¤æ˜“æˆæœ¬ï¼ˆåŸºç‚¹ï¼‰ï¼Œé»˜è®¤10bps=0.1%ï¼ŒåŒ…å«ä½£é‡‘+æ»‘ç‚¹
    
    å…³é”®æ”¹è¿›ï¼ˆv1.6ï¼‰ï¼š
    - ä½¿ç”¨T-1æ—¥ä¿¡å·å†³å®šTæ—¥é…ç½®ï¼Œé¿å…å‰è§†åå·®
    - è®¡å…¥äº¤æ˜“æˆæœ¬
    """
    ensure_fred_cached()
    # 1. Define Asset Universe
    # If using proxies (for long-term history > 20 years), we map ETFs to Indices
    if use_proxies:
        # Proxy Mapping:
        # IWY -> ^GSPC (S&P 500) as generic equity
        # WTMF -> Cash (Simulated) or similar? Hard to proxy. We'll use Gold as partial proxy or just Cash?
        # Let's map WTMF to Gold for Crisis Alpha in history? Or just Cash.
        # Let's use ^GSPC for Equity, TLT for Bonds (needs check), GLD for Gold.
        # Note: Yahoo data for GLD starts 2004. TLT 2002. 
        # For meaningful 1990s backtest, we need Indices.
        # But yfinance index data for 'Total Return' is hard. ^GSPC is price only (no div).
        assets = ['^GSPC', '^NDX', 'TLT', 'GLD', 'VUSTX', 'GC=F'] # Minimal set
    else:
        assets = ['IWY', 'WTMF', 'LVHI', 'G3B.SI', 'MBH.SI', 'GSD.SI', 'SRT.SI', 'AJBU.SI', 'TLT', 'SPY']
    
    # 2. Fetch Price Data
    fetch_start = pd.to_datetime(start_date) - pd.Timedelta(days=365)
    
    try:
        price_data = yf.download(assets, start=fetch_start, end=end_date, progress=False, auto_adjust=False)['Adj Close']
    except:
        # Fallback if Adj Close issue
        try:
             price_data = yf.download(assets, start=fetch_start, end=end_date, progress=False, auto_adjust=False)['Close']
        except Exception as e:
            return None, None, f"Data fetch failed: {e}"

    if price_data.empty:
         return None, None, "No price data fetched."

    # Fill missing
    price_data = price_data.ffill().bfill()
    
    # Calculate Asset Trends for Backtest (Dual Momentum)
    # Use dynamic MA window
    ma_all = price_data.rolling(ma_window).mean()
    trend_bear_all = price_data < ma_all
    
    # Filter to requested range
    mask = (price_data.index >= pd.to_datetime(start_date)) & (price_data.index <= pd.to_datetime(end_date))
    price_data = price_data[mask]
    trend_bear_all = trend_bear_all[mask]
    
    if price_data.empty:
         return None, None, "Price data empty after filtering."

    # Align states with prices
    common_idx = price_data.index.intersection(df_states.index)
    price_data = price_data.loc[common_idx]
    trend_bear_all = trend_bear_all.loc[common_idx]
    df_states = df_states.loc[common_idx]
    
    if len(price_data) < 10:
        return None, None, "Insufficient data points for backtest."

    # 3. Strategy Simulation (Daily Rebalancing Approximation)
    portfolio_values = []
    current_val = initial_capital
    
    # Track allocation history
    history_records = []
    
    # Turnover tracking
    prev_targets = {}
    prev_rets = None
    
    # === ä¼˜åŒ–æœºåˆ¶çŠ¶æ€å˜é‡ ===
    # æ³¢åŠ¨ç‡ç›®æ ‡æœºåˆ¶
    portfolio_returns_history = []  # ç”¨äºè®¡ç®—å®ç°æ³¢åŠ¨ç‡
    
    # åŠ¨æ€æ­¢æŸæœºåˆ¶
    peak_nav = initial_capital  # å†å²æœ€é«˜å‡€å€¼
    in_stop_loss_mode = False  # æ˜¯å¦å¤„äºæ­¢æŸæ¨¡å¼
    
    # ä¿¡å·ç¡®è®¤å»¶è¿Ÿæœºåˆ¶
    pending_state = None  # å¾…ç¡®è®¤çš„æ–°çŠ¶æ€
    pending_state_days = 0  # å¾…ç¡®è®¤çŠ¶æ€çš„è¿ç»­å¤©æ•°
    confirmed_state = None  # å·²ç¡®è®¤çš„çŠ¶æ€
    
    # çŠ¶æ€è½¬æ¢å¹³æ»‘æœºåˆ¶
    transition_from_weights = None  # è¿‡æ¸¡èµ·å§‹æƒé‡
    transition_day = 0  # å½“å‰è¿‡æ¸¡å¤©æ•°
    is_in_transition = False  # æ˜¯å¦æ­£åœ¨è¿‡æ¸¡
    
    # We iterate daily. To speed up, we could vectorise, but logic is complex.
    # Logic: Daily return = Sum(Weight_i * Return_i)
    # Rebalancing frequency controls when we update target weights.
    
    returns_df = price_data.pct_change().fillna(0)
    
    # Determine rebalancing dates based on frequency
    def is_rebalance_day(date, freq, prev_date=None):
        """Check if current date is a rebalancing day."""
        if freq == 'Daily':
            return True
        elif freq == 'Weekly':
            # Rebalance on Monday (weekday=0)
            return date.weekday() == 0
        elif freq == 'Monthly':
            # Rebalance on first trading day of month
            if prev_date is None:
                return True
            return date.month != prev_date.month
        elif freq == 'Quarterly':
            # Rebalance on first trading day of quarter
            if prev_date is None:
                return True
            return (date.month - 1) // 3 != (prev_date.month - 1) // 3
        return True
    
    # Proxy Mapper Function
    def map_target_to_asset(target_ticker, current_date=None):
        if not use_proxies:
            return target_ticker
        
        # Simple Proxy Logic
        if target_ticker in ['IWY', 'G3B.SI', 'LVHI', 'SRT.SI', 'AJBU.SI', 'SPY']:
            if '^NDX' in price_data.columns and target_ticker == 'IWY': return '^NDX'
            return '^GSPC' if '^GSPC' in price_data.columns else target_ticker
        if target_ticker in ['TLT', 'MBH.SI']:
            if 'VUSTX' in price_data.columns: return 'VUSTX'
            return 'TLT' 
        if target_ticker in ['GSD.SI']:
            if current_date and current_date < pd.Timestamp('2004-11-18') and 'GC=F' in price_data.columns:
                return 'GC=F'
            return 'GLD'
        if target_ticker in ['WTMF']:
            return 'CASH' # Simulate Cash for managed futures in proxy mode
        return target_ticker

    prev_date = None
    
    # === å…³é”®ä¿®å¤ï¼šä½¿ç”¨T-1æ—¥ä¿¡å·å†³å®šTæ—¥é…ç½®ï¼ˆé¿å…å‰è§†åå·®ï¼‰===
    # é¢„å­˜å‰ä¸€å¤©çš„çŠ¶æ€ä¿¡æ¯ç”¨äºå½“å¤©å†³ç­–
    prev_row_state = None  # T-1æ—¥çš„çŠ¶æ€
    prev_row_gb = None     # T-1æ—¥çš„Gold_Bear
    prev_row_vr = None     # T-1æ—¥çš„Value_Regime
    prev_row_date = None   # T-1æ—¥çš„æ—¥æœŸ
    
    for date, row in df_states.iterrows():
        # ===ã€é‡è¦ã€‘ä½¿ç”¨T-1æ—¥çš„çŠ¶æ€æ¥å†³å®šTæ—¥é…ç½® ===
        # è¿™æ¨¡æ‹Ÿäº†çœŸå®äº¤æ˜“ï¼šT-1æ”¶ç›˜åçœ‹åˆ°æ•°æ®ï¼ŒTæ—¥å¼€ç›˜æ‰§è¡Œ
        if prev_row_state is None:
            # ç¬¬ä¸€å¤©ï¼šæ— å‰ä¸€å¤©æ•°æ®ï¼Œä½¿ç”¨å½“å¤©ï¼ˆè¿™æ˜¯ä¸å¯é¿å…çš„ï¼‰
            raw_state = row['State']
            gb = row['Gold_Bear']
            vr = row['Value_Regime']
            decision_date = date  # ç”¨äºè·å–è¶‹åŠ¿ç­‰è¾…åŠ©ä¿¡æ¯
        else:
            # ä½¿ç”¨T-1æ—¥çš„çŠ¶æ€ä¿¡æ¯åšTæ—¥å†³ç­–
            raw_state = prev_row_state
            gb = prev_row_gb
            vr = prev_row_vr
            decision_date = prev_row_date  # ä½¿ç”¨T-1æ—¥çš„è¶‹åŠ¿ä¿¡æ¯
        
        # ä¿å­˜å½“å¤©çŠ¶æ€ä¾›ä¸‹ä¸€å¤©ä½¿ç”¨
        prev_row_state = row['State']
        prev_row_gb = row['Gold_Bear']
        prev_row_vr = row['Value_Regime']
        prev_row_date = date
        
        # === ä¼˜åŒ–1: ä¿¡å·ç¡®è®¤å»¶è¿Ÿæœºåˆ¶ ===
        # çŠ¶æ€åˆ‡æ¢éœ€è¿ç»­ SIGNAL_CONFIRM_DAYS å¤©ç¡®è®¤æ‰ç”Ÿæ•ˆ
        # v1.6: EXTREME_ACCUMULATIONä½¿ç”¨æ›´å¿«ç¡®è®¤ï¼ˆæŠ„åº•æœºä¼šç¨çºµå³é€ï¼‰
        if confirmed_state is None:
            # é¦–æ—¥ç›´æ¥ç¡®è®¤
            confirmed_state = raw_state
            pending_state = None
            pending_state_days = 0
        elif raw_state != confirmed_state:
            # æ£€æµ‹åˆ°çŠ¶æ€å˜åŒ–
            # v1.6: æ ¹æ®ç›®æ ‡çŠ¶æ€é€‰æ‹©ç¡®è®¤å¤©æ•°
            required_confirm_days = EXTREME_CONFIRM_DAYS if raw_state == "EXTREME_ACCUMULATION" else SIGNAL_CONFIRM_DAYS
            
            if pending_state == raw_state:
                # ç»§ç»­ç¡®è®¤åŒä¸€ä¸ªå¾…åˆ‡æ¢çŠ¶æ€
                pending_state_days += 1
                if pending_state_days >= required_confirm_days:
                    # ç¡®è®¤åˆ‡æ¢ï¼å¯åŠ¨è¿‡æ¸¡
                    confirmed_state = raw_state
                    pending_state = None
                    pending_state_days = 0
                    # æ ‡è®°å¼€å§‹çŠ¶æ€è¿‡æ¸¡
                    is_in_transition = True
                    transition_day = 0
                    transition_from_weights = prev_targets.copy() if prev_targets else None
            else:
                # æ–°çš„å¾…åˆ‡æ¢çŠ¶æ€
                pending_state = raw_state
                pending_state_days = 1
        else:
            # çŠ¶æ€å›å½’åˆ°å·²ç¡®è®¤çŠ¶æ€ï¼Œå–æ¶ˆå¾…ç¡®è®¤
            pending_state = None
            pending_state_days = 0
        
        # ä½¿ç”¨ç¡®è®¤åçš„çŠ¶æ€
        s = confirmed_state
        
        # Check if this is a rebalancing day
        should_rebalance = is_rebalance_day(date, rebal_freq, prev_date)
        
        # Get trends for this date - ä½¿ç”¨decision_dateï¼ˆT-1æ—¥ï¼‰æ¥è·å–è¶‹åŠ¿ä¿¡æ¯
        # è¿™ç¡®ä¿å†³ç­–åŸºäºå‰ä¸€å¤©çš„ä¿¡æ¯
        daily_trends = {}
        trend_lookup_date = decision_date if decision_date in trend_bear_all.index else date
        
        if use_proxies:
            proxy_trend_bear = False
            if '^GSPC' in trend_bear_all.columns and trend_lookup_date in trend_bear_all.index:
                proxy_trend_bear = trend_bear_all.loc[trend_lookup_date]['^GSPC']
            
            for t in ['IWY', 'G3B.SI', 'LVHI', 'SRT.SI', 'AJBU.SI']:
                daily_trends[t] = proxy_trend_bear
                
            gold_proxy = 'GLD'
            if trend_lookup_date < pd.Timestamp('2004-11-18') and 'GC=F' in trend_bear_all.columns:
                 gold_proxy = 'GC=F'
            
            if gold_proxy in trend_bear_all.columns and trend_lookup_date in trend_bear_all.index:
                daily_trends['GSD.SI'] = trend_bear_all.loc[trend_lookup_date][gold_proxy]

            bond_proxy = 'TLT'
            if 'VUSTX' in trend_bear_all.columns:
                bond_proxy = 'VUSTX'
            
            if bond_proxy in trend_bear_all.columns and trend_lookup_date in trend_bear_all.index:
                daily_trends['MBH.SI'] = trend_bear_all.loc[trend_lookup_date][bond_proxy]
        else:
            if trend_lookup_date in trend_bear_all.index:
                daily_trends = trend_bear_all.loc[trend_lookup_date].to_dict()
        
        # === ä½¿ç”¨T-1æ—¥çš„æŒ‡æ ‡æ•°æ®åšå†³ç­– ===
        # ä»df_statesä¸­è·å–decision_dateå¯¹åº”çš„æŒ‡æ ‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if decision_date in df_states.index:
            decision_row = df_states.loc[decision_date]
            vix_val = decision_row.get('VIX')
            yc_val = decision_row.get('YieldCurve')
            sahm_val = decision_row.get('Sahm')
            corr_val = decision_row.get('Corr')
        else:
            vix_val = row.get('VIX')
            yc_val = row.get('YieldCurve')
            sahm_val = row.get('Sahm')
            corr_val = row.get('Corr')
        
        # è®¡ç®—åŠ¨é‡å¼ºåº¦åˆ†æ•° (price - ma) / ma - ä½¿ç”¨T-1æ—¥æ•°æ®
        momentum_scores = {}
        momentum_date = decision_date if decision_date in price_data.index else date
        if momentum_date in price_data.index and momentum_date in ma_all.index:
            for ticker in price_data.columns:
                try:
                    p = price_data.loc[momentum_date, ticker]
                    m = ma_all.loc[momentum_date, ticker]
                    if pd.notna(p) and pd.notna(m) and m > 0:
                        momentum_scores[ticker] = (p - m) / m
                except:
                    pass
            # æ˜ å°„ä»£ç†èµ„äº§çš„åŠ¨é‡åˆ°åŸå§‹èµ„äº§
            if use_proxies and '^GSPC' in momentum_scores:
                momentum_scores['IWY'] = momentum_scores['^GSPC']
        
        # æ£€æŸ¥è¿‘æœŸVIXå³°å€¼ï¼ˆç”¨äºå‡å€¼å›å½’åŠ ä»“ï¼‰- åŸºäºdecision_date
        vix_recent_peak = None
        if 'VIX' in df_states.columns and decision_date in df_states.index:
            lookback_start = max(0, df_states.index.get_loc(decision_date) - 60)
            vix_history = df_states['VIX'].iloc[lookback_start:df_states.index.get_loc(decision_date)+1]
            if len(vix_history) > 0:
                vix_recent_peak = vix_history.max()
        
        # æ£€æŸ¥è¿‘12ä¸ªæœˆæ˜¯å¦æ›¾æ·±åº¦å€’æŒ‚ - åŸºäºdecision_date
        yc_recently_inverted = False
        if 'YieldCurve' in df_states.columns and decision_date in df_states.index:
            lookback_start = max(0, df_states.index.get_loc(decision_date) - 252)
            yc_history = df_states['YieldCurve'].iloc[lookback_start:df_states.index.get_loc(decision_date)+1]
            if len(yc_history) > 0:
                yc_recently_inverted = (yc_history.min() < -0.20)
        
        # Calculate base target weights (with new optimization parameters)
        targets = get_target_percentages(
            s, gold_bear=gb, value_regime=vr, asset_trends=daily_trends, 
            vix=vix_val, yield_curve=yc_val,
            sahm=sahm_val, corr=corr_val, momentum_scores=momentum_scores,
            yc_recently_inverted=yc_recently_inverted, vix_recent_peak=vix_recent_peak
        )
        
        # === ä¼˜åŒ–4: VIXå“åº”å¹³æ»‘åŒ– ===
        # æ›¿ä»£åŸæœ‰çš„é˜¶æ¢¯å¼VIXè°ƒæ•´ï¼Œä½¿ç”¨è¿ç»­å‡½æ•°
        if s == "NEUTRAL" and vix_val is not None and vix_val > VIX_SMOOTH_START:
            # çº¿æ€§å¹³æ»‘å“åº”: VIXä»15åˆ°30çº¿æ€§å‡ä»“0åˆ°40%
            smooth_reduction = min((vix_val - VIX_SMOOTH_START) / (VIX_SMOOTH_END - VIX_SMOOTH_START) * VIX_MAX_REDUCTION, VIX_MAX_REDUCTION)
            iwy_current = targets.get('IWY', 0)
            move_amt = iwy_current * smooth_reduction
            targets['IWY'] = iwy_current - move_amt
            targets['WTMF'] = targets.get('WTMF', 0) + move_amt
        
        # --- Map Targets to Available Assets (Proxy Translation) ---
        new_target_weights = {}
        for t, w in targets.items():
            mapped_asset = map_target_to_asset(t, date)
            if mapped_asset != 'CASH' and mapped_asset in price_data.columns:
                new_target_weights[mapped_asset] = new_target_weights.get(mapped_asset, 0.0) + w
        
        # --- Calculate Drifted Weights from previous day ---
        drifted_weights = {}
        if prev_targets:
            drifted_values = {}
            total_drifted_val = 0.0
            
            for t, w in prev_targets.items():
                r = 0.0
                if prev_rets is not None and t in prev_rets:
                    r = prev_rets[t]
                val = w * (1 + r)
                drifted_values[t] = val
                total_drifted_val += val
                
            prev_cash_w = max(0.0, 1.0 - sum(prev_targets.values()))
            drifted_cash_val = prev_cash_w * 1.0
            total_drifted_val += drifted_cash_val
            
            if total_drifted_val > 0:
                drifted_weights = {t: v / total_drifted_val for t, v in drifted_values.items()}
            else:
                drifted_weights = prev_targets.copy()
        
        # === ä¼˜åŒ–6: çŠ¶æ€è½¬æ¢å¹³æ»‘è¿‡æ¸¡ ===
        # æ–°æ—§æƒé‡æŒ‰è¿‡æ¸¡å¤©æ•°åŠ æƒæ··åˆ
        if is_in_transition and transition_from_weights:
            transition_day += 1
            transition_progress = min(transition_day / STATE_TRANSITION_DAYS, 1.0)
            
            # æ··åˆæƒé‡
            blended_weights = {}
            all_assets = set(new_target_weights.keys()) | set(transition_from_weights.keys())
            for asset in all_assets:
                old_w = transition_from_weights.get(asset, 0.0)
                new_w = new_target_weights.get(asset, 0.0)
                blended_weights[asset] = old_w * (1 - transition_progress) + new_w * transition_progress
            
            new_target_weights = blended_weights
            
            if transition_day >= STATE_TRANSITION_DAYS:
                is_in_transition = False
                transition_from_weights = None
                transition_day = 0
        
        # === ä¼˜åŒ–5: å†å¹³è¡¡å®¹å¿å¸¦ ===
        # åªæœ‰å½“æƒé‡åç¦»è¶…è¿‡é˜ˆå€¼æ—¶æ‰å†å¹³è¡¡
        needs_rebalance_by_threshold = False
        if drifted_weights and new_target_weights:
            all_assets = set(new_target_weights.keys()) | set(drifted_weights.keys())
            for asset in all_assets:
                target_w = new_target_weights.get(asset, 0.0)
                drifted_w = drifted_weights.get(asset, 0.0)
                if abs(target_w - drifted_w) > REBALANCE_THRESHOLD:
                    needs_rebalance_by_threshold = True
                    break
        
        # ç»¼åˆåˆ¤æ–­æ˜¯å¦å†å¹³è¡¡
        should_actually_rebalance = (should_rebalance and needs_rebalance_by_threshold) or not prev_targets or is_in_transition
        
        # --- Determine actual weights for today ---
        if should_actually_rebalance:
            final_weights = new_target_weights
        else:
            final_weights = drifted_weights if drifted_weights else new_target_weights
        
        # === ä¼˜åŒ–2: æ³¢åŠ¨ç‡ç›®æ ‡æœºåˆ¶ (ä½¿ç”¨T-1æ•°æ®ï¼Œé¿å…å‰è§†åå·®) ===
        # æ ¹æ®å®ç°æ³¢åŠ¨ç‡è°ƒæ•´ä»“ä½
        # å…³é”®ä¿®å¤ï¼šä½¿ç”¨portfolio_returns_history[:-1]ï¼Œå³ä¸åŒ…å«å½“å¤©çš„æ”¶ç›Šï¼ˆå½“å¤©æ”¶ç›Šå°šæœªå‘ç”Ÿï¼‰
        # è¿™æ ·ç¡®ä¿åœ¨tæ—¥åšå†³ç­–æ—¶ï¼Œåªä½¿ç”¨t-1åŠä¹‹å‰çš„ä¿¡æ¯
        vol_history_for_calc = portfolio_returns_history[:-1] if len(portfolio_returns_history) > 1 else []
        if len(vol_history_for_calc) >= VOL_LOOKBACK:
            realized_vol = np.std(vol_history_for_calc[-VOL_LOOKBACK:]) * np.sqrt(252)
            if realized_vol > 0:
                vol_scalar = TARGET_VOL / realized_vol
                vol_scalar = max(VOL_SCALAR_MIN, min(vol_scalar, VOL_SCALAR_MAX))
                
                # åº”ç”¨æ³¢åŠ¨ç‡ç¼©æ”¾
                scaled_weights = {}
                total_weight = sum(final_weights.values())
                if total_weight > 0:
                    for asset, w in final_weights.items():
                        scaled_weights[asset] = w * vol_scalar
                    # ç¡®ä¿æ€»æƒé‡ä¸è¶…è¿‡1ï¼ˆè¶…å‡ºéƒ¨åˆ†å˜ä¸ºç°é‡‘ï¼‰
                    total_scaled = sum(scaled_weights.values())
                    if total_scaled > 1.0:
                        for asset in scaled_weights:
                            scaled_weights[asset] /= total_scaled
                    final_weights = scaled_weights
        
        # === ä¼˜åŒ–3: åŠ¨æ€æ­¢æŸæœºåˆ¶ï¼ˆv1.5 åˆ†é˜¶æ®µæ¢å¤ï¼‰===
        # ç»„åˆå›æ’¤è¶…è¿‡é˜ˆå€¼æ—¶å‡ä»“ï¼Œæ¢å¤æ—¶åˆ†é˜¶æ®µæ¸è¿›
        current_drawdown = (current_val - peak_nav) / peak_nav if peak_nav > 0 else 0
        
        if not in_stop_loss_mode and current_drawdown < DRAWDOWN_STOP_LOSS:
            # è§¦å‘æ­¢æŸ
            in_stop_loss_mode = True
        elif in_stop_loss_mode:
            # v1.5: åˆ†é˜¶æ®µæ¢å¤æ£€æŸ¥
            # æ‰¾åˆ°å½“å‰å›æ’¤å¯¹åº”çš„æ¢å¤é˜¶æ®µ
            recovery_ratio = DRAWDOWN_REDUCE_RATIO  # é»˜è®¤ç»´æŒæ­¢æŸå‡ä»“
            for threshold, ratio in STOP_LOSS_RECOVERY_STAGES:
                if current_drawdown > threshold:
                    recovery_ratio = 1 - ratio  # è½¬æ¢ä¸ºå‡ä»“æ¯”ä¾‹
                    if ratio >= 1.0:
                        in_stop_loss_mode = False  # å®Œå…¨æ¢å¤
                    break
        
        if in_stop_loss_mode:
            # æ­¢æŸæ¨¡å¼ï¼šæ‰€æœ‰é£é™©èµ„äº§æŒ‰æ¢å¤é˜¶æ®µå‡ä»“
            stop_loss_weights = {}
            # è®¡ç®—å½“å‰æ¢å¤æ¯”ä¾‹
            current_recovery_ratio = 1 - DRAWDOWN_REDUCE_RATIO  # é»˜è®¤50%ä»“ä½
            for threshold, ratio in STOP_LOSS_RECOVERY_STAGES:
                if current_drawdown > threshold:
                    current_recovery_ratio = ratio
                    break
            
            for asset, w in final_weights.items():
                # WTMFå’ŒGSDè§†ä¸ºé¿é™©èµ„äº§ï¼Œä¸å‡ä»“
                if asset in ['WTMF', 'GSD.SI']:
                    stop_loss_weights[asset] = w
                else:
                    stop_loss_weights[asset] = w * current_recovery_ratio
            final_weights = stop_loss_weights
        
        # --- Calculate Turnover (Trading Volume) ---
        daily_turnover = 0.0
        
        if not prev_targets:
            daily_turnover = sum(final_weights.values())
        elif should_actually_rebalance:
            diff_sum = 0.0
            all_assets = set(final_weights.keys()) | set(drifted_weights.keys())
            
            for t in all_assets:
                w_tgt = final_weights.get(t, 0.0)
                w_drift = drifted_weights.get(t, 0.0)
                diff_sum += abs(w_tgt - w_drift)
            
            curr_cash_w = max(0.0, 1.0 - sum(final_weights.values()))
            prev_cash_w = max(0.0, 1.0 - sum(drifted_weights.values())) if drifted_weights else 0
            diff_sum += abs(curr_cash_w - prev_cash_w)
            
            daily_turnover = diff_sum / 2.0
        
        # === äº¤æ˜“æˆæœ¬æ‰£å‡ ===
        # transaction_cost_bps æ˜¯åŸºç‚¹ï¼Œ1bps = 0.01% = 0.0001
        # äº¤æ˜“æˆæœ¬ = æ¢æ‰‹ç‡ * æˆæœ¬ç‡
        trading_cost = daily_turnover * (transaction_cost_bps / 10000.0)
            
        # Record history (with enhanced info)
        rec = targets.copy()
        rec['Date'] = date
        rec['State'] = s
        rec['RawState'] = raw_state  # åŸå§‹æœªç¡®è®¤çŠ¶æ€
        rec['Turnover'] = daily_turnover
        rec['TradingCost'] = trading_cost  # æ–°å¢ï¼šè®°å½•äº¤æ˜“æˆæœ¬
        rec['Rebalanced'] = should_actually_rebalance
        rec['InStopLoss'] = in_stop_loss_mode
        rec['Drawdown'] = current_drawdown
        rec['InTransition'] = is_in_transition
        history_records.append(rec)
        
        # Calculate Portfolio Return for this day
        daily_ret = 0.0
        current_rets = pd.Series(dtype=float)
        
        if date in returns_df.index:
            current_rets = returns_df.loc[date]
            for t, w in final_weights.items():
                if t in current_rets:
                    daily_ret += w * current_rets[t]
        
        # === æ‰£é™¤äº¤æ˜“æˆæœ¬ ===
        daily_ret -= trading_cost
        
        # è®°å½•æ”¶ç›Šç”¨äºæ³¢åŠ¨ç‡è®¡ç®—
        portfolio_returns_history.append(daily_ret)
        
        current_val = current_val * (1 + daily_ret)
        portfolio_values.append(current_val)
        
        # æ›´æ–°å†å²æœ€é«˜å‡€å€¼
        if current_val > peak_nav:
            peak_nav = current_val
        
        # Prepare for next iteration
        prev_targets = final_weights
        prev_rets = current_rets
        prev_date = date

        
    s_strategy = pd.Series(portfolio_values, index=df_states.index, name="Strategy")
    
    # Create History DataFrame
    df_history = pd.DataFrame(history_records)
    if not df_history.empty:
        df_history = df_history.set_index('Date')
    
    # 4. Benchmarks
    # SPY
    s_spy = pd.Series(dtype=float)
    bench_ticker = 'SPY'
    if use_proxies and '^GSPC' in price_data.columns:
        bench_ticker = '^GSPC'
        
    if bench_ticker in price_data.columns:
        spy_prices = price_data[bench_ticker]
        s_spy = (spy_prices / spy_prices.iloc[0]) * initial_capital
        s_spy.name = f"{bench_ticker} (Benchmark)"

    # IWY
    s_iwy = pd.Series(dtype=float)
    growth_ticker = 'IWY'
    if use_proxies and '^NDX' in price_data.columns:
        growth_ticker = '^NDX'
    elif use_proxies and '^GSPC' in price_data.columns:
        growth_ticker = '^GSPC'
        
    if growth_ticker in price_data.columns:
        iwy_prices = price_data[growth_ticker]
        s_iwy = (iwy_prices / iwy_prices.iloc[0]) * initial_capital
        s_iwy.name = f"{growth_ticker} (Growth)"

    # 60/40
    s_6040 = pd.Series(dtype=float)
    bond_ticker = 'TLT'
    if use_proxies and 'VUSTX' in price_data.columns:
        bond_ticker = 'VUSTX'
    # If TLT missing in proxy mode, maybe we can't do 60/40 easily without a bond proxy
    
    if bench_ticker in price_data.columns and bond_ticker in price_data.columns:
        spy = price_data[bench_ticker] / price_data[bench_ticker].iloc[0]
        tlt = price_data[bond_ticker] / price_data[bond_ticker].iloc[0]
        s_6040 = (0.6 * spy + 0.4 * tlt) * initial_capital
        s_6040.name = "60/40 (Balanced)"
        
    # Neutral Config (Buy & Hold / Fixed Weight)
    # Note: Neutral config logic relies on original ETFs. 
    # In proxy mode, we need to map default targets too.
    default_targets = get_target_percentages("NEUTRAL", False, False)
    neutral_vals = []
    curr_n = initial_capital
    
    for date in df_states.index:
        daily_ret = 0.0
        if date in returns_df.index:
            rets = returns_df.loc[date]
            for t, w in default_targets.items():
                mapped_t = map_target_to_asset(t, date)
                if mapped_t in rets and mapped_t != 'CASH':
                    daily_ret += w * rets[mapped_t]
        curr_n = curr_n * (1 + daily_ret)
        neutral_vals.append(curr_n)
        
    s_neutral = pd.Series(neutral_vals, index=df_states.index, name="Neutral Config")
    
    return pd.DataFrame({
        "Dynamic Strategy": s_strategy,
        "SPY (Benchmark)": s_spy,
        "Growth (Benchmark)": s_iwy,
        "60/40 (Balanced)": s_6040,
        "Neutral (Fixed)": s_neutral
    }), df_history, None




# --- Page 3: State Machine Check ---

# --- Constants & Config ---
MACRO_STATES = {
    "INFLATION_SHOCK": {
        "display": "ğŸ”´ æ»èƒ€ / åŠ æ¯å†²å‡» (Inflation Shock)",
        "desc": "âš ï¸ **ä¸¥é‡è­¦æŠ¥**ï¼šåˆ©ç‡é£™å‡æˆ–å‡ºç°è‚¡å€ºåŒæ€ã€‚ä¼ ç»Ÿèµ„äº§å¤±æ•ˆã€‚**ç°é‡‘ä¸ºç‹**ï¼Œæ¸…ä»“é•¿ä¹…æœŸèµ„äº§ã€‚",
        "bg_color": "#fce8e6", "border_color": "#d93025", "icon": "ğŸ”´"
    },
    "DEFLATION_RECESSION": {
        "display": "ğŸ”µ è¡°é€€ / å´©ç›˜ (Deflation/Crash)",
        "desc": "âš ï¸ **ä¸¥é‡è­¦æŠ¥**ï¼šç»æµè¡°é€€ç¡®è®¤æˆ–æµåŠ¨æ€§å±æœºã€‚**å…¨é¢é˜²å¾¡**ï¼Œé”å®šå›½å€ºæ”¶ç›Šï¼Œé…ç½®é»„é‡‘é¿é™©ã€‚",
        "bg_color": "#e8f0fe", "border_color": "#1a73e8", "icon": "ğŸ”µ"
    },
    "EXTREME_ACCUMULATION": {
        "display": "ğŸš€ æåº¦è´ªå©ª / æŠ„åº• (Accumulation)",
        "desc": "ğŸ”” **æœºä¼šæç¤º**ï¼šå¸‚åœºæåº¦ææ…Œä½†åŸºæœ¬é¢æœªå´©åã€‚å»ºè®®**é‡ä»“æŠ„åº•**æˆé•¿è‚¡ï¼Œåˆ©ç”¨åˆ«äººçš„ææ…Œè·åˆ©ã€‚",
        "bg_color": "rgba(142, 36, 170, 0.2)", "border_color": "#8e24aa", "icon": "ğŸš€"
    },
    "CAUTIOUS_TREND": {
        "display": "âš ï¸ è°¨æ… / è¶‹åŠ¿ç ´ä½ (Bear Trend)",
        "desc": "ğŸ“‰ **é£é™©æç¤º**ï¼šé•¿æœŸè¶‹åŠ¿è½¬ç©ºä½†ææ…Œæœªèµ·ï¼ˆé˜´è·Œï¼‰ã€‚å»ºè®®è½¬ä¸º**é˜²å¾¡é…ç½®**ï¼Œé‡ä»“çº¢åˆ©ä¸ç°é‡‘ã€‚",
        "bg_color": "#fff3e0", "border_color": "#f57c00", "icon": "ğŸ“‰"
    },
    "CAUTIOUS_VOL": {
        "display": "âš¡ è°¨æ… / é«˜æ³¢éœ‡è¡ (High Volatility)",
        "desc": "ğŸŒŠ **é£é™©æç¤º**ï¼šè¶‹åŠ¿å°šå¯ä½†æ³¢åŠ¨åŠ å‰§ã€‚å»ºè®®ä¿ç•™æˆé•¿ä»“ä½ï¼Œä½†å¢åŠ **å±æœºAlpha (WTMF)** è¿›è¡Œå¯¹å†²ã€‚",
        "bg_color": "#fff8e1", "border_color": "#ffb74d", "icon": "âš¡"
    },
    "NEUTRAL": {
        "display": "ğŸŸ¢ å¸¸æ€ / ç‰›å¸‚ (Neutral)",
        "desc": "âœ… å¸‚åœºè¿è¡Œå¹³ç¨³ï¼Œæ³¢åŠ¨ç‡ä½ä¸”è¶‹åŠ¿å‘ä¸Šã€‚å»ºè®®ç»´æŒ**æ ‡å‡†å¢é•¿é…ç½®**ï¼Œäº«å—å¤åˆ©å¢é•¿ã€‚",
        "bg_color": "#e6f4ea", "border_color": "#1e8e3e", "icon": "ğŸŸ¢"
    }
}

# --- Utility Functions for Backtest ---
def safe_div(a, b, default=0.0):
    """Safe division to avoid ZeroDivisionError."""
    return a / b if b != 0 else default

def get_state_segments(df, state_col='State'):
    """
    Extract state segments from a DataFrame with state column.
    Returns DataFrame with columns: grp, State, Start, End, Duration
    """
    if df is None or df.empty or state_col not in df.columns:
        return pd.DataFrame()
    df_copy = df.copy()
    df_copy['state_grp'] = (df_copy[state_col] != df_copy[state_col].shift()).cumsum()
    segments = df_copy.groupby(['state_grp', state_col]).agg(
        Start=(state_col, lambda x: x.index[0]),
        End=(state_col, lambda x: x.index[-1])
    ).reset_index()
    segments.columns = ['grp', 'State', 'Start', 'End']
    segments['Duration'] = (segments['End'] - segments['Start']).dt.days + 1
    return segments

def validate_date_range(start_date, end_date, min_days=30):
    """
    Validate date range for backtest.
    Returns (is_valid, error_message)
    """
    if start_date is None or end_date is None:
        return False, "è¯·é€‰æ‹©æœ‰æ•ˆçš„æ—¥æœŸèŒƒå›´"
    if start_date >= end_date:
        return False, "ç»“æŸæ—¥æœŸå¿…é¡»æ™šäºå¼€å§‹æ—¥æœŸ"
    if (end_date - start_date).days < min_days:
        return False, f"å›æµ‹å‘¨æœŸè‡³å°‘éœ€è¦ {min_days} å¤©"
    return True, None

def normalize_weights(weights_dict):
    """
    Normalize weights to sum to 1.0, handling edge cases.
    """
    if not weights_dict:
        return {}
    total = sum(weights_dict.values())
    if total <= 0:
        return {k: 0.0 for k in weights_dict}
    return {k: v / total for k, v in weights_dict.items()}

def calculate_state_transition_matrix(df_states, state_col='State'):
    """
    Calculate state transition matrix from state history.
    Returns DataFrame with transition counts and probabilities.
    """
    if df_states is None or df_states.empty or state_col not in df_states.columns:
        return None, None
    states = df_states[state_col]
    transitions = pd.crosstab(states.shift(1), states, dropna=True)
    # Normalize to probabilities
    trans_prob = transitions.div(transitions.sum(axis=1), axis=0).fillna(0)
    return transitions, trans_prob

def calculate_state_statistics(df_history, state_col='State'):
    """
    Calculate statistics for each state: count, avg duration, total days.
    """
    segments = get_state_segments(df_history, state_col)
    if segments.empty:
        return pd.DataFrame()
    stats = segments.groupby('State').agg(
        Occurrences=('grp', 'count'),
        AvgDuration=('Duration', 'mean'),
        TotalDays=('Duration', 'sum'),
        MinDuration=('Duration', 'min'),
        MaxDuration=('Duration', 'max')
    ).round(1)
    return stats

def calculate_state_returns(df_history, returns_series, state_col='State'):
    """
    Calculate return statistics by state.
    """
    if df_history is None or df_history.empty or returns_series is None or returns_series.empty:
        return pd.DataFrame()
    # Align indices
    common_idx = df_history.index.intersection(returns_series.index)
    if len(common_idx) == 0:
        return pd.DataFrame()
    states = df_history.loc[common_idx, state_col]
    rets = returns_series.loc[common_idx]
    
    result = rets.groupby(states).agg(['mean', 'std', 'sum', 'count'])
    result.columns = ['AvgDailyRet', 'StdDev', 'CumulativeRet', 'Days']
    result['AvgDailyRet'] = result['AvgDailyRet'] * 100  # Convert to %
    result['StdDev'] = result['StdDev'] * 100
    result['CumulativeRet'] = result['CumulativeRet'] * 100
    result['AnnualizedRet'] = result['AvgDailyRet'] * 252
    return result.round(2)

def determine_macro_state(row, params=None):
    """
    Determines macro state based on a row of indicators.
    Expected row keys: Sahm, RateShock, Corr, VIX, Trend_Bear
    """
    if params is None:
        params = {
            'sahm_threshold': 0.50,
            'rate_shock_threshold': 0.20,
            'corr_threshold': 0.30,
            'vix_panic': 32,
            'vix_recession': 35,
            'vix_elevated': 20
        }
        
    is_rec = row['Sahm'] >= params['sahm_threshold']
    is_shock = row['RateShock'] > params['rate_shock_threshold']
    is_c_broken = row['Corr'] > params['corr_threshold']
    is_f = row['VIX'] > params['vix_panic']
    is_down = row['Trend_Bear']
    is_vol_elevated = row['VIX'] > params['vix_elevated']
    
    if is_shock or (is_rec and is_c_broken):
        return "INFLATION_SHOCK"
    elif is_rec or (is_down and row['VIX'] > params['vix_recession']):
        return "DEFLATION_RECESSION"
    elif is_f and not is_shock and not is_rec:
        return "EXTREME_ACCUMULATION"
    elif is_down:
        # Optimized: If Trend is Down, prioritize Trend signal (Defensive) over Volatility signal.
        return "CAUTIOUS_TREND"
    elif is_vol_elevated:
        return "CAUTIOUS_VOL"
    else:
        return "NEUTRAL"

@st.cache_data(ttl=900, show_spinner=False)
def fetch_yf_with_retry(tickers, start=None, end=None, auto_adjust=False, attempts: int = 2, backoff: int = 3, interval: str = "1d"):
    tickers_list = list(tickers) if isinstance(tickers, (list, tuple, set)) else [tickers]
    last_err = None
    for i in range(attempts):
        try:
            data_raw = yf.download(
                tickers_list,
                start=start,
                end=end,
                progress=False,
                auto_adjust=auto_adjust,
                timeout=12,
                interval=interval,
            )
            if data_raw is not None and not data_raw.empty:
                return data_raw
        except Exception as e:
            last_err = str(e)
        time.sleep(backoff * (i + 1))
    log_event("ERROR", "yfinance download failed", {"tickers": tickers_list, "err": last_err})
    return pd.DataFrame()


@st.cache_data(ttl=900, show_spinner=False)
def get_live_prices(tickers):
    tickers_list = [t for t in (list(tickers) if isinstance(tickers, (list, tuple, set)) else [tickers]) if t]
    if not tickers_list:
        return {}
    end = datetime.date.today() + datetime.timedelta(days=1)
    start = end - datetime.timedelta(days=7)
    df_raw = fetch_yf_with_retry(tickers_list, start=start, end=end, auto_adjust=False)
    if df_raw is None or df_raw.empty:
        return {}
    df = normalize_yf_prices(df_raw).ffill().tail(2)
    if df.empty:
        return {}
    latest = df.iloc[-1]
    prev = df.iloc[-2] if len(df) > 1 else df.iloc[-1]
    out = {}
    for t in tickers_list:
        try:
            p = latest.get(t)
            prev_p = prev.get(t)
            if pd.isna(p):
                continue
            change_pct = None
            if prev_p and not pd.isna(prev_p) and prev_p != 0:
                change_pct = (p - prev_p) / prev_p * 100
            out[t] = {"price": float(p), "change_pct": float(change_pct) if change_pct is not None else None}
        except Exception:
            continue
    return out


@st.cache_data
def get_historical_macro_data(start_date, end_date, ma_window=200, params=None, use_proxies=False):
    """
    Fetches and calculates macro states for a given date range.
    Includes buffer to ensure valid data at start_date.
    use_proxies: If True, prioritizes Indices (^GSPC, VUSTX) over ETFs for longer history.
    """
    if params is None:
        params = {
            'sahm_threshold': 0.50,
            'rate_shock_threshold': 0.20,
            'corr_threshold': 0.30,
            'vix_panic': 32,
            'vix_recession': 35,
            'vix_elevated': 20
        }

    buffer_days = 365 * 2 # Increase buffer for Sahm Rule (12m min)
    fetch_start = pd.to_datetime(start_date) - pd.Timedelta(days=buffer_days)
    fetch_end = pd.to_datetime(end_date)

    # 1. Fetch Market Data
    # Added ^GSPC (S&P 500) for longer history check if IWY is missing
    # Added VUSTX (Vanguard Long-Term Treasury) for longer bond history (since 1986)
    tickers = ['IWY', 'TLT', '^TNX', '^VIX', 'GLD', 'IWD', '^GSPC', 'VUSTX']
    df_all = fetch_yf_with_retry(tickers, start=fetch_start, end=fetch_end, auto_adjust=False)
    if df_all is None or df_all.empty:
        return pd.DataFrame(), "Market data fetch failed or incomplete."

    data = normalize_yf_prices(df_all)
    
    if data.empty:
         return pd.DataFrame(), "Market data fetch failed or incomplete."

    # 2. Fetch FRED Data (UNRATE & T10Y2Y)
    try:
        unrate = fetch_fred_data("UNRATE")
        yc = fetch_fred_data("T10Y2Y") 
        
        if unrate.empty:
            raise ValueError("Fetched empty data for UNRATE")
            
        unrate.columns = ['UNRATE']
        unrate = unrate[unrate.index >= fetch_start]
        # Reindex
        unrate_daily = unrate.reindex(data.index).ffill()
        
        if not yc.empty:
            yc.columns = ['T10Y2Y']
            yc = yc[yc.index >= fetch_start]
            yc_daily = yc.reindex(data.index).ffill()
        else:
            yc_daily = pd.DataFrame(0.0, index=data.index, columns=['T10Y2Y'])

    except Exception as e:
        return pd.DataFrame(), f"Error fetching FRED data: {str(e)}"

    # 3. Calculate Indicators
    try:
        # Sahm Rule
        u_monthly = unrate['UNRATE']
        u_3m_avg = u_monthly.rolling(window=3).mean()
        u_12m_low = u_3m_avg.rolling(window=12).min().shift(1)
        sahm_monthly = u_3m_avg - u_12m_low
        sahm_series = sahm_monthly.reindex(data.index).ffill()
        
        # Rate Shock
        tnx_col = '^TNX' if '^TNX' in data.columns else data.columns[0]
        tnx_roc = (data[tnx_col] - data[tnx_col].shift(21)) / data[tnx_col].shift(21)
        
        # Correlation & Series Selection
        # If use_proxies is True, we FORCE the use of Indices (^GSPC, VUSTX) to ensure we get data back to 1990s.
        # Otherwise, we prefer the actual ETFs (IWY, TLT).
        
        prefer_etfs = ('IWY' in data.columns) and ('TLT' in data.columns) and (not use_proxies)

        if prefer_etfs:
            corr = data['IWY'].rolling(60).corr(data['TLT'])
            iwy_series = data['IWY']
        elif '^GSPC' in data.columns:
            # Fallback or Proxy mode: Use S&P 500
            iwy_series = data['^GSPC']
            
            # For correlation, prefer VUSTX if using proxies or if TLT is missing/short
            bond_series = None
            if use_proxies and 'VUSTX' in data.columns:
                bond_series = data['VUSTX']
            elif 'TLT' in data.columns:
                # Check if TLT has enough history? 
                # For simplicity, if not forcing proxies, try TLT first, fallback to VUSTX
                tlt_series = data['TLT']
                if 'VUSTX' in data.columns:
                    bond_series = data['VUSTX']
                else:
                    bond_series = data['TLT']
            elif 'VUSTX' in data.columns:
                bond_series = data['VUSTX']
            
            if bond_series is not None:
                corr = data['^GSPC'].rolling(60).corr(bond_series)
            else:
                corr = pd.Series(0, index=data.index)
        else:
            corr = pd.Series(0, index=data.index)
            iwy_series = data.iloc[:, 0] if not data.empty else pd.Series(dtype=float)
        
        # Trend
        iwy_ma = iwy_series.rolling(ma_window).mean()
        trend_bear = iwy_series < iwy_ma
        
        # Gold Trend
        gold_trend_bear = pd.Series(False, index=data.index)
        if 'GLD' in data.columns:
            gld_ma = data['GLD'].rolling(ma_window).mean()
            gold_trend_bear = data['GLD'] < gld_ma
            
        # Style Trend
        style_value_regime = pd.Series(False, index=data.index)
        # Only use IWY/IWD if NOT using proxies (since IWD history is short)
        if not use_proxies and 'IWY' in data.columns and 'IWD' in data.columns:
            pair_ratio = data['IWY'] / data['IWD']
            pair_ma = pair_ratio.rolling(ma_window).mean()
            style_value_regime = pair_ratio < pair_ma 
        else:
            # Fallback for style if ETFs missing
            style_value_regime = pd.Series(False, index=data.index)

        # Assemble DataFrame
        df_hist = pd.DataFrame({
            'IWY': iwy_series,
            'Sahm': sahm_series,
            'RateShock': tnx_roc,
            'Corr': corr,
            'VIX': data.get('^VIX', pd.Series(0, index=data.index)),
            'Trend_Bear': trend_bear,
            'YieldCurve': yc_daily['T10Y2Y'],
            'Gold_Bear': gold_trend_bear,
            'Value_Regime': style_value_regime
        }).dropna()
        
        # 4. Determine States
        # Pass params to the state determinator
        df_hist['State'] = df_hist.apply(lambda row: determine_macro_state(row, params), axis=1)
        
        # Filter Output
        df_final = df_hist.loc[(df_hist.index >= pd.to_datetime(start_date)) & (df_hist.index <= pd.to_datetime(end_date))]
        return df_final, None

    except Exception as e:
        return pd.DataFrame(), f"Error in calculation: {str(e)}"

# --- UI Components ---

def render_manual_data_import():
    """Renders the manual data import expander."""
    with st.expander("ğŸ“‚ æ‰‹åŠ¨å¯¼å…¥å®è§‚æ•°æ® (ç½‘ç»œå—é™æ—¶ä½¿ç”¨)", expanded=False):
        st.info("å¦‚æœç½‘ç»œå—é™å¯¼è‡´ FRED æ•°æ® (UNRATE, T10Y2Y) è·å–å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨ä¸‹è½½å¹¶ä¸Šä¼  CSV æ–‡ä»¶ã€‚")
        col_u1, col_u2 = st.columns(2)

        # UNRATE Import
        with col_u1:
            st.markdown("**1. å¤±ä¸šç‡ (UNRATE)**")
            unrate_path = os.path.join(os.path.dirname(__file__), "fred_UNRATE.csv")
            if os.path.exists(unrate_path):
                file_time = datetime.datetime.fromtimestamp(os.path.getmtime(unrate_path)).strftime('%Y-%m-%d %H:%M')
                st.success(f"âœ… å·²æ£€æµ‹åˆ°æœ¬åœ°æ•°æ® ({file_time})")
            else:
                st.warning("âš ï¸ æœªæ£€æµ‹åˆ°æœ¬åœ°æ–‡ä»¶")

            st.markdown("[ğŸ“¥ ä¸‹è½½ UNRATE.csv](https://fred.stlouisfed.org/graph/fredgraph.csv?id=UNRATE)")
            uploaded_file = st.file_uploader("ä¸Šä¼  UNRATE.csv", type=['csv'], key="uploader_unrate")
            
            if uploaded_file is not None:
                file_id = f"{uploaded_file.name}-{uploaded_file.size}"
                if st.session_state.get("processed_unrate_id") != file_id:
                    try:
                        df_test = pd.read_csv(uploaded_file)
                        if 'observation_date' in df_test.columns:
                            uploaded_file.seek(0)
                            with open(unrate_path, "wb") as f:
                                f.write(uploaded_file.getbuffer())
                            st.session_state["processed_unrate_id"] = file_id
                            st.success("âœ… UNRATE å·²ä¿å­˜! åˆ·æ–°ä¸­...")
                            st.cache_data.clear()
                            time.sleep(1)
                            st.rerun()
                        else:
                            st.error("æ ¼å¼é”™è¯¯: ç¼ºå°‘ 'observation_date'")
                    except Exception as e:
                        st.error(f"é”™è¯¯: {e}")

        # T10Y2Y Import
        with col_u2:
            st.markdown("**2. æ”¶ç›Šç‡æ›²çº¿ (T10Y2Y)**")
            yc_path = os.path.join(os.path.dirname(__file__), "fred_T10Y2Y.csv")
            if os.path.exists(yc_path):
                file_time = datetime.datetime.fromtimestamp(os.path.getmtime(yc_path)).strftime('%Y-%m-%d %H:%M')
                st.success(f"âœ… å·²æ£€æµ‹åˆ°æœ¬åœ°æ•°æ® ({file_time})")
            else:
                st.warning("âš ï¸ æœªæ£€æµ‹åˆ°æœ¬åœ°æ–‡ä»¶")

            st.markdown("[ğŸ“¥ ä¸‹è½½ T10Y2Y.csv](https://fred.stlouisfed.org/graph/fredgraph.csv?id=T10Y2Y)")
            uploaded_yc = st.file_uploader("ä¸Šä¼  T10Y2Y.csv", type=['csv'], key="uploader_t10y2y")
            
            if uploaded_yc is not None:
                file_id = f"{uploaded_yc.name}-{uploaded_yc.size}"
                if st.session_state.get("processed_t10y2y_id") != file_id:
                    try:
                        df_test = pd.read_csv(uploaded_yc)
                        if 'observation_date' in df_test.columns:
                            uploaded_yc.seek(0)
                            with open(yc_path, "wb") as f:
                                f.write(uploaded_yc.getbuffer())
                            st.session_state["processed_t10y2y_id"] = file_id
                            st.cache_data.clear()
                            st.success("âœ… T10Y2Y å·²ä¿å­˜! åˆ·æ–°ä¸­...")
                            time.sleep(1)
                            st.rerun()
                        else:
                            st.error("æ ¼å¼é”™è¯¯: ç¼ºå°‘ 'observation_date'")
                    except Exception as e:
                        st.error(f"é”™è¯¯: {e}")

def render_reference_guide():
    """Renders the state reference guide."""
    with st.expander("ğŸ“– æ–°æ‰‹æŒ‡å—ï¼šå¸‚åœºçŠ¶æ€ä¸åº”å¯¹ç­–ç•¥ (Beginner's Guide)", expanded=False):
        st.info(
            """ğŸ’¡ **åˆ¤å®šæµç¨‹ï¼ˆè¶Šä¸Šé¢ä¼˜å…ˆçº§è¶Šé«˜ï¼‰**
1) æ‹‰å–æ•°æ®ï¼šè‚¡æŒ‡/é•¿å€º (^TNX, IWY/TLT)ã€VIXã€å¤±ä¸šç‡ UNRATEã€æ”¶ç›Šç‡æ›²çº¿ T10Y2Yã€‚
2) è®¡ç®—æŒ‡æ ‡ï¼šSahmâ‰¥0.50 åˆ¤è¡°é€€ï¼›21æ—¥åˆ©ç‡æ¶¨å¹… >20% åˆ¤åˆ©ç‡å†²å‡»ï¼›è‚¡å€ºç›¸å…³æ€§>0.30 åˆ¤ç›¸å…³æ€§å¤±æ•ˆï¼›VIX>32 åˆ¤ææ…Œï¼›ä»·æ ¼<MA200 åˆ¤è¶‹åŠ¿ç ´ä½ã€‚
3) çŠ¶æ€åˆ¤å®šä¼˜å…ˆçº§ï¼šInflation Shock â†’ Deflation/Recession â†’ Extreme Accumulation â†’ Cautious Trend â†’ Cautious Vol â†’ Neutralã€‚
4) è¾“å‡ºå¯¹åº”çš„èµ„äº§é…ç½®å»ºè®®ï¼ˆè§ä¸‹è¡¨ï¼‰ã€‚"""
        )
        
        guide_cards = [
            {
                "key": "INFLATION_SHOCK",
                "trigger": "åˆ©ç‡21æ—¥æ¶¨å¹…>20% æˆ– è‚¡å€ºç›¸å…³æ€§>0.30 ä¸”æ³¢åŠ¨ä¸Šå‡",
                "action": "ç°é‡‘ä¸ºç‹ï¼Œå‰Šå‡è‚¡ç¥¨/é•¿ä¹…æœŸå€ºï¼Œæå‡å±æœºAlpha (WTMF)ã€‚",
            },
            {
                "key": "DEFLATION_RECESSION",
                "trigger": "Sahmâ‰¥0.50 æˆ– è¶‹åŠ¿ç ´ä½ä¸”VIX>35ï¼ˆè¡°é€€/æµåŠ¨æ€§é£é™©ï¼‰",
                "action": "å…¨é¢é˜²å¾¡ï¼šé•¿å€º+é»„é‡‘ä¸ºä¸»ï¼Œè‚¡ç¥¨æƒé‡å¤§å¹…ä¸‹è°ƒã€‚",
            },
            {
                "key": "EXTREME_ACCUMULATION",
                "trigger": "VIX>32 ææ…Œä½†æœªè§¦å‘åˆ©ç‡/è¡°é€€æ¡ä»¶",
                "action": "å·¦ä¾§æŠ„åº•ï¼šåŠ å¤§æˆé•¿è‚¡æƒé‡ï¼Œä¿ç•™ä¸€å®šé˜²å¾¡ã€‚",
            },
            {
                "key": "CAUTIOUS_TREND",
                "trigger": "ä»·æ ¼è·Œç ´MA200ï¼ˆé˜´è·Œè¶‹åŠ¿ï¼‰ï¼Œä½†æœªè§¦å‘ææ…Œ/è¡°é€€",
                "action": "é˜²å¾¡é…ç½®ï¼šæé«˜çº¢åˆ©/ä»·å€¼ä¸ç°é‡‘ï¼Œé™ä½æˆé•¿æ•å£ã€‚",
            },
            {
                "key": "CAUTIOUS_VOL",
                "trigger": "è¶‹åŠ¿å°šå¯ä½†VIX>20ï¼ˆé«˜æ³¢éœ‡è¡ï¼‰",
                "action": "ä¿ç•™æ ¸å¿ƒæˆé•¿ï¼Œä½†ç”¨ WTMF/é˜²å¾¡èµ„äº§å¯¹å†²æ³¢åŠ¨ã€‚",
            },
            {
                "key": "NEUTRAL",
                "trigger": "æœªè§¦å‘ä»¥ä¸Šä»»ä¸€è­¦æŠ¥",
                "action": "æ ‡å‡†å¢é•¿é…ç½®ï¼Œè·Ÿéšè¶‹åŠ¿æŒæœ‰ã€‚",
            },
        ]
        
        # 3 columns per row
        cols = st.columns(3)
        for idx, card in enumerate(guide_cards):
            s = MACRO_STATES[card["key"]]
            with cols[idx % 3]:
                st.markdown(
                    f"""
                    <div style="padding: 12px; border-radius: 8px; background-color: {s['bg_color']}; border-left: 4px solid {s['border_color']}; margin-bottom: 12px; min-height: 190px;">
                        <div style="font-weight: 700; font-size: 15px; margin-bottom: 6px;">{s['icon']} {s['display']}</div>
                        <div style="font-size: 13px; color: #3c4043; line-height: 1.5; margin-bottom: 6px;">{s['desc']}</div>
                        <div style="font-size: 12px; color: #111827; line-height: 1.6;">
                            <b>è§¦å‘æ¡ä»¶ï¼š</b>{card['trigger']}<br/>
                            <b>åº”å¯¹ç­–ç•¥ï¼š</b>{card['action']}
                        </div>
                    </div>
                    """,
                    unsafe_allow_html=True,
                )
        
        st.markdown(
            """
            **å•ä¸ªèµ„äº§çš„åŠ¨æ€å¤„ç†è§„åˆ™ï¼ˆç»“åˆè¶‹åŠ¿/æ³¢åŠ¨è‡ªåŠ¨è°ƒæ•´ï¼‰ï¼š**
            - **IWYï¼ˆæˆé•¿ï¼‰**ï¼šæ ¸å¿ƒè¿›æ”»ä»“ã€‚Neutral/Accumulation é‡ä»“ï¼›Cautious/Inflation/Recession é˜¶æ¢¯å‰Šå‡ï¼›è‹¥ä»·æ ¼è·Œç ´ MA200ï¼Œåˆ™æ ‡è®°ä¸ºç†Šå¸‚å¹¶é™ä½æƒé‡ã€‚
            - **WTMFï¼ˆå±æœº Alphaï¼‰**ï¼šå¯¹å†²èµ„äº§ã€‚é«˜æ³¢éœ‡è¡ (Cautious Vol) å’Œ é€šèƒ€å†²å‡» æ—¶æ˜¾è‘—åŠ ä»“ï¼›Neutral ç»´æŒå°æƒé‡ï¼›è‹¥å›åˆ°å¹³ç¨³åˆ™é€æ­¥é™å›åŸºå‡†ã€‚
            - **LVHIï¼ˆçº¢åˆ©/ä»·å€¼ï¼‰**ï¼šé˜²å¾¡æƒç›Šã€‚Cautious Trend/Vol æå‡ï¼›Inflation Shock ä¸‹ä»ä¿ç•™å°æ¯”ä¾‹ï¼›Neutral é€‚ä¸­é…ç½®ã€‚
            - **G3B.SIï¼ˆæœ¬åœ°è“ç­¹ï¼‰**ï¼šä¸æˆé•¿åŒå‘ä½†æ›´é˜²å¾¡ã€‚è¶‹åŠ¿ç ´ä½æˆ–é«˜æ³¢æ—¶ä¸‹è°ƒï¼›é€šèƒ€/è¡°é€€åœºæ™¯å¤§å¹…å‰Šå‡ã€‚
            - **MBH.SI / TLTï¼ˆé•¿å€ºï¼‰**ï¼šè¡°é€€é˜²å¾¡ä¸»åŠ›ã€‚Deflation/Recession å¤§å¹…åŠ ä»“ï¼›Inflation Shock é™è‡³æä½ï¼›Normal/Vol ç»´æŒä¸­ç­‰ã€‚
            - **GSD.SIï¼ˆé»„é‡‘ï¼‰**ï¼šç³»ç»Ÿæ€§é£é™©ä¸é€šèƒ€å¯¹å†²ã€‚Inflation Shock/Deflation æå‡ï¼›å¹³ç¨³æœŸé™ä½è‡³é˜²å¾¡åº•ä»“ã€‚
            - **SRT.SI / AJBU.SIï¼ˆREITs/æ•°æ®ä¸­å¿ƒï¼‰**ï¼šåœ¨ Cautious/Inflation/Recession é˜¶æ®µå‡å°‘ï¼ŒNeutral ç»´æŒå°æƒé‡ï¼ŒAccumulation ä¸é¢å¤–åŠ ç ã€‚
            - **OTHERS**ï¼šé»˜è®¤ä½é…æˆ–æ¸…ç†ï¼›ä»…åœ¨ Neutral/Accumulation ä¸”åŸºæœ¬é¢è‰¯å¥½æ—¶é…Œæƒ…æŒæœ‰ã€‚
            """
        )


def render_portfolio_import():
    """Renders the import from saved portfolios section."""
    saved_portfolios = load_portfolios()
    if saved_portfolios:
        with st.expander("ğŸ“¥ ä»å·²ä¿å­˜çš„æŠ•èµ„ç»„åˆå¯¼å…¥ (Import)", expanded=False):
            c1, c2, c3 = st.columns([2, 1, 1])
            with c1:
                sel_name = st.selectbox("é€‰æ‹©ç»„åˆ", list(saved_portfolios.keys()), key="sm_imp_name")
            with c2:
                imp_cap = st.number_input("æ€»æœ¬é‡‘", value=10000.0, step=1000.0, key="sm_imp_cap")
            with c3:
                if st.button("åº”ç”¨åˆ°æŒä»“", type="secondary"):
                    if sel_name in saved_portfolios:
                        p = saved_portfolios[sel_name]
                        weights = p.get("weights", {})
                        
                        # Reset known
                        known = ['IWY', 'WTMF', 'LVHI', 'G3B.SI', 'MBH.SI', 'GSD.SI', 'SRT.SI', 'AJBU.SI']
                        for t in known: st.session_state[f"hold_{t}"] = 0.0
                        st.session_state["hold_OTHERS"] = 0.0
                        
                        # Populate
                        other_val = 0.0
                        for t, w in weights.items():
                            val = imp_cap * (w / 100.0)
                            if t in known:
                                st.session_state[f"hold_{t}"] = val
                            else:
                                other_val += val
                        st.session_state["hold_OTHERS"] = other_val
                        st.toast(f"å·²å¯¼å…¥: {sel_name}", icon="âœ…")
                        st.rerun()

def render_holdings_input():
    """Renders the holdings input section and returns the total value."""
    with st.expander("ğŸ’¼ è¾“å…¥å½“å‰æŒä»“ (Current Portfolio)", expanded=True):
        st.markdown("è¯·è¾“å…¥å½“å‰è´¦æˆ·å„æ ‡çš„çš„**å¸‚å€¼ (Value)**ã€‚")
        cols = st.columns(2)
        
        inputs = [
            ("IWY (ç¾è‚¡æˆé•¿)", "hold_IWY"), ("WTMF (å±æœºAlpha)", "hold_WTMF"),
            ("LVHI (ç¾è‚¡çº¢åˆ©)", "hold_LVHI"), ("G3B.SI (æ–°åŠ å¡è“ç­¹)", "hold_G3B.SI"),
            ("MBH.SI (æ–°å…ƒå€ºåˆ¸)", "hold_MBH.SI"), ("GSD.SI (é»„é‡‘)", "hold_GSD.SI"),
            ("SRT.SI (è¶…å¸‚REITs)", "hold_SRT.SI"), ("AJBU.SI (æ•°æ®ä¸­å¿ƒ)", "hold_AJBU.SI"),
            ("å…¶ä»–èµ„äº§ (Others)", "hold_OTHERS")
        ]
        
        # Init state
        for _, key in inputs:
            if key not in st.session_state: st.session_state[key] = 0.0
            
        for i, (label, key) in enumerate(inputs):
            with cols[i % 2]:
                st.number_input(label, step=100.0, key=key)
        
        current_holdings = {k.replace("hold_", ""): st.session_state[k] for _, k in inputs}
        total_value = sum(current_holdings.values())
        st.caption(f"ğŸ’° å½“å‰è´¦æˆ·æ€»å¸‚å€¼: **{total_value:,.2f}**")
        return current_holdings, total_value

def render_status_card(state):
    """Renders the main status card."""
    s_conf = MACRO_STATES.get(state, MACRO_STATES["NEUTRAL"])
    
    st.markdown(f"""
    <div style="padding: 20px; border-radius: 10px; background-color: {s_conf['bg_color']}; border-left: 6px solid {s_conf['border_color']}; margin-bottom: 20px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">
        <h2 style="margin:0; color: #202124; font-size: 28px;">{s_conf['icon']} {s_conf['display']}</h2>
        <p style="margin-top:10px; font-size: 16px; color: #3c4043; font-weight: 500;">{s_conf['desc']}</p>
    </div>
    """, unsafe_allow_html=True)

def render_factor_dashboard(metrics):
    """Renders the metrics dashboard with mini trendlines."""
    st.markdown("### ğŸ“Š æ ¸å¿ƒå®è§‚å› å­ (Macro Factors)")
    hist = metrics.get('factor_trends')
    if hist is None or (isinstance(hist, pd.DataFrame) and hist.empty):
        hist = pd.DataFrame()

    def get_series(col):
        if isinstance(hist, pd.DataFrame) and col in hist.columns:
            return hist[col].dropna()
        return pd.Series(dtype=float)

    def sparkline_fig(series, color="#2962FF"):
        fig = go.Figure()
        fig.add_trace(go.Scatter(x=series.index, y=series, mode="lines", line=dict(color=color, width=2), hovertemplate="%{y:.2f}<extra></extra>"))
        fig.update_layout(
            height=140,
            margin=dict(l=10, r=10, t=10, b=10),
            template="plotly_white",
            showlegend=False,
            xaxis=dict(visible=False),
            yaxis=dict(title=None, zeroline=False, showgrid=True, tickfont=dict(size=10)),
        )
        return fig

    factor_items = [
        {
            "title": "åˆ©ç‡å†²å‡» (TNX ROC)",
            "value": f"{metrics['tnx_roc']:+.1%}",
            "status": "âš ï¸ è§¦å‘" if metrics['rate_shock'] else "âœ… å®‰å…¨",
            "color": "#7c3aed",
            "series": get_series("RateShock"),
            "delta": metrics['tnx_roc'],
        },
        {
            "title": "è¡°é€€ä¿¡å· (Sahm)",
            "value": f"{metrics['sahm']:.2f}",
            "status": "âš ï¸ è§¦å‘" if metrics['recession'] else "âœ… å®‰å…¨",
            "color": "#0ea5e9",
            "series": get_series("Sahm"),
            "delta": metrics['sahm'],
        },
        {
            "title": "è‚¡å€ºç›¸å…³æ€§ (Corr)",
            "value": f"{metrics['corr']:.2f}",
            "status": "âš ï¸ å¤±æ•ˆ" if metrics['corr_broken'] else "âœ… æ­£å¸¸",
            "color": "#fb923c",
            "series": get_series("Corr"),
            "delta": metrics['corr'],
        },
        {
            "title": "ææ…ŒæŒ‡æ•° (VIX)",
            "value": f"{metrics['vix']:.1f}",
            "status": "âš ï¸ ææ…Œ" if metrics['fear'] else "âœ… æ­£å¸¸",
            "color": "#ef4444",
            "series": get_series("VIX"),
            "delta": metrics['vix'],
        },
    ]

    for i in range(0, len(factor_items), 2):
        cols = st.columns(2)
        for j in range(2):
            if i + j >= len(factor_items):
                continue
            item = factor_items[i + j]
            with cols[j]:
                st.metric(item["title"], item["value"], item["status"], delta_color="inverse" if "âš ï¸" in item["status"] else "normal")
                if not item["series"].empty:
                    st.plotly_chart(sparkline_fig(item["series"], item["color"]), use_container_width=True)

    st.markdown("#### ğŸ¯ æˆ˜æœ¯å¾®è°ƒ (Tactical Modifiers)")
    c1, c2, c3 = st.columns(3)
    with c1:
        yc = metrics['yield_curve']
        status = "âš ï¸ å€’æŒ‚/è§£å€’æŒ‚" if (yc < 0 or metrics['yc_un_invert']) else "âœ… æ­£å¸¸"
        st.metric("æ”¶ç›Šç‡æ›²çº¿ (10Y-2Y)", f"{yc:.2f}%", status, delta_color="off" if yc > 0 else "inverse")
    with c2:
        gb = metrics['gold_bear']
        st.metric("é»„é‡‘è¶‹åŠ¿", "Bearish (Weak)" if gb else "Bullish (Strong)", "Avoid Gold" if gb else "Hold", delta_color="inverse" if gb else "normal")
    with c3:
        vr = metrics['value_regime']
        st.metric("é£æ ¼è½®åŠ¨", "Value Regime" if vr else "Growth Regime", "Tilt Value" if vr else "Tilt Growth", delta_color="off")

    # Show Active Adjustments
    adjustments = get_adjustment_reasons(
        metrics['state'], 
        gold_bear=metrics['gold_bear'], 
        value_regime=metrics['value_regime'], 
        asset_trends=metrics.get('asset_trends', {}),
        vix=metrics.get('vix'),
        yield_curve=metrics.get('yield_curve'),
        sahm=metrics.get('sahm'),
        corr=metrics.get('corr'),
        yc_recently_inverted=metrics.get('yc_un_invert', False)
    )
    
    if adjustments:
        with st.expander("ğŸ”§ åŠ¨æ€é£æ§è§¦å‘ (Active Strategy Adjustments)", expanded=True):
            for adj in adjustments:
                st.markdown(f"- {adj}")


def render_data_health_badges(metrics):
    freshness_days = metrics.get('freshness_days')
    latest_date = metrics.get('date')
    warnings = metrics.get('data_warnings', []) or []
    fetch_ts = metrics.get('fetch_ts')
    badge = "ğŸŸ¢ æ•°æ®æœ€æ–°"
    note = f"æ•°æ®æˆªè‡³ {latest_date}" if latest_date else "æ•°æ®æ—¶é—´æœªçŸ¥"
    if freshness_days is not None:
        note += f" ï½œ æ»å {freshness_days} å¤©" if freshness_days > 0 else " ï½œ å½“æ—¥æ•°æ®"
    if fetch_ts:
        note += f" ï½œ ä¸Šæ¬¡æ‹‰å–: {fetch_ts}"
    if freshness_days is not None and freshness_days > 5:
        badge = "ğŸ”´ æ•°æ®å·²è¿‡æœŸ"
    elif freshness_days is not None and freshness_days > 2:
        badge = "ğŸŸ¡ æ•°æ®å¾…æ›´æ–°"

    st.markdown(
        f"""
        <div style="padding:12px;border-radius:8px;border:1px solid #e5e7eb;background:#f8fafc;margin-bottom:12px;">
            <div style="font-weight:700;color:#0f172a;">{badge}</div>
            <div style="color:#475467;font-size:13px;">{note}</div>
        </div>
        """,
        unsafe_allow_html=True,
    )

    if warnings:
        with st.expander("âš ï¸ æ•°æ®å¥åº·æé†’", expanded=False):
            for w in warnings:
                st.markdown(f"- {w}")

def render_rebalancing_table(state, current_holdings, total_value, is_gold_bear, is_value_regime, asset_trends=None, vix=None, yield_curve=None, price_info=None, sahm=None, corr=None, yc_recently_inverted=False):
    """Renders the rebalancing table with live prices."""
    if asset_trends is None: asset_trends = {}
    targets = get_target_percentages(state, gold_bear=is_gold_bear, value_regime=is_value_regime, asset_trends=asset_trends, vix=vix, yield_curve=yield_curve, sahm=sahm, corr=corr, yc_recently_inverted=yc_recently_inverted)
    
    # Add Current Holdings not in targets
    all_tickers = set(targets.keys()).union(current_holdings.keys())
    if price_info is None:
        price_info = get_live_prices(all_tickers)
    
    data = []
    if total_value == 0:
        st.warning("âš ï¸ è¯·è¾“å…¥æŒä»“å¸‚å€¼ä»¥è·å–å»ºè®®ã€‚")
        return

    for tkr in all_tickers:
        tgt_pct = targets.get(tkr, 0.0)
        curr_val = current_holdings.get(tkr, 0.0)
        curr_pct = curr_val / total_value if total_value > 0 else 0
        
        diff_val = (tgt_pct - curr_pct) * total_value
        price = price_info.get(tkr, {}).get("price") if price_info else None
        chg = price_info.get(tkr, {}).get("change_pct") if price_info else None
        price_text = f"${price:,.2f}" if price is not None else "-"
        chg_text = f"{chg:+.2f}%" if chg is not None else "-"
        
        # Action Text
        action = "âœ… æŒæœ‰"
        if tkr not in targets:
            if curr_val > 1: action = f"ğŸ”´ æ¸…ä»“ (-{curr_val:,.0f})"
        else:
            if abs(diff_val) > total_value * 0.01: # 1% threshold
                if diff_val > 0: action = f"ğŸŸ¢ ä¹°å…¥ (+{diff_val:,.0f})"
                else: action = f"ğŸ”´ å–å‡º ({diff_val:,.0f})"
        
        data.append({
            "ä»£ç ": tkr,
            "åç§°": ASSET_NAMES.get(tkr, tkr),
            "ç›®æ ‡ä»“ä½": tgt_pct * 100,
            "å½“å‰ä»“ä½": curr_pct * 100,
            "æœ€æ–°ä»·": price_text,
            "æ—¥å˜åŠ¨": chg_text,
            "å½“å‰å¸‚å€¼": curr_val,
            "å»ºè®®æ“ä½œ": action,
            "diff": diff_val # For sort
        })
    
    df = pd.DataFrame(data)
    if not df.empty:
        # Sort: Sells first, then Buys
        df['sort_key'] = df['diff'].apply(lambda x: 0 if x < 0 else (1 if x > 0 else 2))
        df = df.sort_values('sort_key')
        
        st.dataframe(
            df,
            column_config={
                "ç›®æ ‡ä»“ä½": st.column_config.NumberColumn(format="%.1f%%"),
                "å½“å‰ä»“ä½": st.column_config.NumberColumn(format="%.1f%%"),
                "å½“å‰å¸‚å€¼": st.column_config.NumberColumn(format="$%.0f"),
                "æ—¥å˜åŠ¨": st.column_config.TextColumn(help="ç›¸å¯¹å‰ä¸€äº¤æ˜“æ—¥çš„æ¶¨è·Œå¹…"),
            },
            hide_index=True,
            use_container_width=True
        )


def render_export_options(metrics, adjustments, targets):
    state = metrics.get('state')
    report_date = metrics.get('date')
    lines = [
        f"è¯Šæ–­æ—¶é—´: {metrics.get('fetch_ts', '')}",
        f"æ•°æ®æˆªè‡³: {report_date}",
        f"å½“å‰çŠ¶æ€: {state} ({MACRO_STATES.get(state, {}).get('display', '')})",
        "",
        "å…³é”®å› å­:",
        f"- åˆ©ç‡å†²å‡»: {metrics.get('tnx_roc', 0):+.1%}",
        f"- Sahm: {metrics.get('sahm', 0):.2f}",
        f"- è‚¡å€ºç›¸å…³æ€§: {metrics.get('corr', 0):.2f}",
        f"- VIX: {metrics.get('vix', 0):.1f}",
        f"- æ”¶ç›Šç‡æ›²çº¿: {metrics.get('yield_curve', 0):.2f}%",
        "",
        "åŠ¨æ€é£æ§è§¦å‘:",
    ]
    lines.extend([f"- {a}" for a in adjustments] or ["- æ— "])
    lines.append("")
    lines.append("ç›®æ ‡é…ç½®:")
    for k, v in targets.items():
        if v > 0:
            lines.append(f"- {ASSET_NAMES.get(k, k)} ({k}): {v*100:.1f}%")
    summary = "\n".join(lines)

    st.markdown("#### ğŸ“¤ å¯¼å‡ºè¯Šæ–­ç»“æœ")
    st.text_area("è¯Šæ–­æ‘˜è¦ (å¯å¤åˆ¶)", summary, height=160)
    st.download_button(
        label="ä¸‹è½½è¯Šæ–­æ‘˜è¦ (.txt)",
        data=summary.encode('utf-8'),
        file_name=f"diagnosis_{report_date or 'latest'}.txt",
        mime="text/plain",
        use_container_width=True,
    )


def render_historical_backtest_section():
    """Renders the independent historical backtest section."""
    st.markdown("---")
    st.markdown("### ğŸ•°ï¸ å†å²çŠ¶æ€å›æº¯ä¸ç­–ç•¥ä»¿çœŸ")
    
    # --- Initialize all session state at the top ---
    session_defaults = {
        "bt_use_proxies": False,
        "bt_ma_window": 200,
        "bt_p_sahm": 0.50,
        "bt_p_vix_panic": 32,
        "bt_p_vix_rec": 35,
        "bt_rebal_freq": "Daily",
        "bt_cost_bps": 10,
    }
    for key, val in session_defaults.items():
        if key not in st.session_state:
            st.session_state[key] = val
    
    # --- Advanced Settings (Sensitivity & Proxies) ---
    with st.expander("âš™ï¸ é«˜çº§å›æµ‹è®¾ç½® (å‚æ•°æ•æ„Ÿæ€§ä¸æ ·æœ¬å¤–æµ‹è¯•)", expanded=False):
        # Reset Button
        if st.button("ğŸ”„ æ¢å¤é»˜è®¤è®¾ç½®"):
            for key, val in session_defaults.items():
                st.session_state[key] = val
            st.rerun()

        c_adv1, c_adv2, c_adv3 = st.columns(3)
        with c_adv1:
            st.markdown("**1. æ ·æœ¬å¤–æµ‹è¯• (Out-of-Sample)**")
            use_proxies = st.checkbox("å¯ç”¨ä»£ç†èµ„äº§ (Use Proxies)", help="ä½¿ç”¨ S&P500, VUSTX(1986+), GC=F ç­‰æ›¿ä»£ ETF ä»¥æ”¯æŒæ›´é•¿å†å²å›æµ‹ (1990+)ã€‚", key="bt_use_proxies")
            ma_window = st.number_input("åŠ¨é‡çª—å£ (MA Window)", step=10, help="é»˜è®¤ 200 æ—¥å‡çº¿ã€‚å°è¯• 150 æˆ– 250 æµ‹è¯•æ•æ„Ÿæ€§ã€‚", key="bt_ma_window")
            
        with c_adv2:
            st.markdown("**2. é˜ˆå€¼æ•æ„Ÿæ€§ (Sensitivity)**")
            p_sahm = st.number_input("Sahm Rule", step=0.01, format="%.2f", key="bt_p_sahm")
            p_vix_panic = st.number_input("VIX Panic", step=1, key="bt_p_vix_panic")
            p_vix_rec = st.number_input("VIX Recession", step=1, key="bt_p_vix_rec")
        
        with c_adv3:
            st.markdown("**3. äº¤æ˜“å‚æ•° (Trading)**")
            rebal_freq = st.selectbox("å†å¹³è¡¡é¢‘ç‡", ["Daily", "Weekly", "Monthly", "Quarterly"], key="bt_rebal_freq", help="Daily=æ¯æ—¥, Weekly=æ¯å‘¨ä¸€, Monthly=æ¯æœˆåˆ, Quarterly=æ¯å­£åº¦åˆ")
            cost_bps = st.number_input("äº¤æ˜“æˆæœ¬ (bps)", min_value=0, max_value=100, step=5, key="bt_cost_bps", help="å•è¾¹äº¤æ˜“æˆæœ¬ï¼Œé»˜è®¤ 10bps = 0.1%")
    
    # Construct params dict
    custom_params = {
        'sahm_threshold': p_sahm,
        'rate_shock_threshold': 0.20,
        'corr_threshold': 0.30,
        'vix_panic': int(p_vix_panic),
        'vix_recession': int(p_vix_rec),
        'vix_elevated': 20
    }

    c1, c2, c3 = st.columns([2, 1, 1])
    with c1:
        # Default start date logic
        def_start = datetime.date(1990, 1, 1) if use_proxies else datetime.date.today()-datetime.timedelta(days=365*5)
        if def_start > datetime.date.today(): def_start = datetime.date.today() - datetime.timedelta(days=365)
        
        dates = st.date_input("å›æµ‹æ—¶é—´", [def_start, datetime.date.today()])
    with c2:
        cap = st.number_input("åˆå§‹èµ„é‡‘", value=10000)
    with c3:
        st.write(""); st.write("")
        run = st.button("ğŸš€ è¿è¡Œå›æµ‹", type="primary")
    
    # --- Date Validation ---
    if run:
        if not isinstance(dates, (tuple, list)) or len(dates) != 2:
            st.error("è¯·é€‰æ‹©æœ‰æ•ˆçš„æ—¥æœŸèŒƒå›´ï¼ˆå¼€å§‹æ—¥æœŸå’Œç»“æŸæ—¥æœŸï¼‰")
            return
        is_valid, err_msg = validate_date_range(dates[0], dates[1], min_days=30)
        if not is_valid:
            st.error(err_msg)
            return
        
    if run and isinstance(dates, (tuple, list)) and len(dates)==2:
        with st.spinner("å›æµ‹ä¸­..."):
            df_states, err = get_historical_macro_data(dates[0], dates[1], ma_window=int(ma_window), params=custom_params, use_proxies=use_proxies)
            if not df_states.empty:
                res, df_history, err = run_dynamic_backtest(df_states, dates[0], dates[1], cap, ma_window=int(ma_window), use_proxies=use_proxies, rebal_freq=rebal_freq, transaction_cost_bps=cost_bps)
                if res is not None:
                    # Metrics & Charts (Simplified for brevity as logic exists in run_dynamic_backtest return)
                    st.success("å›æµ‹å®Œæˆ")
                    
                    # 1. Curve
                    fig = go.Figure()
                    for c in res.columns:
                        fig.add_trace(go.Scatter(x=res.index, y=res[c], name=c))
                    
                    # Add Background Colors for States
                    shapes_curve = []
                    annotations_curve = []
                    
                    if df_history is not None and not df_history.empty:
                        # Create a copy to avoid affecting downstream logic
                        df_viz = df_history.copy()
                        df_viz['state_grp'] = (df_viz['State'] != df_viz['State'].shift()).cumsum()
                        
                        # Group by state segments
                        state_segments = df_viz.groupby(['state_grp', 'State'])['State'].agg(
                            ['first', lambda x: x.index[0], lambda x: x.index[-1]]
                        ).reset_index()
                        state_segments.columns = ['grp', 'State', 'State_Name', 'Start', 'End']
                        
                        for _, seg in state_segments.iterrows():
                            s_conf = MACRO_STATES.get(seg['State'], MACRO_STATES["NEUTRAL"])
                            color = s_conf['bg_color']
                            
                            # Add shape
                            shapes_curve.append(dict(
                                type="rect",
                                xref="x", yref="paper",
                                x0=seg['Start'], x1=seg['End'],
                                y0=0, y1=1,
                                fillcolor=color,
                                opacity=0.3,
                                layer="below",
                                line_width=0,
                            ))
                            
                            # Add icon label if segment is long enough
                            if (seg['End'] - seg['Start']).days > 15:
                                annotations_curve.append(dict(
                                    x=seg['Start'] + (seg['End'] - seg['Start'])/2,
                                    y=1.05,
                                    xref="x", yref="paper",
                                    text=s_conf['icon'],
                                    showarrow=False,
                                    font=dict(size=14)
                                ))

                    fig.update_layout(
                        title="å‡€å€¼æ›²çº¿ (Net Value Curve)", 
                        template="plotly_white",
                        shapes=shapes_curve,
                        annotations=annotations_curve,
                        hovermode="x unified"
                    )
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # --- NEW: State & Allocation Visualization ---
                    st.markdown("### ğŸ—ï¸ ä»“ä½å†å²ä¸çŠ¶æ€åˆ†å¸ƒ (Allocation & Regimes)")
                    
                    if df_history is not None and not df_history.empty:
                        # Stacked Area Chart
                        fig_alloc = go.Figure()
                        
                        # Identify asset columns (float types)
                        asset_cols = df_history.select_dtypes(include=[np.number]).columns
                        
                        for asset in asset_cols:
                            fig_alloc.add_trace(go.Scatter(
                                x=df_history.index, 
                                y=df_history[asset],
                                mode='lines',
                                name=ASSET_NAMES.get(asset, asset),
                                stackgroup='one',
                                groupnorm='percent', # Normalize to 0-100
                                hoverinfo='x+y+name'
                            ))
                        
                        # Add Background Colors for States
                        # 1. Simplify states to segments
                        df_history['state_grp'] = (df_history['State'] != df_history['State'].shift()).cumsum()
                        state_segments = df_history.groupby(['state_grp', 'State'])['State'].agg(['first', lambda x: x.index[0], lambda x: x.index[-1]]).reset_index()
                        state_segments.columns = ['grp', 'State', 'State_Name', 'Start', 'End']
                        
                        shapes = []
                        annotations = []
                        
                        for _, seg in state_segments.iterrows():
                            s_conf = MACRO_STATES.get(seg['State'], MACRO_STATES["NEUTRAL"])
                            color = s_conf['bg_color']
                            
                            # Add shape
                            shapes.append(dict(
                                type="rect",
                                xref="x", yref="paper",
                                x0=seg['Start'], x1=seg['End'],
                                y0=0, y1=1,
                                fillcolor=color,
                                opacity=0.3,
                                layer="below",
                                line_width=0,
                            ))
                            
                            # Add label if segment is long enough (e.g. > 10 days)
                            if (seg['End'] - seg['Start']).days > 15:
                                annotations.append(dict(
                                    x=seg['Start'] + (seg['End'] - seg['Start'])/2,
                                    y=1.05,
                                    xref="x", yref="paper",
                                    text=s_conf['icon'],
                                    showarrow=False,
                                    font=dict(size=14)
                                ))

                        fig_alloc.update_layout(
                            title="å†å²æŒä»“åˆ†å¸ƒä¸å¸‚åœºçŠ¶æ€ (Portfolio Allocation & Market Regimes)",
                            template="plotly_white",
                            yaxis=dict(title="Allocation %", range=[0, 100]),
                            shapes=shapes,
                            annotations=annotations,
                            hovermode="x unified"
                        )
                        st.plotly_chart(fig_alloc, use_container_width=True)
                        
                        # Legend for states (Optional text below)
                        st.caption("èƒŒæ™¯é¢œè‰²ä»£è¡¨å¸‚åœºçŠ¶æ€: " + " | ".join([f"{v['icon']} {k}" for k, v in MACRO_STATES.items()]))

                    # 2. Drawdown
                    fig_dd = go.Figure()
                    for c in res.columns:
                        dd = (res[c]/res[c].cummax()-1)*100
                        fig_dd.add_trace(go.Scatter(x=dd.index, y=dd, name=c, fill='tozeroy' if 'Dynamic' in c else None))
                    fig_dd.update_layout(title="æœ€å¤§å›æ’¤", template="plotly_white")
                    st.plotly_chart(fig_dd, use_container_width=True)
                    
                    # 3. Metrics Table
                    metrics_list = []
                    for col in res.columns:
                        m = calculate_equity_curve_metrics(res[col])
                        row = {"Strategy": col}
                        row.update(m)
                        metrics_list.append(row)
                    
                    st.markdown("#### ğŸ“Š è¯¦ç»†æ€§èƒ½æŒ‡æ ‡ (Performance Metrics)")
                    df_metrics = pd.DataFrame(metrics_list)
                    
                    # Basic Configs
                    col_config = {
                        "Strategy": st.column_config.TextColumn("ç­–ç•¥åç§°", width="medium"),
                        "Total Return (%)": st.column_config.NumberColumn("æ€»æ”¶ç›Šç‡", format="%.2f%%"),
                        "CAGR (%)": st.column_config.NumberColumn("å¹´åŒ–æ”¶ç›Š (CAGR)", format="%.2f%%"),
                        "Max Drawdown (%)": st.column_config.NumberColumn("æœ€å¤§å›æ’¤", format="%.2f%%"),
                        "Max DD Days": st.column_config.NumberColumn("å›æ’¤ä¿®å¤ (å¤©)", format="%d"),
                        "Volatility (%)": st.column_config.NumberColumn("æ³¢åŠ¨ç‡", format="%.2f%%"),
                        "Sharpe Ratio": st.column_config.NumberColumn("å¤æ™®æ¯”ç‡", format="%.2f"),
                        "Sortino Ratio": st.column_config.NumberColumn("ç´¢æè¯º", format="%.2f"),
                        "Calmar Ratio": st.column_config.NumberColumn("å¡ç›", format="%.2f"),
                        "Win Rate (Daily %)": st.column_config.NumberColumn("èƒœç‡", format="%.1f%%"),
                        "Profit/Loss Ratio": st.column_config.NumberColumn("ç›ˆäºæ¯”", format="%.2f"),
                    }
                    
                    # Add dynamic configs for Years
                    for c in df_metrics.columns:
                        if " (%)" in c and c not in col_config:
                            col_config[c] = st.column_config.NumberColumn(c, format="%.2f%%")
                            
                    st.dataframe(
                        df_metrics, 
                        use_container_width=True,
                        column_config=col_config,
                        hide_index=True
                    )
                    
                    # --- 4. Trading Costs & Frequency Analysis ---
                    if df_history is not None and 'Turnover' in df_history.columns:
                        st.markdown("---")
                        st.markdown("#### ğŸ’¸ äº¤æ˜“æˆæœ¬ä¸é¢‘ç‡ (Trading Costs & Frequency)")
                        
                        # Calculate Stats
                        total_days = len(df_history)
                        years = total_days / 252.0 if total_days > 0 else 0
                        
                        # Total One-sided Turnover (sum of daily portions)
                        # We skip the first day (initial allocation) for "churn" metrics, 
                        # but keeping it shows total volume. Usually exclude day 1 for "Strategy Turnover".
                        
                        if total_days > 1:
                            turnover_series = df_history['Turnover'].iloc[1:] # Exclude initial setup
                            total_turnover = turnover_series.sum()
                            avg_daily_turnover = turnover_series.mean()
                            annual_turnover = avg_daily_turnover * 252
                            
                            # Est Cost (use user-defined cost_bps)
                            total_cost_est = total_turnover * (cost_bps / 10000)
                            annual_cost_est = annual_turnover * (cost_bps / 10000)
                            
                            # Avg Holding Period (Days)
                            avg_hold_days = safe_div(1, avg_daily_turnover, 0)
                        else:
                            annual_turnover = 0
                            annual_cost_est = 0
                            avg_hold_days = 0

                        c1, c2, c3, c4 = st.columns(4)
                        with c1:
                            st.metric("å¹´åŒ–æ¢æ‰‹ç‡ (Annual Turnover)", f"{annual_turnover:.1%}", help="å¹³å‡æ¯å¹´è°ƒæ•´ä»“ä½çš„æ€»æ¯”ä¾‹ (å•è¾¹)")
                        with c2:
                            st.metric("å¹³å‡æŒä»“å‘¨æœŸ (Avg Hold)", f"{avg_hold_days:.1f} å¤©", help="å¹³å‡æ¯ç¬”èµ„é‡‘æŒæœ‰çš„å¤©æ•°")
                        with c3:
                            st.metric("é¢„ä¼°å¹´åŒ–æˆæœ¬ (Est. Cost)", f"{annual_cost_est:.2%}", help=f"åŸºäºå•è¾¹ {cost_bps}bps ({cost_bps/100}%) æ‰‹ç»­è´¹ä¼°ç®—çš„å¹´åŒ–æ‹–ç´¯")
                        with c4:
                            # Trading Frequency (Days with > 1% turnover)
                            active_days = df_history[df_history['Turnover'] > 0.01].count()['Turnover']
                            freq_pct = safe_div(active_days, total_days, 0)
                            st.metric("æ´»è·ƒäº¤æ˜“é¢‘ç‡", f"{freq_pct:.1%}", help="æ—¥æ¢æ‰‹ç‡è¶…è¿‡ 1% çš„å¤©æ•°æ¯”ä¾‹")

                        # Cost Sensitivity Analysis
                        st.markdown("**äº¤æ˜“æˆæœ¬æ•æ„Ÿæ€§åˆ†æ (Cost Sensitivity)**")
                        cost_levels = [5, 10, 15, 20, 30]
                        cost_impact = []
                        for c_bps in cost_levels:
                            annual_drag = annual_turnover * (c_bps / 10000) * 100  # Convert to %
                            cost_impact.append({'æˆæœ¬(bps)': c_bps, 'å¹´åŒ–æ‹–ç´¯%': annual_drag})
                        df_cost = pd.DataFrame(cost_impact)
                        
                        c_sens1, c_sens2 = st.columns([1, 2])
                        with c_sens1:
                            st.dataframe(df_cost.style.format({'å¹´åŒ–æ‹–ç´¯%': '{:.2f}%'}), use_container_width=True, hide_index=True)
                        with c_sens2:
                            fig_cost = go.Figure()
                            fig_cost.add_trace(go.Bar(x=df_cost['æˆæœ¬(bps)'].astype(str) + ' bps', y=df_cost['å¹´åŒ–æ‹–ç´¯%'], marker_color='#ff7043'))
                            fig_cost.update_layout(title="ä¸åŒæˆæœ¬æ°´å¹³ä¸‹çš„å¹´åŒ–æ‹–ç´¯", yaxis_title="å¹´åŒ–æ‹–ç´¯%", template="plotly_white", height=250)
                            st.plotly_chart(fig_cost, use_container_width=True)

                        # Chart: Rolling Turnover
                        # st.bar_chart(df_history['Turnover']) # Simple bar
                        
                        fig_to = go.Figure()
                        fig_to.add_trace(go.Bar(x=df_history.index, y=df_history['Turnover'], name='Daily Turnover'))
                        fig_to.update_layout(
                            title="æ¯æ—¥æ¢æ‰‹ç‡ (Daily Turnover)", 
                            yaxis=dict(title="Turnover %", tickformat=".1%"),
                            template="plotly_white",
                            height=300
                        )
                        st.plotly_chart(fig_to, use_container_width=True)
                    
                    # --- 4.5 v1.5 ä¼˜åŒ–æœºåˆ¶æ•ˆæœåˆ†æ ---
                    if df_history is not None and not df_history.empty:
                        st.markdown("---")
                        st.markdown("#### âš™ï¸ v1.5 ä¼˜åŒ–æœºåˆ¶æ•ˆæœ (Optimization Impact)")
                        st.caption("å±•ç¤ºå„ä¼˜åŒ–æ¨¡å—åœ¨å›æµ‹æœŸé—´çš„è§¦å‘æƒ…å†µä¸æ•ˆæœ")
                        
                        # æ­¢æŸè§¦å‘ç»Ÿè®¡
                        if 'InStopLoss' in df_history.columns:
                            stop_loss_days = df_history['InStopLoss'].sum()
                            stop_loss_pct = stop_loss_days / len(df_history) * 100
                            
                            # è®¡ç®—æ­¢æŸä¿æŠ¤æ•ˆæœ (æ­¢æŸæœŸé—´çš„å¹³å‡å›æ’¤æ¢å¤)
                            if 'Drawdown' in df_history.columns:
                                sl_drawdowns = df_history[df_history['InStopLoss']]['Drawdown']
                                avg_sl_drawdown = sl_drawdowns.mean() * 100 if len(sl_drawdowns) > 0 else 0
                        else:
                            stop_loss_days = 0
                            stop_loss_pct = 0
                            avg_sl_drawdown = 0
                        
                        # çŠ¶æ€è¿‡æ¸¡ç»Ÿè®¡
                        if 'InTransition' in df_history.columns:
                            transition_days = df_history['InTransition'].sum()
                        else:
                            transition_days = 0
                        
                        # å®é™…å†å¹³è¡¡ç»Ÿè®¡
                        if 'Rebalanced' in df_history.columns:
                            rebal_days = df_history['Rebalanced'].sum()
                            rebal_pct = rebal_days / len(df_history) * 100
                        else:
                            rebal_days = 0
                            rebal_pct = 0
                        
                        # æ˜¾ç¤ºç»Ÿè®¡å¡ç‰‡
                        col_opt1, col_opt2, col_opt3, col_opt4 = st.columns(4)
                        with col_opt1:
                            st.metric(
                                "ğŸ›¡ï¸ æ­¢æŸä¿æŠ¤å¤©æ•°", 
                                f"{stop_loss_days} å¤© ({stop_loss_pct:.1f}%)",
                                help=f"è§¦å‘æ­¢æŸæœºåˆ¶çš„å¤©æ•°ï¼ˆå›æ’¤>{abs(DRAWDOWN_STOP_LOSS)*100:.0f}%ï¼‰"
                            )
                        with col_opt2:
                            st.metric(
                                "ğŸ“‰ æ­¢æŸæœŸå¹³å‡å›æ’¤",
                                f"{avg_sl_drawdown:.2f}%" if stop_loss_days > 0 else "N/A",
                                help="æ­¢æŸä¿æŠ¤æœŸé—´çš„å¹³å‡å›æ’¤æ°´å¹³"
                            )
                        with col_opt3:
                            st.metric(
                                "ğŸ”€ çŠ¶æ€è¿‡æ¸¡å¤©æ•°",
                                f"{transition_days} å¤©",
                                help=f"çŠ¶æ€åˆ‡æ¢æ—¶çš„å¹³æ»‘è¿‡æ¸¡æœŸï¼ˆ{STATE_TRANSITION_DAYS}å¤©æ¸è¿›ï¼‰"
                            )
                        with col_opt4:
                            st.metric(
                                "ğŸ“Š å®é™…å†å¹³è¡¡",
                                f"{rebal_days} æ¬¡ ({rebal_pct:.1f}%)",
                                help=f"è¶…è¿‡å®¹å¿å¸¦{REBALANCE_THRESHOLD*100:.0f}%æ‰å†å¹³è¡¡"
                            )
                        
                        # è¯¦ç»†ä¼˜åŒ–æœºåˆ¶è¯´æ˜
                        with st.expander("ğŸ“– v1.5 ä¼˜åŒ–æœºåˆ¶è¯¦è§£", expanded=False):
                            st.markdown(f"""
**å½“å‰å¯ç”¨çš„ä¼˜åŒ–æœºåˆ¶ï¼š**

| æœºåˆ¶ | å‚æ•° | è¯´æ˜ |
|------|------|------|
| ğŸ›¡ï¸ åŠ¨æ€æ­¢æŸ | å›æ’¤>{abs(DRAWDOWN_STOP_LOSS)*100:.0f}%è§¦å‘ | å‡ä»“{DRAWDOWN_REDUCE_RATIO*100:.0f}%ï¼Œåˆ†é˜¶æ®µæ¢å¤ |
| ğŸ”„ ä¿¡å·ç¡®è®¤ | {SIGNAL_CONFIRM_DAYS}å¤©ç¡®è®¤æœŸ | çŠ¶æ€åˆ‡æ¢éœ€è¿ç»­{SIGNAL_CONFIRM_DAYS}å¤©ç¡®è®¤ |
| ğŸ“Š æ³¢åŠ¨ç‡ç›®æ ‡ | ç›®æ ‡{TARGET_VOL*100:.0f}%å¹´åŒ– | æ ¹æ®å®ç°æ³¢åŠ¨ç‡åŠ¨æ€è°ƒæ•´ä»“ä½ |
| ğŸ“ å†å¹³è¡¡å¸¦ | >{REBALANCE_THRESHOLD*100:.0f}%æ‰è°ƒä»“ | å‡å°‘é¢‘ç¹äº¤æ˜“æˆæœ¬ |
| ğŸ”€ å¹³æ»‘è¿‡æ¸¡ | {STATE_TRANSITION_DAYS}å¤©è¿‡æ¸¡ | çŠ¶æ€åˆ‡æ¢æ—¶æ¸è¿›è°ƒä»“ |
| ğŸ’µ ç°é‡‘ç¼“å†² | VIX>{CASH_BUFFER_VIX_THRESHOLD:.0f}æ—¶å¢åŠ  | åŸºç¡€{CASH_BUFFER_BASE*100:.0f}%ï¼Œæœ€é«˜{CASH_BUFFER_MAX*100:.0f}% |
| ğŸ“ˆ åŒå‡çº¿è¶‹åŠ¿ | MA{TREND_MA_SHORT}/MA{TREND_MA_LONG} | å¼ºç†Šå‡{STRONG_BEAR_REDUCTION*100:.0f}%ï¼Œå¼±ç†Šå‡{WEAK_BEAR_REDUCTION*100:.0f}% |
| ğŸ”— ç›¸å…³æ€§æ¸è¿› | {CORR_MID_THRESHOLD}-{CORR_HIGH_THRESHOLD}åŒºé—´ | è‚¡å€ºç›¸å…³æ€§å‡é«˜æ—¶æ¸è¿›è½¬ç§» |
| ğŸ“Š å¸‚åœºå¹¿åº¦ | <{MARKET_BREADTH_LOW*100:.0f}%æ—¶ä¿å®ˆ | è·¨èµ„äº§åŠ¨é‡å…±æŒ¯æ£€æµ‹ |
""")
                        
                        # æ­¢æŸè§¦å‘æ—¶é—´çº¿
                        if 'InStopLoss' in df_history.columns and stop_loss_days > 0:
                            st.markdown("**ğŸ›¡ï¸ æ­¢æŸä¿æŠ¤æ—¶é—´çº¿**")
                            
                            # æ‰¾å‡ºæ­¢æŸåŒºé—´
                            df_sl = df_history.copy()
                            df_sl['sl_change'] = df_sl['InStopLoss'].astype(int).diff().fillna(0)
                            
                            sl_starts = df_sl[df_sl['sl_change'] == 1].index.tolist()
                            sl_ends = df_sl[df_sl['sl_change'] == -1].index.tolist()
                            
                            # åŒ¹é…æ­¢æŸåŒºé—´
                            sl_periods = []
                            for i, start in enumerate(sl_starts):
                                # æ‰¾åˆ°å¯¹åº”çš„ç»“æŸç‚¹
                                end = None
                                for e in sl_ends:
                                    if e > start:
                                        end = e
                                        break
                                if end is None:
                                    end = df_history.index[-1]
                                duration = (end - start).days
                                sl_periods.append({
                                    'å¼€å§‹': start.strftime('%Y-%m-%d'),
                                    'ç»“æŸ': end.strftime('%Y-%m-%d'),
                                    'æŒç»­(å¤©)': duration
                                })
                            
                            if sl_periods:
                                st.dataframe(pd.DataFrame(sl_periods), hide_index=True, use_container_width=True)
                    
                    # --- 5. State Transition Analysis ---
                    st.markdown("---")
                    st.markdown("#### ğŸ”„ çŠ¶æ€è½¬æ¢åˆ†æ (State Transition Analysis)")
                    
                    if df_history is not None and not df_history.empty and 'State' in df_history.columns:
                        tab_trans, tab_attr, tab_yearly = st.tabs(["çŠ¶æ€è½¬æ¢çŸ©é˜µ", "æ”¶ç›Šå½’å› ", "åˆ†å¹´åº¦æ”¶ç›Š"])
                        
                        with tab_trans:
                            # State Transition Matrix
                            trans_counts, trans_probs = calculate_state_transition_matrix(df_history, 'State')
                            if trans_counts is not None and trans_probs is not None and not trans_counts.empty:
                                c_mat1, c_mat2 = st.columns(2)
                                with c_mat1:
                                    st.markdown("**è½¬æ¢æ¬¡æ•° (Counts)**")
                                    st.dataframe(trans_counts.style.background_gradient(cmap='Blues'), use_container_width=True)
                                with c_mat2:
                                    st.markdown("**è½¬æ¢æ¦‚ç‡ (Probabilities)**")
                                    st.dataframe(trans_probs.style.background_gradient(cmap='Greens', vmin=0, vmax=1).format("{:.1%}"), use_container_width=True)
                                
                                # State Statistics
                                st.markdown("**çŠ¶æ€ç»Ÿè®¡ (State Statistics)**")
                                state_stats = calculate_state_statistics(df_history, 'State')
                                if not state_stats.empty:
                                    state_stats_display = state_stats.copy()
                                    state_stats_display.columns = ['å‡ºç°æ¬¡æ•°', 'å¹³å‡æŒç»­(å¤©)', 'æ€»å¤©æ•°', 'æœ€çŸ­(å¤©)', 'æœ€é•¿(å¤©)']
                                    st.dataframe(state_stats_display, use_container_width=True)
                                    
                                    # Duration Distribution Chart
                                    segments = get_state_segments(df_history, 'State')
                                    if not segments.empty:
                                        fig_dur = go.Figure()
                                        for state in segments['State'].unique():
                                            durations = segments[segments['State'] == state]['Duration']
                                            s_conf = MACRO_STATES.get(state, MACRO_STATES["NEUTRAL"])
                                            fig_dur.add_trace(go.Box(y=durations, name=f"{s_conf['icon']} {state}", marker_color=s_conf['border_color']))
                                        fig_dur.update_layout(title="çŠ¶æ€æŒç»­æ—¶é—´åˆ†å¸ƒ (Duration Distribution)", yaxis_title="å¤©æ•°", template="plotly_white", height=350)
                                        st.plotly_chart(fig_dur, use_container_width=True)
                            else:
                                st.info("çŠ¶æ€æ•°æ®ä¸è¶³ä»¥ç”Ÿæˆè½¬æ¢çŸ©é˜µ")
                        
                        with tab_attr:
                            # Attribution Analysis by State
                            if 'Dynamic Strategy' in res.columns:
                                daily_rets = res['Dynamic Strategy'].pct_change().dropna()
                                state_rets = calculate_state_returns(df_history, daily_rets, 'State')
                                if not state_rets.empty:
                                    st.markdown("**æŒ‰çŠ¶æ€æ”¶ç›Šå½’å›  (Returns by State)**")
                                    state_rets_display = state_rets.copy()
                                    state_rets_display.columns = ['æ—¥å‡æ”¶ç›Š%', 'æ ‡å‡†å·®%', 'ç´¯è®¡æ”¶ç›Š%', 'å¤©æ•°', 'å¹´åŒ–æ”¶ç›Š%']
                                    st.dataframe(state_rets_display.style.background_gradient(subset=['ç´¯è®¡æ”¶ç›Š%'], cmap='RdYlGn'), use_container_width=True)
                                    
                                    # Contribution Bar Chart
                                    fig_attr = go.Figure()
                                    for state in state_rets.index:
                                        s_conf = MACRO_STATES.get(state, MACRO_STATES["NEUTRAL"])
                                        fig_attr.add_trace(go.Bar(
                                            x=[f"{s_conf['icon']} {state}"],
                                            y=[state_rets.loc[state, 'CumulativeRet']],
                                            name=state,
                                            marker_color=s_conf['border_color']
                                        ))
                                    fig_attr.update_layout(title="å„çŠ¶æ€æ”¶ç›Šè´¡çŒ® (Contribution by State)", yaxis_title="ç´¯è®¡æ”¶ç›Š%", template="plotly_white", showlegend=False, height=350)
                                    st.plotly_chart(fig_attr, use_container_width=True)
                                else:
                                    st.info("æ— æ³•è®¡ç®—æ”¶ç›Šå½’å› ")
                            else:
                                st.info("éœ€è¦ Dynamic Strategy åˆ—æ¥è®¡ç®—å½’å› ")
                        
                        with tab_yearly:
                            # Yearly Returns Table
                            if 'Dynamic Strategy' in res.columns:
                                # Calculate yearly returns
                                yearly_data = []
                                for col in res.columns:
                                    yearly_rets = res[col].resample('Y').last().pct_change().dropna() * 100
                                    for yr, ret in yearly_rets.items():
                                        yearly_data.append({'ç­–ç•¥': col, 'å¹´ä»½': yr.year, 'æ”¶ç›Šç‡%': ret})
                                
                                if yearly_data:
                                    df_yearly = pd.DataFrame(yearly_data)
                                    df_yearly_pivot = df_yearly.pivot(index='å¹´ä»½', columns='ç­–ç•¥', values='æ”¶ç›Šç‡%')
                                    
                                    st.markdown("**åˆ†å¹´åº¦æ”¶ç›Šç‡ (Yearly Returns)**")
                                    st.dataframe(df_yearly_pivot.style.background_gradient(cmap='RdYlGn', axis=None).format("{:.2f}%"), use_container_width=True)
                                    
                                    # Yearly Bar Chart
                                    fig_yearly = go.Figure()
                                    for col in df_yearly_pivot.columns:
                                        fig_yearly.add_trace(go.Bar(x=df_yearly_pivot.index.astype(str), y=df_yearly_pivot[col], name=col))
                                    fig_yearly.update_layout(title="åˆ†å¹´åº¦æ”¶ç›Šå¯¹æ¯” (Yearly Returns Comparison)", yaxis_title="æ”¶ç›Šç‡%", barmode='group', template="plotly_white", height=400)
                                    st.plotly_chart(fig_yearly, use_container_width=True)
                                    
                                    # Monthly Heatmap for Dynamic Strategy
                                    st.markdown("**æœˆåº¦æ”¶ç›Šçƒ­åŠ›å›¾ (Monthly Returns Heatmap)**")
                                    daily_s = res['Dynamic Strategy']
                                    if len(daily_s) > 30:
                                        monthly_rets = daily_s.resample('M').last().pct_change().dropna() * 100
                                        if len(monthly_rets) > 0:
                                            monthly_df = pd.DataFrame({
                                                'Year': monthly_rets.index.year,
                                                'Month': monthly_rets.index.month,
                                                'Return': monthly_rets.values
                                            })
                                            monthly_pivot = monthly_df.pivot(index='Year', columns='Month', values='Return')
                                            monthly_pivot.columns = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'][:len(monthly_pivot.columns)]
                                            
                                            fig_heat = go.Figure(data=go.Heatmap(
                                                z=monthly_pivot.values,
                                                x=monthly_pivot.columns,
                                                y=monthly_pivot.index.astype(str),
                                                colorscale='RdYlGn',
                                                zmid=0,
                                                text=np.round(monthly_pivot.values, 1),
                                                texttemplate="%{text}%",
                                                hovertemplate="Year: %{y}<br>Month: %{x}<br>Return: %{z:.2f}%<extra></extra>"
                                            ))
                                            fig_heat.update_layout(title="Dynamic Strategy æœˆåº¦æ”¶ç›Š", template="plotly_white", height=max(300, len(monthly_pivot) * 30))
                                            st.plotly_chart(fig_heat, use_container_width=True)
                                else:
                                    st.info("å›æµ‹å‘¨æœŸä¸è¶³ä¸€å¹´ï¼Œæ— æ³•ç”Ÿæˆå¹´åº¦æ•°æ®")
                            else:
                                st.info("éœ€è¦ç­–ç•¥æ•°æ®æ¥ç”Ÿæˆå¹´åº¦æ”¶ç›Š")
                    
                    # --- 6. Export Options ---
                    st.markdown("---")
                    st.markdown("#### ğŸ“¤ å¯¼å‡ºå›æµ‹ç»“æœ (Export Results)")
                    
                    c_exp1, c_exp2, c_exp3 = st.columns(3)
                    with c_exp1:
                        # Export Net Value Curve
                        csv_nv = res.to_csv()
                        st.download_button(
                            label="ğŸ“ˆ ä¸‹è½½å‡€å€¼æ›²çº¿ (CSV)",
                            data=csv_nv,
                            file_name=f"backtest_nav_{dates[0]}_{dates[1]}.csv",
                            mime="text/csv"
                        )
                    with c_exp2:
                        # Export Allocation History
                        if df_history is not None and not df_history.empty:
                            csv_alloc = df_history.to_csv()
                            st.download_button(
                                label="ğŸ“Š ä¸‹è½½æŒä»“å†å² (CSV)",
                                data=csv_alloc,
                                file_name=f"backtest_allocation_{dates[0]}_{dates[1]}.csv",
                                mime="text/csv"
                            )
                    with c_exp3:
                        # Export Summary Report
                        report_lines = [
                            f"å›æµ‹æŠ¥å‘Š - {dates[0]} è‡³ {dates[1]}",
                            f"ç”Ÿæˆæ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M')}",
                            "",
                            "=== å‚æ•°è®¾ç½® ===",
                            f"åˆå§‹èµ„é‡‘: {cap:,.0f}",
                            f"åŠ¨é‡çª—å£: {ma_window}",
                            f"ä½¿ç”¨ä»£ç†èµ„äº§: {'æ˜¯' if use_proxies else 'å¦'}",
                            f"å†å¹³è¡¡é¢‘ç‡: {rebal_freq}",
                            f"äº¤æ˜“æˆæœ¬: {cost_bps}bps",
                            "",
                            "=== æ€§èƒ½æŒ‡æ ‡ ===",
                        ]
                        for _, row in df_metrics.iterrows():
                            report_lines.append(f"\n{row['Strategy']}:")
                            for col in df_metrics.columns:
                                if col != 'Strategy':
                                    val = row[col]
                                    if isinstance(val, float):
                                        report_lines.append(f"  {col}: {val:.2f}")
                                    else:
                                        report_lines.append(f"  {col}: {val}")
                        
                        report_text = "\n".join(report_lines)
                        st.download_button(
                            label="ğŸ“ ä¸‹è½½æ‘˜è¦æŠ¥å‘Š (TXT)",
                            data=report_text,
                            file_name=f"backtest_report_{dates[0]}_{dates[1]}.txt",
                            mime="text/plain"
                        )
                    
            else:
                msg = err if err else "è¯¥æ—¶é—´æ®µå†…æ— æœ‰æ•ˆæ•°æ® (å¯èƒ½æ˜¯å› ä¸ºæ•°æ®æºä¸è¶³ï¼Œè¯·å°è¯•å‹¾é€‰ 'Use Proxies' æˆ–ç¼©çŸ­æ—¶é—´èŒƒå›´)"
                st.error(f"æ— æ³•è·å–æ•°æ®: {msg}")

def render_alert_config_ui():
    """Renders the configuration UI for auto-alerts."""
    with st.expander("ğŸ”” è‡ªåŠ¨æé†’è®¾ç½® (Auto-Alert Configuration)", expanded=False):
        st.caption("è®¾ç½®å®šæ—¶è‡ªåŠ¨åˆ†æå¸‚åœºçŠ¶æ€ï¼Œå¹¶å°†ç­–ç•¥å»ºè®®å‘é€åˆ°æ‚¨çš„é‚®ç®±ã€‚éœ€ä¿æŒåå°è„šæœ¬è¿è¡Œæˆ–ç½‘é¡µå¼€å¯ã€‚")
        
        config = load_alert_config()
        
        # Current snapshot for status cards
        email_to_saved = str(config.get("email_to", "")).strip()
        email_from_saved = str(config.get("email_from", "")).strip()
        email_ready = bool(email_to_saved and email_from_saved and config.get("email_pwd"))
        enabled_saved = bool(config.get("enabled", False))
        freq_saved = str(config.get("frequency", "Manual"))
        time_str_saved = str(config.get("trigger_time", "09:30"))
        try:
            trigger_time_saved = datetime.datetime.strptime(time_str_saved, "%H:%M").time()
        except Exception:
            trigger_time_saved = datetime.time(9, 30)

        def _next_run_preview(freq: str, trig_time: datetime.time):
            if freq not in ["Daily", "Weekly"]:
                return "æ‰‹åŠ¨è§¦å‘"
            sg_tz = datetime.timezone(datetime.timedelta(hours=8))
            now = datetime.datetime.now(sg_tz)
            today_trigger = datetime.datetime.combine(now.date(), trig_time, tzinfo=sg_tz)
            if freq == "Daily":
                nxt = today_trigger if now < today_trigger else today_trigger + datetime.timedelta(days=1)
            else:  # Weekly (Monday)
                days_ahead = (0 - now.weekday()) % 7
                if days_ahead == 0 and now >= today_trigger:
                    days_ahead = 7
                nxt = today_trigger + datetime.timedelta(days=days_ahead)
            return nxt.strftime("%Y-%m-%d %H:%M")

        next_run_preview = _next_run_preview(freq_saved, trigger_time_saved)
        status_color = "ğŸŸ¢ å·²å¯ç”¨" if (enabled_saved and email_ready) else ("ğŸŸ¡ å¾…è¡¥å…¨" if enabled_saved else "âšª æœªå¯ç”¨")

        c_status, c_next, c_last = st.columns(3)
        with c_status:
            st.metric("å½“å‰çŠ¶æ€", status_color, help="éœ€è¦åŒæ—¶å¼€å¯å¼€å…³å¹¶å¡«å†™é‚®ä»¶ä¿¡æ¯ã€‚")
        with c_next:
            st.metric("ä¸‹æ¬¡è§¦å‘", next_run_preview)
        with c_last:
            st.metric("ä¸Šæ¬¡è¿è¡Œ", config.get("last_run", "Never"))

        st.divider()

        with st.form("alert_config_form"):
            c1, c2 = st.columns(2)
            with c1:
                st.subheader("ğŸ“§ é‚®ä»¶é…ç½® (Email)")
                email_to = st.text_input("æ¥æ”¶é‚®ç®± (To)", value=email_to_saved, placeholder="you@example.com")
                email_from = st.text_input("å‘é€é‚®ç®± (From)", value=email_from_saved, placeholder="sender@gmail.com")
                email_pwd = st.text_input("æˆæƒç /å¯†ç  (App Password)", value=str(config.get("email_pwd", "")), type="password", help="Gmail/Outlook è¯·ä½¿ç”¨åº”ç”¨ä¸“ç”¨å¯†ç ï¼Œé¿å…ä½¿ç”¨çœŸå®ç™»å½•å¯†ç ã€‚")
                
                c1a, c1b = st.columns(2)
                with c1a:
                    smtp_server = st.text_input("SMTP æœåŠ¡å™¨", value=str(config.get("smtp_server", "smtp.gmail.com")))
                with c1b:
                    val_port = config.get("smtp_port", 587)
                    try: val_port = int(val_port)
                    except: val_port = 587
                    smtp_port = st.number_input("SMTP ç«¯å£", value=val_port)
            
            with c2:
                st.subheader("â° è§¦å‘è§„åˆ™ (Trigger)")
                enabled = st.checkbox("å¯ç”¨è‡ªåŠ¨æé†’ (Enable)", value=enabled_saved)
                
                curr_freq = freq_saved if freq_saved in ["Manual", "Daily", "Weekly"] else "Manual"
                frequency = st.selectbox("è§¦å‘é¢‘ç‡", ["Manual", "Daily", "Weekly"], index=["Manual", "Daily", "Weekly"].index(curr_freq))
                
                time_str = time_str_saved
                try:
                    time_obj = datetime.datetime.strptime(time_str, "%H:%M").time()
                except Exception:
                    time_obj = datetime.time(9, 30)
                trigger_time = st.time_input("è§¦å‘æ—¶é—´ (Local Time)", value=time_obj)

                st.markdown("**å®æ—¶é£æ§æé†’**")
                state_change_alert = st.checkbox("çŠ¶æ€å˜åŒ–æ—¶ç«‹å³æé†’", value=bool(config.get("state_change_alert", False)))
                vix_alert_enabled = st.checkbox("VIX è¶…é˜ˆå€¼æé†’", value=bool(config.get("vix_alert_enabled", False)))
                vix_alert_threshold = st.number_input("VIX é˜ˆå€¼", value=float(config.get("vix_alert_threshold", 35)), step=1.0)

                st.info("ä»…åœ¨åº”ç”¨è¿è¡Œæ—¶è§¦å‘ï¼›é»˜è®¤æ–°åŠ å¡æ—¶é—´ 09:30ï¼Œè¯·æ ¹æ®æœ¬åœ°/æœåŠ¡å™¨æ—¶åŒºè‡ªè¡Œè°ƒæ•´ã€‚")

            channels_cfg = config.get("channels", {}) or {}
            with st.expander("ğŸ“¡ å¤šæ¸ é“å ä½ (Telegram / ä¼ä¸šå¾®ä¿¡)", expanded=False):
                telegram_bot_token = st.text_input("Telegram Bot Token", value=str(channels_cfg.get("telegram_bot_token", "")))
                telegram_chat_id = st.text_input("Telegram Chat ID", value=str(channels_cfg.get("telegram_chat_id", "")))
                wechat_webhook = st.text_input("ä¼ä¸šå¾®ä¿¡ Webhook", value=str(channels_cfg.get("wechat_webhook", "")))

            if st.form_submit_button("ğŸ’¾ ä¿å­˜é…ç½®"):
                email_ready_form = bool(email_to.strip() and email_from.strip() and email_pwd)
                if enabled and not email_ready_form:
                    st.error("å¯ç”¨è‡ªåŠ¨æé†’éœ€è¦å¡«å†™æ”¶ä»¶äººã€å‘ä»¶äººå’Œæˆæƒç ã€‚")
                else:
                    new_config = {
                        "enabled": enabled,
                        "email_to": email_to.strip(),
                        "email_from": email_from.strip(),
                        "email_pwd": email_pwd,
                        "smtp_server": smtp_server.strip() or "smtp.gmail.com",
                        "smtp_port": smtp_port,
                        "frequency": frequency,
                        "trigger_time": trigger_time.strftime("%H:%M"),
                        "last_run": config.get("last_run", ""),
                        "state_change_alert": state_change_alert,
                        "vix_alert_enabled": vix_alert_enabled,
                        "vix_alert_threshold": vix_alert_threshold,
                        "channels": {
                            "telegram_bot_token": telegram_bot_token,
                            "telegram_chat_id": telegram_chat_id,
                            "wechat_webhook": wechat_webhook,
                        }
                    }
                    merged, issues, warns = validate_alert_config(new_config)
                    if issues:
                        for i in issues:
                            st.error(i)
                    else:
                        save_alert_config(merged)
                        for w in warns:
                            st.warning(w)
                        st.success("é…ç½®å·²ä¿å­˜!")
                        st.rerun()

        # Test Button
        if st.button("ğŸ“¨ ç«‹å³å‘é€æµ‹è¯•é‚®ä»¶ (Send Test Email)", type="secondary"):
            with st.spinner("æ­£åœ¨åˆ†æå¹¶å‘é€..."):
                cfg = load_alert_config()
                if cfg.get("enabled") and (not cfg.get("email_to") or not cfg.get("email_from") or not cfg.get("email_pwd")):
                    st.error("è¯·å…ˆè¡¥å…¨é‚®ç®±é…ç½®åå†æµ‹è¯•å‘é€ã€‚")
                else:
                    success, res = analyze_market_state_logic()
                    if success:
                        ok, msg = send_strategy_email(res, cfg)
                        if ok:
                            st.success(f"âœ… å‘é€æˆåŠŸ! è¯·æ£€æŸ¥é‚®ç®±: {cfg['email_to']}")
                        else:
                            st.error(f"âŒ å‘é€å¤±è´¥: {msg}")
                    else:
                        st.error(f"âŒ åˆ†æå¤±è´¥: {res}")

def render_state_machine_check():
    st.header("ğŸ›¡ï¸ å®è§‚çŠ¶æ€æœºä¸èµ„äº§é…ç½® (Macro State & Allocation)")
    st.caption("å…¨è‡ªåŠ¨èµ„äº§é…ç½®ç”Ÿæˆå™¨ (Auto-Allocator)")
    
    use_cache = st.toggle("âš¡ 5åˆ†é’Ÿç¼“å­˜ (å‡å°‘é‡å¤æ‹‰å–)", value=True)
    
    # 1. Alert Config
    render_alert_config_ui()
    
    # 2. Imports & Inputs
    render_manual_data_import()
    render_reference_guide()
    render_portfolio_import()
    current_holdings, total_value = render_holdings_input()
    
    # 3. Manual Analysis
    if st.button("ğŸš€ å¼€å§‹è¯Šæ–­ (Run Analysis)", type="primary", use_container_width=True):
        with st.status("æ­£åœ¨è¿›è¡Œå®è§‚æ‰«æ...", expanded=True) as status:
            st.write("ğŸ“¡ è·å–æ•°æ®å¹¶è®¡ç®—æŒ‡æ ‡...")
            
            # Use the shared logic with optional cache
            success, metrics = analyze_market_state_logic_cached() if use_cache else analyze_market_state_logic()
            
            if not success:
                status.update(label="è¯Šæ–­å¤±è´¥", state="error")
                st.error(metrics) # metrics is error msg here
            else:
                st.write("âœ… æ•°æ®è·å–ä¸è®¡ç®—å®Œæˆ")
                status.update(label="è¯Šæ–­å®Œæˆ", state="complete", expanded=False)

                # State history & alerts
                history = record_state_history(metrics['state'], metrics)
                change_info = get_state_change_info(history, metrics['state'], metrics.get('latest_date'))
                cfg = load_alert_config()
                if change_info:
                    prev_state = change_info.get('prev_state')
                    days_in_state = change_info.get('days_in_state')
                    changed_on = change_info.get('changed_on')
                    msg = f"å½“å‰çŠ¶æ€å·²æŒç»­ {days_in_state} å¤©" if days_in_state else "çŠ¶æ€æŒç»­æ—¶é—´æœªçŸ¥"
                    if prev_state:
                        msg = f"ä¸Šæ¬¡çŠ¶æ€ï¼š{prev_state} â†’ å½“å‰ï¼š{metrics['state']}ï¼Œè‡ª {changed_on} èµ· {days_in_state} å¤©"
                    st.info(msg)
                if cfg.get("state_change_alert") and change_info and change_info.get('prev_state') and change_info.get('prev_state') != metrics['state']:
                    st.warning("çŠ¶æ€å‘ç”Ÿå˜åŒ–ï¼Œå·²è§¦å‘æé†’ (å ä½)ã€‚")
                if cfg.get("vix_alert_enabled") and metrics.get('vix') is not None and metrics['vix'] >= cfg.get('vix_alert_threshold', 35):
                    st.error(f"VIX è¾¾åˆ° {metrics['vix']:.1f}ï¼Œè¶…è¿‡é˜ˆå€¼ {cfg.get('vix_alert_threshold', 35)}ã€‚")

                # è®°å½•æŒä»“å¿«ç…§ï¼ˆç”¨äºå›æ’¤è®¡ç®—ï¼‰
                if total_value > 0:
                    record_portfolio_snapshot(total_value, current_holdings, metrics['state'])

                # Render Results
                render_data_health_badges(metrics)
                render_status_card(metrics['state'])
                render_factor_dashboard(metrics)

                adjustments = get_adjustment_reasons(
                    metrics['state'],
                    gold_bear=metrics['gold_bear'],
                    value_regime=metrics['value_regime'],
                    asset_trends=metrics.get('asset_trends', {}),
                    vix=metrics.get('vix'),
                    yield_curve=metrics.get('yield_curve'),
                    sahm=metrics.get('sahm'),
                    corr=metrics.get('corr'),
                    yc_recently_inverted=metrics.get('yc_un_invert', False)
                )
                targets = get_target_percentages(
                    metrics['state'],
                    gold_bear=metrics['gold_bear'],
                    value_regime=metrics['value_regime'],
                    asset_trends=metrics.get('asset_trends', {}),
                    vix=metrics.get('vix'),
                    yield_curve=metrics.get('yield_curve'),
                    sahm=metrics.get('sahm'),
                    corr=metrics.get('corr'),
                    yc_recently_inverted=metrics.get('yc_un_invert', False)
                )
                price_info = get_live_prices(set(targets.keys()).union(current_holdings.keys()))
                
                st.markdown("---")
                render_rebalancing_table(
                    metrics['state'], 
                    current_holdings, 
                    total_value, 
                    metrics['gold_bear'], 
                    metrics['value_regime'], 
                    metrics.get('asset_trends', {}),
                    vix=metrics.get('vix'),
                    yield_curve=metrics.get('yield_curve'),
                    price_info=price_info,
                    sahm=metrics.get('sahm'),
                    corr=metrics.get('corr'),
                    yc_recently_inverted=metrics.get('yc_un_invert', False)
                )

                # æ‰§è¡Œå»ºè®®æç¤º
                st.markdown("---")
                execution_tips = generate_execution_tips(
                    metrics, 
                    change_info, 
                    current_holdings=current_holdings,
                    targets=targets,
                    total_value=total_value
                )
                render_execution_tips(execution_tips)

                # å¢å¼ºç‰ˆæ·±åº¦è¯Šæ–­
                render_enhanced_diagnosis(metrics, current_holdings, total_value, targets, change_info)

                render_export_options(metrics, adjustments, targets)
                if history:
                    hist_df = pd.DataFrame(history).tail(10)
                    with st.expander("ğŸ“œ æœ€è¿‘çŠ¶æ€å˜æ›´è®°å½•", expanded=False):
                        st.dataframe(hist_df, use_container_width=True, hide_index=True)
                
    render_historical_backtest_section()


# --- Page 2: Portfolio Backtest ---

def render_portfolio_backtest():
    st.header("ğŸ“Š æŠ•èµ„ç»„åˆå›æµ‹ (Portfolio Backtest)")
    st.caption("Design, test, and optimize your investment strategy.")
    
    # Init session state
    if 'port_selected_popular' not in st.session_state:
        st.session_state['port_selected_popular'] = ["SPY", "TLT"]
    if 'port_custom_tickers' not in st.session_state:
        st.session_state['port_custom_tickers'] = ""
    
    popular_etfs = {
        "US": ["SPY", "QQQ", "VOO", "VTI", "TLT", "GLD", "XLK", "XLF", "VNQ", "IWM"],
        "SG": ["ES3.SI", "G3B.SI", "S27.SI", "A35.SI", "O9P.SI", "CLR.SI"]
    }
    all_popular = popular_etfs["US"] + popular_etfs["SG"]

    # --- Sidebar: Global Settings & Portfolio Management ---
    st.sidebar.header("âš™ï¸ Configuration")
    
    with st.sidebar.expander("ğŸ“… Time & Capital", expanded=True):
        start_date = st.date_input("Start Date", pd.to_datetime("2022-01-01"))
        end_date = st.date_input("End Date", pd.to_datetime("today"))
        initial_capital = st.number_input("Initial Capital ($)", value=10000, step=1000, format="%d")

    # Portfolio Load/Save Management
    saved_portfolios = load_portfolios()
    
    with st.sidebar.expander("ğŸ“‚ Portfolio Manager", expanded=False):
        selected_saved = st.selectbox("Select Saved Portfolio", ["-- New / Unselected --"] + list(saved_portfolios.keys()))
        
        col_load, col_del = st.columns(2)
        if col_load.button("Load", use_container_width=True):
            if selected_saved != "-- New / Unselected --":
                p_data = saved_portfolios[selected_saved]
                saved_tickers = p_data.get("tickers", [])
                saved_weights = p_data.get("weights", {})
                
                # Update Tickers
                new_popular = [t for t in saved_tickers if t in all_popular]
                new_custom = [t for t in saved_tickers if t not in all_popular]
                
                st.session_state['port_selected_popular'] = new_popular
                st.session_state['port_custom_tickers'] = ", ".join(new_custom)
                
                # Update Weights
                for t, w in saved_weights.items():
                    st.session_state[f"w_{t}"] = w
                    
                st.toast(f"Loaded: {selected_saved}", icon="âœ…")
                st.rerun()

        if col_del.button("Delete", use_container_width=True):
            if selected_saved != "-- New / Unselected --":
                delete_portfolio(selected_saved)
                st.toast(f"Deleted {selected_saved}", icon="ğŸ—‘ï¸")
                st.rerun()

    # --- Main Area: Composition & Analysis ---
    
    # 1. Portfolio Composition Area (Card Style)
    with st.container():
        st.subheader("ğŸ› ï¸ Build Your Portfolio")
        
        col_comp_1, col_comp_2 = st.columns([1, 1])
        
        with col_comp_1:
            st.markdown("**1. Select Assets**")
            selected_popular = st.multiselect("Popular ETFs", all_popular, key="port_selected_popular")
            custom_tickers = st.text_input("Custom Tickers (comma separated)", placeholder="e.g. MSFT, D05.SI", key="port_custom_tickers")
            
            # Merge tickers
            tickers = list(selected_popular)
            if custom_tickers:
                custom_list = [t.strip().upper() for t in custom_tickers.split(",") if t.strip()]
                for t in custom_list:
                    if t not in tickers:
                        tickers.append(t)
        
        with col_comp_2:
            st.markdown("**2. Allocation (%)**")
            if not tickers:
                st.info("Please select assets first.")
                weights = {}
            else:
                # --- Fix Data Editor State Sync ---
                # Initialize state variables if not present
                if 'last_tickers' not in st.session_state:
                    st.session_state['last_tickers'] = []
                if 'internal_weights_df' not in st.session_state:
                    st.session_state['internal_weights_df'] = pd.DataFrame(columns=["Ticker", "Weight"])

                # Check if tickers have changed (Add/Remove assets)
                current_tickers = tickers
                if current_tickers != st.session_state['last_tickers']:
                    # Re-initialize weights logic
                    default_w = 100.0 / len(current_tickers)
                    new_data = []
                    
                    # Try to preserve existing weights from the internal DF or previous individual keys
                    prev_weights = {}
                    if not st.session_state['internal_weights_df'].empty:
                         prev_weights = dict(zip(st.session_state['internal_weights_df']['Ticker'], st.session_state['internal_weights_df']['Weight']))
                    
                    # Fallback to check individual keys if DF is empty (migration)
                    for t in current_tickers:
                        if t in prev_weights:
                            w = prev_weights[t]
                        elif f"w_{t}" in st.session_state:
                             w = st.session_state[f"w_{t}"]
                        else:
                            w = default_w
                        new_data.append({"Ticker": t, "Weight": w})
                    
                    st.session_state['internal_weights_df'] = pd.DataFrame(new_data)
                    st.session_state['last_tickers'] = current_tickers

                # Render Data Editor using the persistent DataFrame
                edited_df = st.data_editor(
                    st.session_state['internal_weights_df'],
                    column_config={
                        "Ticker": st.column_config.TextColumn("Asset", disabled=True),
                        "Weight": st.column_config.NumberColumn("Weight (%)", min_value=0, max_value=100, step=1, format="%.1f")
                    },
                    hide_index=True,
                    use_container_width=True,
                    key="weight_editor"
                )
                
                # Update the persistent DataFrame immediately with edited values
                st.session_state['internal_weights_df'] = edited_df
                
                # Extract weights for calculation
                weights = dict(zip(edited_df['Ticker'], edited_df['Weight']))
                
                # Sync back to individual keys (optional, but keeps compatibility)
                for t, w in weights.items():
                    st.session_state[f"w_{t}"] = w

        # Validation & Actions
        if tickers:
            # --- Benchmark Selection (Moved Up) ---
            st.markdown("---")
            col_bench_1, col_bench_2 = st.columns([3, 1])
            with col_bench_1:
                default_benchmarks = {
                    "Benchmark: S&P 500 (SPY)": {"tickers": ["SPY"], "weights": {"SPY": 100}},
                    "Benchmark: Nasdaq 100 (QQQ)": {"tickers": ["QQQ"], "weights": {"QQQ": 100}},
                    "Benchmark: 60/40 Balanced": {"tickers": ["SPY", "TLT"], "weights": {"SPY": 60, "TLT": 40}}
                }
                available_comparisons = list(default_benchmarks.keys()) + list(saved_portfolios.keys())
                selected_comparisons = st.multiselect(
                    "âš”ï¸ Benchmark / Compare Against (Optional):", 
                    available_comparisons,
                    placeholder="Select benchmarks to compare performance..."
                )
            
            st.divider()

            total_weight = sum(weights.values())
            
            # Action Bar
            col_act_1, col_act_2, col_act_3 = st.columns([2, 1, 1])
            with col_act_1:
                 if abs(total_weight - 100) > 0.1:
                    st.warning(f"Total Allocation: {total_weight:.1f}% (Will be normalized to 100%)", icon="âš ï¸")
                 else:
                    st.success(f"Total Allocation: {total_weight:.1f}%", icon="âœ…")
            
            with col_act_2:
                # Save Logic
                with st.popover("ğŸ’¾ Save Portfolio"):
                    save_name = st.text_input("Name", placeholder="My Portfolio")
                    if st.button("Confirm Save", type="primary"):
                        if save_name:
                            save_portfolio(save_name, tickers, weights)
                            st.toast(f"Saved '{save_name}'!", icon="ğŸ’¾")
                        else:
                            st.error("Name required.")

            with col_act_3:
                run_backtest = st.button("ğŸš€ Run Backtest", type="primary", use_container_width=True)
        else:
            run_backtest = False

    st.markdown("---")

    # 2. Backtest Analysis Area
    if run_backtest and tickers:
        
        with st.spinner("Crunching numbers..."):
            try:
                # 1. Collect Tickers
                all_tickers_set = set(tickers)
                comparison_specs = {} 
                
                for comp_name in selected_comparisons:
                    # Check if it's a default benchmark
                    if comp_name in default_benchmarks:
                        comp_data = default_benchmarks[comp_name]
                    else:
                        comp_data = saved_portfolios.get(comp_name, {})
                        
                    c_tickers = comp_data.get("tickers", [])
                    c_weights_raw = comp_data.get("weights", {})
                    
                    if c_tickers:
                        all_tickers_set.update(c_tickers)
                        c_total_w = sum(c_weights_raw.values())
                        if c_total_w > 0:
                            c_weights_norm = {k: v / c_total_w for k, v in c_weights_raw.items()}
                            comparison_specs[comp_name] = {"tickers": c_tickers, "weights": c_weights_norm}

                download_tickers = list(all_tickers_set)

                # 2. Fetch Data
                # Added auto_adjust=False to maintain consistent behavior
                data_raw = yf.download(download_tickers, start=start_date, end=end_date, progress=False, auto_adjust=False)['Adj Close']
                
                if data_raw.empty:
                    st.error("No data found. Check tickers or internet connection.")
                    return
                
                if isinstance(data_raw, pd.Series):
                    data_raw = data_raw.to_frame(name=download_tickers[0])
                elif isinstance(data_raw, pd.DataFrame) and len(download_tickers) == 1:
                    data_raw.columns = download_tickers
                
                data = data_raw.dropna(axis=1, how='all')
                available_tickers = set(data.columns.tolist())
                
                if not available_tickers:
                    st.error("No data for selected assets.")
                    return
                
                data = data.ffill().bfill()
                normalized_prices = data / data.iloc[0]

                # --- Calculation Helper ---
                def calculate_portfolio_performance(p_tickers, p_weights_norm, p_name):
                    valid_p_tickers = [t for t in p_tickers if t in available_tickers]
                    if not valid_p_tickers: return None
                    
                    valid_w_sum = sum([p_weights_norm.get(t, 0) for t in valid_p_tickers])
                    if valid_w_sum == 0: return None
                    
                    p_weights_final = {t: p_weights_norm.get(t, 0) / valid_w_sum for t in valid_p_tickers}
                    
                    # Calc Value
                    val_series = pd.Series(0, index=data.index)
                    for t in valid_p_tickers:
                        w = p_weights_final[t]
                        val_series += normalized_prices[t] * (initial_capital * w)
                    
                    val_series.name = p_name
                    
                    # Metrics
                    tot_ret = (val_series.iloc[-1] / val_series.iloc[0] - 1) * 100
                    days = (val_series.index[-1] - val_series.index[0]).days
                    cagr = ((val_series.iloc[-1] / val_series.iloc[0]) ** (365 / days) - 1) * 100 if days > 0 else 0
                    
                    rolling_max = val_series.cummax()
                    dd = (val_series / rolling_max - 1) * 100
                    max_dd = dd.min()
                    
                    daily_ret = val_series.pct_change().dropna()
                    vol = daily_ret.std() * np.sqrt(252) * 100
                    
                    rf_daily = 0.03 / 252
                    excess = daily_ret - rf_daily
                    sharpe = (excess.mean() / daily_ret.std()) * np.sqrt(252) if daily_ret.std() > 0 else 0
                    
                    # Sortino Ratio
                    downside_returns = daily_ret[daily_ret < 0]
                    downside_std = downside_returns.std() * np.sqrt(252)
                    sortino = (excess.mean() * 252 * 100) / (downside_std * 100) if downside_std > 0 else 0
                    
                    # Calmar Ratio
                    calmar = cagr / abs(max_dd) if max_dd != 0 else 0

                    # Max Drawdown Duration (Longest Recovery Time)
                    # Logic: Find peaks, fill dates forward, subtract current date from last peak date
                    # Uses Calendar Days
                    is_peak = val_series == rolling_max
                    peak_dates = pd.Series(val_series.index, index=val_series.index).where(is_peak).ffill()
                    dd_days = val_series.index - peak_dates
                    max_duration_days = dd_days.max().days if not dd_days.empty else 0
                    
                    return {
                        "name": p_name,
                        "series": val_series,
                        "drawdown": dd,
                        "metrics": {
                            "Final Balance": val_series.iloc[-1],
                            "Total Return (%)": tot_ret,
                            "CAGR (%)": cagr,
                            "Max Drawdown (%)": max_dd,
                            "Max DD Duration (Days)": max_duration_days,
                            "Volatility (%)": vol,
                            "Sharpe Ratio": sharpe,
                            "Sortino Ratio": sortino,
                            "Calmar Ratio": calmar
                        }
                    }

                # 3. Calculate "Current" Portfolio
                current_perf = calculate_portfolio_performance(tickers, weights, "Current Portfolio")
                if not current_perf:
                    st.error("Invalid current portfolio data.")
                    return
                
                results = [current_perf]
                
                # 4. Calculate Comparison Portfolios
                for c_name, c_spec in comparison_specs.items():
                    c_perf = calculate_portfolio_performance(c_spec["tickers"], c_spec["weights"], c_name)
                    if c_perf:
                        results.append(c_perf)
                    else:
                        st.warning(f"Skipping '{c_name}': insufficient data.")
                
                # --- Display Results ---
                st.subheader("ğŸ“ˆ Backtest Results")
                
                # A. Summary Metrics (Top Row - KPI Cards)
                curr_metrics = results[0]["metrics"]
                
                cols_kpi = st.columns(4)
                cols_kpi[0].metric("Total Return", f"{curr_metrics['Total Return (%)']:.2f}%", help="Cumulative return over period")
                cols_kpi[1].metric("CAGR", f"{curr_metrics['CAGR (%)']:.2f}%", help="Compound Annual Growth Rate")
                cols_kpi[2].metric("Max Drawdown", f"{curr_metrics['Max Drawdown (%)']:.2f}%", help="Deepest peak-to-valley decline")
                cols_kpi[3].metric("Sharpe Ratio", f"{curr_metrics['Sharpe Ratio']:.2f}", help="Risk-adjusted return")

                # B. Interactive Charts & Details
                tab_chart, tab_dd, tab_monthly, tab_stats, tab_corr = st.tabs(["ğŸ’° Value Growth", "ğŸ“‰ Drawdowns", "ğŸ“… Monthly Returns", "ğŸ“‹ Detailed Stats", "ğŸ”¥ Correlation"])
                
                with tab_chart:
                    fig = go.Figure()
                    # Add Current (Thicker line)
                    curr_s = results[0]["series"]
                    fig.add_trace(go.Scatter(x=curr_s.index, y=curr_s, name=results[0]["name"], line=dict(width=3, color='#2962FF')))
                    
                    # Add Comparisons
                    colors = ['#FF6D00', '#00C853', '#AA00FF', '#FFD600', '#D50000', '#3E2723']
                    for i, res in enumerate(results[1:]):
                        col = colors[i % len(colors)]
                        fig.add_trace(go.Scatter(
                            x=res["series"].index, 
                            y=res["series"], 
                            name=res["name"], 
                            line=dict(width=2, color=col, dash='dot')
                        ))
                    
                    fig.update_layout(
                        title="Portfolio Value Comparison",
                        xaxis_title="Date",
                        yaxis_title="Value ($)",
                        height=550,
                        template="plotly_white",
                        hovermode="x unified",
                        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1)
                    )
                    st.plotly_chart(fig, use_container_width=True)

                with tab_dd:
                    fig_dd = go.Figure()
                    # Current
                    fig_dd.add_trace(go.Scatter(x=results[0]["drawdown"].index, y=results[0]["drawdown"], name=results[0]["name"], line=dict(width=2, color='#2962FF'), fill='tozeroy'))
                    
                    # Comparisons
                    for i, res in enumerate(results[1:]):
                        col = colors[i % len(colors)]
                        fig_dd.add_trace(go.Scatter(x=res["drawdown"].index, y=res["drawdown"], name=res["name"], line=dict(width=1, color=col)))
                        
                    fig_dd.update_layout(title="Portfolio Drawdown (%)", yaxis_title="Drawdown %", template="plotly_white", height=500, hovermode="x unified")
                    st.plotly_chart(fig_dd, use_container_width=True)

                with tab_monthly:
                    st.markdown("#### ğŸ“… Monthly Returns Heatmap")
                    
                    # Select portfolio to visualize
                    port_names = [r["name"] for r in results]
                    selected_heatmap_port = st.selectbox("Select Portfolio:", port_names, key="heatmap_port_select")
                    
                    # Find selected result
                    sel_res = next((r for r in results if r["name"] == selected_heatmap_port), results[0])
                    
                    # Calculate Monthly Returns
                    daily_s = sel_res["series"]
                    monthly_s = daily_s.resample('M').last().pct_change() * 100
                    
                    if not monthly_s.empty:
                        # Prepare Pivot Table
                        monthly_df = monthly_s.to_frame(name='Return')
                        monthly_df['Year'] = monthly_df.index.year
                        monthly_df['Month'] = monthly_df.index.month_name().str[:3] # Jan, Feb...
                        
                        # Pivot: Index=Year, Columns=Month
                        month_order = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']
                        pivot_ret = monthly_df.pivot_table(index='Year', columns='Month', values='Return')
                        pivot_ret = pivot_ret.reindex(columns=month_order)
                        
                        # Add Year Total
                        year_ret = daily_s.resample('Y').last().pct_change() * 100
                        year_ret.index = year_ret.index.year
                        pivot_ret['YTD'] = year_ret
                        
                        # Heatmap using Plotly
                        fig_hm = go.Figure(data=go.Heatmap(
                            z=pivot_ret.values,
                            x=pivot_ret.columns,
                            y=pivot_ret.index,
                            colorscale='RdBu',
                            zmid=0,
                            text=np.round(pivot_ret.values, 1),
                            texttemplate="%{text}%",
                            showscale=True
                        ))
                        fig_hm.update_layout(
                            title=f"{selected_heatmap_port} - Monthly Returns (%)",
                            height=max(400, len(pivot_ret)*30 + 100),
                            yaxis=dict(autorange="reversed", type='category')
                        )
                        st.plotly_chart(fig_hm, use_container_width=True)
                    else:
                        st.info("Not enough data for monthly analysis.")

                with tab_stats:
                    metrics_data = []
                    for res in results:
                        m = res["metrics"]
                        row = {"Portfolio": res["name"]}
                        row.update(m)
                        metrics_data.append(row)
                    
                    metrics_df = pd.DataFrame(metrics_data)
                    st.dataframe(
                        metrics_df,
                        use_container_width=True,
                        column_config={
                            "Final Balance": st.column_config.NumberColumn(format="$%.2f"),
                            "Total Return (%)": st.column_config.NumberColumn(format="%.2f%%"),
                            "CAGR (%)": st.column_config.NumberColumn(format="%.2f%%"),
                            "Max Drawdown (%)": st.column_config.NumberColumn(format="%.2f%%"),
                            "Max DD Duration (Days)": st.column_config.NumberColumn(help="Longest time to recover from a drawdown (in trading days)"),
                            "Volatility (%)": st.column_config.NumberColumn(format="%.2f%%"),
                            "Sharpe Ratio": st.column_config.NumberColumn(format="%.2f"),
                            "Sortino Ratio": st.column_config.NumberColumn(format="%.2f"),
                            "Calmar Ratio": st.column_config.NumberColumn(format="%.2f"),
                        },
                        hide_index=True
                    )

                with tab_corr:
                    if len(tickers) > 1:
                        # Extract data for current portfolio tickers only
                        valid_curr_tickers = [t for t in tickers if t in available_tickers]
                        if len(valid_curr_tickers) > 1:
                            curr_data = data[valid_curr_tickers]
                            corr = curr_data.pct_change().corr()
                            fig_corr = go.Figure(data=go.Heatmap(
                                z=corr.values,
                                x=corr.columns,
                                y=corr.index,
                                colorscale='RdBu',
                                zmin=-1, zmax=1,
                                text=np.round(corr.values, 2),
                                texttemplate="%{text}",
                                showscale=True
                            ))
                            fig_corr.update_layout(height=600, title="Asset Correlation Matrix")
                            st.plotly_chart(fig_corr, use_container_width=True)
                    else:
                        st.info("Correlation matrix requires at least 2 assets in the portfolio.")

                # --- Download Section ---
                st.markdown("### ğŸ“¥ Export Data")
                
                # Prepare Daily Data CSV
                df_export = pd.DataFrame(index=data.index)
                for res in results:
                    df_export[f"{res['name']} Value"] = res["series"]
                    df_export[f"{res['name']} Drawdown"] = res["drawdown"]
                
                csv_data = df_export.to_csv().encode('utf-8')
                
                st.download_button(
                    label="Download Daily Backtest Data (CSV)",
                    data=csv_data,
                    file_name="backtest_daily_data.csv",
                    mime="text/csv",
                )

            except Exception as e:
                st.error(f"Analysis Error: {e}")


# --- Main App Navigation ---

st.sidebar.title("App Navigation")
page = st.sidebar.radio("é€‰æ‹©åŠŸèƒ½", ["çŠ¶æ€æœºæ£€æŸ¥", "æŠ•èµ„ç»„åˆå›æµ‹"])

if page == "çŠ¶æ€æœºæ£€æŸ¥":
    render_state_machine_check()
elif page == "æŠ•èµ„ç»„åˆå›æµ‹":
    render_portfolio_backtest()
